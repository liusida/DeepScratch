{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todo list\n",
    "\n",
    "1. one_hot √\n",
    "\n",
    "* mini-batch\n",
    "\n",
    "* normalization\n",
    "\n",
    "* train/dev/test set √\n",
    "\n",
    "* linear function √\n",
    "\n",
    "* sigmoid function\n",
    "\n",
    "* tanh function\n",
    "\n",
    "* relu function √\n",
    "\n",
    "* softmax function √\n",
    "\n",
    "* loss function √\n",
    "\n",
    "* cost function ?\n",
    "\n",
    "* regularization\n",
    "\n",
    "* drop out\n",
    "\n",
    "* batch normalization\n",
    "\n",
    "* momentum\n",
    "\n",
    "* exponentially moving average\n",
    "\n",
    "* Adam\n",
    "\n",
    "* all backpropagation of above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\"\"\" helper functions \"\"\"\n",
    "def names_in(dictionary):\n",
    "    \"\"\" list all names in a dictionary \"\"\"\n",
    "    print([name for name,_ in sorted(dictionary.items())])\n",
    "def names_shape_in(dictionary):\n",
    "    pprint([(name, val.shape) for name,val in sorted(dictionary.items())])\n",
    "\n",
    "def debug_show_all_variables():\n",
    "    print(\"cache: \", end='')\n",
    "    names_shape_in(cache)\n",
    "    print(\"parameters: \", end='')\n",
    "    names_shape_in(parameters)\n",
    "    print(\"hyper_parameters: \", end='')\n",
    "    names_in(hyper_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_mnist():\n",
    "    import struct\n",
    "    from array import array\n",
    "    \"\"\" \n",
    "    load MNIST dataset into numpy array \n",
    "    MNIST dataset can be downloaded manually.\n",
    "    url: http://yann.lecun.com/exdb/mnist/\n",
    "    \"\"\"\n",
    "    ret = {}\n",
    "    with open('MNIST/train-images.idx3-ubyte', 'rb') as f:\n",
    "        magic, size, rows, cols = struct.unpack(\">IIII\", f.read(16))\n",
    "        assert(magic==2051)\n",
    "        ret['X_train'] = np.array(array(\"B\", f.read())).reshape(size,rows,cols)\n",
    "\n",
    "    with open('MNIST/t10k-images.idx3-ubyte', 'rb') as f:\n",
    "        magic, size, rows, cols = struct.unpack(\">IIII\", f.read(16))\n",
    "        assert(magic==2051)\n",
    "        ret['X_test'] = np.array(array(\"B\", f.read())).reshape(size,rows,cols)\n",
    "\n",
    "    with open('MNIST/train-labels.idx1-ubyte', 'rb') as f:\n",
    "        magic, size = struct.unpack(\">II\", f.read(8))\n",
    "        assert(magic==2049)\n",
    "        ret['Y_train'] = np.array(array(\"B\", f.read())).reshape(size,1)\n",
    "\n",
    "    with open('MNIST/t10k-labels.idx1-ubyte', 'rb') as f:\n",
    "        magic, size = struct.unpack(\">II\", f.read(8))\n",
    "        assert(magic==2049)\n",
    "        ret['Y_test'] = np.array(array(\"B\", f.read())).reshape(size,1)\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (50000, 28, 28) X_dev: (10000, 28, 28) X_test: (10000, 28, 28)\n",
      "Y_train: (50000, 1) Y_dev: (10000, 1) Y_test: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "mnist_original = load_mnist()\n",
    "\n",
    "\"\"\" random shuffle the training set \"\"\"\n",
    "np.random.seed(1)\n",
    "permutation = np.random.permutation(mnist_original['X_train'].shape[0])\n",
    "mnist_original['X_train'] = mnist_original['X_train'][permutation]\n",
    "mnist_original['Y_train'] = mnist_original['Y_train'][permutation]\n",
    "\n",
    "\"\"\" divide trainset into trainset and devset \"\"\"\n",
    "len_of_dev = 10000\n",
    "mnist_original['X_dev'] = mnist_original['X_train'][:len_of_dev]\n",
    "mnist_original['Y_dev'] = mnist_original['Y_train'][:len_of_dev]\n",
    "mnist_original['X_train'] = mnist_original['X_train'][len_of_dev:]\n",
    "mnist_original['Y_train'] = mnist_original['Y_train'][len_of_dev:]\n",
    "\n",
    "print('X_train:', mnist_original['X_train'].shape,\n",
    "      'X_dev:', mnist_original['X_dev'].shape,\n",
    "      'X_test:', mnist_original['X_test'].shape)\n",
    "print('Y_train:', mnist_original['Y_train'].shape,\n",
    "      'Y_dev:', mnist_original['Y_dev'].shape,\n",
    "      'Y_test:', mnist_original['Y_test'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def manually_validate_dataset(dataset):\n",
    "    random_train = np.random.randint(1, len(dataset['X_train']))-1\n",
    "    random_dev = np.random.randint(1, len(dataset['X_dev']))-1\n",
    "    random_test = np.random.randint(1, len(dataset['X_test']))-1\n",
    "    print(dataset['Y_train'][random_train], dataset['Y_dev'][random_dev], dataset['Y_test'][random_test])\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1,3, sharey=True, figsize=[10,3])\n",
    "    ax1.imshow(dataset['X_train'][random_train], cmap='gray')\n",
    "    ax2.imshow(dataset['X_dev'][random_dev], cmap='gray')\n",
    "    ax3.imshow(dataset['X_test'][random_test], cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "#manually_validate_dataset(mnist_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ X = \n",
    "\\begin{bmatrix}\n",
    "\\vert & & \\vert & & \\vert \\\\\n",
    "x^{(1)} & ... & x^{(i)} & ... & x^{(m)} \\\\\n",
    "\\vert & & \\vert & & \\vert\n",
    "\\end{bmatrix} \n",
    "\\quad \\quad\n",
    "Y = \n",
    "\\begin{bmatrix}\n",
    "\\vert & & \\vert & & \\vert \\\\\n",
    "y_{one\\_hot}^{(1)} & ... & y_{one\\_hot}^{(i)} & ... & y_{one\\_hot}^{(m)} \\\\\n",
    "\\vert & & \\vert & & \\vert \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 50000) (10, 50000)\n"
     ]
    }
   ],
   "source": [
    "mnist = {}\n",
    "\n",
    "\"\"\" X is 28*28 image \"\"\"\n",
    "def flatten(X):\n",
    "    \"\"\" prepare X to (nx, m) shape \"\"\"\n",
    "    X = X.reshape(-1, 28*28).T\n",
    "    return X\n",
    "\n",
    "def normalize(X):\n",
    "    u = 0\n",
    "    sigma = 255\n",
    "    return (X-u) / sigma\n",
    "\n",
    "mnist['X_train'] = normalize(flatten(mnist_original['X_train']))\n",
    "mnist['X_dev'] = normalize(flatten(mnist_original['X_dev']))\n",
    "mnist['X_test'] = normalize(flatten(mnist_original['X_test']))\n",
    "\n",
    "\"\"\" Y is label 0-9 \"\"\"\n",
    "def one_hot(Y, C):\n",
    "    \"\"\" prepare Y to (1, m) shape \"\"\"\n",
    "    assert(Y.shape[1]==1)\n",
    "    Y_ret = np.zeros((Y.shape[0], C))\n",
    "    Y_ret[np.arange(Y.shape[0]), Y.reshape(-1).astype(int)] = 1\n",
    "    Y_ret = Y_ret.T\n",
    "    return Y_ret\n",
    "\n",
    "def back_one_hot(Y):\n",
    "    \"\"\" convert one hot Y back to real number \"\"\"\n",
    "    Y_ret = np.repeat( [np.arange(Y.shape[0])], repeats=Y.shape[1], axis=0 )\n",
    "    assert(Y_ret.shape == Y.T.shape)\n",
    "    Y_ret = Y_ret[Y.T.astype(bool)]\n",
    "    return Y_ret.reshape(-1,1)\n",
    "\n",
    "mnist['Y_train'] = one_hot(mnist_original['Y_train'], 10)\n",
    "mnist['Y_dev'] = one_hot(mnist_original['Y_dev'], 10)\n",
    "mnist['Y_test'] = one_hot(mnist_original['Y_test'], 10)\n",
    "\n",
    "print(mnist['X_train'].shape, mnist['Y_train'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" init cache, parameters and hyper_parameters \"\"\"\n",
    "def init(X_size, hidden_layers=[10,10,5], C=10, learning_rate=0.01):\n",
    "    global cache, parameters, hyper_parameters\n",
    "    cache = {}\n",
    "    parameters = {}\n",
    "    hyper_parameters = {}\n",
    "    initialize_hyper_parameters(X_size, hidden_layers, C, learning_rate)\n",
    "    initialize_parameters()\n",
    "\n",
    "def initialize_hyper_parameters(X_size, hidden_layers, C, learning_rate):\n",
    "    global hyper_parameters\n",
    "    hyper_parameters['X_size'] = X_size\n",
    "    # layers of network, include the last softmax layer\n",
    "    # hidden layers use ReLU activation function, and the last layer use Softmax function\n",
    "    hyper_parameters['layers'] = hidden_layers + [C]\n",
    "    # L layers in total\n",
    "    hyper_parameters['L'] = len(hyper_parameters['layers'])\n",
    "    # Class numbers 0-9 is 10\n",
    "    hyper_parameters['C'] = C\n",
    "    hyper_parameters['learning_rate'] = learning_rate\n",
    "\n",
    "def initialize_parameters():\n",
    "    \"\"\" init W b or any other parameters in every layer \"\"\"\n",
    "    global parameters, hyper_parameters\n",
    "    layers = hyper_parameters['layers']\n",
    "    x_size = hyper_parameters['X_size']\n",
    "    cells_prev = x_size\n",
    "    for layer_idx, cells in enumerate(layers):\n",
    "        parameters['W'+str(layer_idx+1)] = np.random.randn(cells, cells_prev) * 0.01\n",
    "        parameters['b'+str(layer_idx+1)] = np.zeros((cells, 1))\n",
    "        cells_prev = layers[layer_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "ReLu = max(0, x)\n",
    "\\quad \\quad\n",
    "Softmax = \\frac{\\exp(Z)}{\\sum_i^n{\\exp(Z)}}\n",
    "$$\n",
    "***\n",
    "$$\n",
    "Z = W X + b\n",
    "\\quad \\quad\n",
    "A = active(Z)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(X):\n",
    "    return X * (X > 0)\n",
    "\n",
    "def softmax(X):\n",
    "    s = np.sum(np.exp(X), axis=0)\n",
    "    return np.exp(X) / s\n",
    "\n",
    "def forward_propagation_each_layer(W, A_prev, b, activation_function=ReLU):\n",
    "    Z = np.dot(W, A_prev) + b\n",
    "    A = activation_function(Z)\n",
    "    return Z,A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is logistic regression loss:\n",
    "$$\n",
    "L(\\hat{y}, y) = -\\frac{1}{m} \\sum_i^m{(y_i\\log(\\hat{y}_i) + (1-y_i)\\log(1-\\hat{y}_i))}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is multi-class classification loss:\n",
    "$$\n",
    "L(\\hat{y}, y) = -\\frac{1}{m} \\sum_i^m \\sum_j^C (y_j^{(i)}\\log(\\hat{y}_j^{(i)}))\n",
    "$$\n",
    "where m is batch size, C is class number (10 classes), (i) is the i-th example, j is the vertical bit of result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(Y_hat, Y):\n",
    "    A = np.multiply(Y, np.log(Y_hat))\n",
    "    B = np.multiply(1-Y, np.log(1-Y_hat))\n",
    "    Loss = -A-B\n",
    "    #why not use Loss = -np.sum(A+B) / m?\n",
    "    return Loss\n",
    "\n",
    "def cost(loss):\n",
    "    return np.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(Y_hat):\n",
    "    return np.argmax(Y_hat,axis=0).reshape(-1,1)\n",
    "\n",
    "def accuracy(Y_predict, Y):\n",
    "    return np.sum(np.equal(Y_predict, back_one_hot(Y))) / Y_predict.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This formula is wrong, This is the logistic regression(binary classifier) Loss\n",
    "\n",
    "\n",
    "We already know L formula above:\n",
    "$$\n",
    "L = -\\frac{1}{m} \\sum_i^{m}[y^{(i)} log(\\hat{y}^{(i)}) + (1-y^{(i)}) log(1-\\hat{y}^{(i)})]\n",
    "$$\n",
    "***\n",
    "$$\n",
    "A^{[L]} = \\hat{Y} = \n",
    "\\begin{bmatrix}\n",
    "\\hat{y}^{(1)} & ... & \\hat{y}^{(m)} \n",
    "\\end{bmatrix}\n",
    "\\quad, \\quad\n",
    "Y =\n",
    "\\begin{bmatrix}\n",
    "y^{(1)} & ... & y^{(m)}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "***\n",
    "So:\n",
    "$$\n",
    "\\textbf{dAL} = \\frac{\\partial L}{\\partial A^{[L]}}\n",
    "= - \\frac{1}{m} ( \\frac{Y}{A^{[L]}} - \\frac{1-Y}{1-A^{[L]}} )\n",
    "= \\frac{A^{[L]}-Y}{m A^{[L]}(1-A^{[L]})}\n",
    "$$\n",
    "\n",
    "in which **dAL** represents the Python Variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def backpropagate_cost(Y, AL):\n",
    "    #m = Y.shape[1] #why not 1/m?\n",
    "    #dAL = -np.divide(Y, AL) #why not use this formula?\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    return dAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\textbf{dZL}\n",
    "= \\frac{\\partial L}{\\partial Z^{[L]}}\n",
    "= \\frac{\\partial L}{\\partial A^{[L]}} \\frac{\\partial A^{[L]}}{\\partial Z^{[L]}}\n",
    "= \\textbf{dAL} * \\frac{\\partial A^{[L]}}{\\partial Z^{[L]}}\n",
    "$$\n",
    "***\n",
    "$$\n",
    "\\frac{\\partial A^{[L]}}{\\partial Z^{[L]}}\n",
    "= A^{[L]} - (A^{[L]})^{2}\n",
    "$$\n",
    "***\n",
    "$$\n",
    "\\textbf{dZL} = \\textbf{dAL} * (\\textbf{AL} - \\textbf{AL}^{2})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagate_softmax(AL, dAL, Y=None, ZL=None):\n",
    "    dZ = np.multiply(dAL, (AL-np.power(AL,2)))\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\because\n",
    "$$\n",
    "$$\n",
    "Z^{[l]} = W^{[l]} * A^{[l-1]} + b^{[l]}\n",
    "$$\n",
    "***\n",
    "$$\n",
    "\\textbf{dZ} = \\frac{\\partial L}{\\partial Z^{[l]}}\n",
    "$$\n",
    "***\n",
    "$$\n",
    "\\therefore\n",
    "$$\n",
    "$$\n",
    "\\textbf{dA\\_prev} \n",
    "= \\frac{\\partial L}{\\partial A^{[l-1]}}\n",
    "= \\frac{\\partial L}{\\partial Z^{[l]}} \\frac{\\partial Z^{[l]}}{\\partial A^{[l-1]}}\n",
    "= \\textbf{dZ} * W^{[l]}\n",
    "$$\n",
    "***\n",
    "$$\n",
    "\\textbf{dW}\n",
    "= \\frac{\\partial L}{\\partial W^{[l]}}\n",
    "= \\frac{\\partial L}{\\partial Z^{[l]}} \\frac{\\partial Z^{[l]}}{\\partial W^{[l]}}\n",
    "= \\textbf{dZ} * A^{[l-1]}\n",
    "$$\n",
    "***\n",
    "$$\n",
    "\\textbf{db}\n",
    "= \\frac{\\partial L}{\\partial b^{[l]}}\n",
    "= \\frac{\\partial L}{\\partial Z^{[l]}} \\frac{\\partial Z^{[l]}}{\\partial b^{[l]}}\n",
    "= \\textbf{dZ} * 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backpropagate_linear(dZ, W, A_prev):\n",
    "    m = dZ.shape[1] # I still don't know why move 1/m here from backpropagate_cost.\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    dW = 1/m * np.dot(dZ, A_prev.T)\n",
    "    db = 1/m * np.sum(dZ, axis=1).reshape(-1,1)\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\because\n",
    "$$\n",
    "\n",
    "$$\n",
    "A^{[l]} = ReLU(Z^{[l]}) = max(0, Z^{[l]})\n",
    "$$\n",
    "***\n",
    "$$\n",
    "\\therefore\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\textbf{dZ}\n",
    "= \\frac{\\partial L}{\\partial Z^{[l]}}\n",
    "= \\frac{\\partial L}{\\partial A^{[l]}} \\frac{\\partial A^{[l]}}{\\partial Z^{[l]}}\n",
    "= \\textbf{dA} * ReLU^{-1}(Z^{[l]})\n",
    "= \\textbf{dA} * \n",
    "\\begin{bmatrix}\n",
    "1 (z \\geq 0)\\\\\n",
    "0 (z < 0)\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def backpropagate_ReLU(dA, Z):\n",
    "    return np.multiply(dA, (Z>=0).astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" neural network logic level \"\"\"\n",
    "\n",
    "def forwardpropagation_all(X):\n",
    "    global cache, parameters, hyper_parameters\n",
    "    layers = hyper_parameters['layers']\n",
    "    A_prev = X\n",
    "    for layer_idx, cells in enumerate(layers):\n",
    "        W = parameters['W'+str(layer_idx+1)]\n",
    "        b = parameters['b'+str(layer_idx+1)]\n",
    "        if layer_idx==len(layers)-1:\n",
    "            # Last layer's activation function should be softmax\n",
    "            Z, A = forward_propagation_each_layer(W, A_prev, b, softmax)\n",
    "        else:\n",
    "            Z, A = forward_propagation_each_layer(W, A_prev, b)\n",
    "        cache['Z'+str(layer_idx+1)] = Z\n",
    "        cache['A'+str(layer_idx+1)] = A\n",
    "        A_prev = A\n",
    "\n",
    "def backpropagate_all(X, Y):\n",
    "    global mnist, cache, parameters, hyper_parameters\n",
    "    # 1. Last layer has coss function and softmax function\n",
    "    L = str(hyper_parameters['L'])\n",
    "    cache['dA'+L] = backpropagate_cost(Y,cache['A'+L])\n",
    "    AL = cache['A'+L]\n",
    "    dAL = cache['dA'+L]\n",
    "    cache['dZ'+L] = backpropagate_softmax(AL, dAL)\n",
    "\n",
    "    # 2. Layers in between are similar: linear and ReLU\n",
    "    for i in np.arange(start=hyper_parameters['L']-1, stop=0, step=-1):\n",
    "        L = str(i)\n",
    "        cache['dA'+L], cache['dW'+str(i+1)], cache['db'+str(i+1)] = backpropagate_linear(cache['dZ'+str(i+1)], \n",
    "                                                                           parameters['W'+str(i+1)], \n",
    "                                                                           cache['A'+L])\n",
    "        cache['dZ'+L] = backpropagate_ReLU(cache['dA'+L], cache['Z'+L])\n",
    "\n",
    "    # 3. First layer has different parameter.\n",
    "    _, cache['dW1'], cache['db1'] = backpropagate_linear(cache['dZ1'], parameters['W1'], X)\n",
    "\n",
    "def update_parameters():\n",
    "    global cache, parameters\n",
    "    for i in range(hyper_parameters['L']):\n",
    "        L = str(i+1)\n",
    "        parameters['W'+L] -= cache['dW'+L] * hyper_parameters['learning_rate']\n",
    "        parameters['b'+L] -= cache['db'+L] * hyper_parameters['learning_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 > training error =  0.325038366892 ; accu = 0.078125\n",
      "30 > training error =  0.31792193448 ; accu = 0.1796875\n",
      "60 > training error =  0.295007319198 ; accu = 0.1796875\n",
      "90 > training error =  0.233226486235 ; accu = 0.4765625\n",
      "120 > training error =  0.149351638568 ; accu = 0.8203125\n",
      "150 > training error =  0.0862896958758 ; accu = 0.9296875\n",
      "180 > training error =  0.0488487100112 ; accu = 0.984375\n",
      "210 > training error =  0.028338181132 ; accu = 1.0\n",
      "240 > training error =  0.0171974913036 ; accu = 1.0\n",
      "270 > training error =  0.0111907814449 ; accu = 1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEFCAYAAAAPCDf9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl4VdW9//H392QiCSQESJiTMIYpIhKw4gBVwQlnbbXt\ndb5YrRSHqrVOdaBWr9aixYH2VutstSo/5VpFmRRRCYgi84zMIQwJCSHT+v1xDjRiQk4gyT7D5/U8\neXKyz9on3/UEPnuftddZ25xziIhIdPB5XYCIiDQfhb6ISBRR6IuIRBGFvohIFFHoi4hEEYW+iEgU\nUehLWDGzjmb2hpltMrO1Zvb8Eb5ev4a8hpl9ZWY/P2jbO2Z2fT37XW9mK81sr5mNPsxyRY6YQl/C\nhpnFAO8DXwBdnHPZwINH+LIZwIAGtH8bOKdGTYnAKcBbh9rJOfeUc64n/tpFPKPQl3AyHKh2zj3q\nnKsGcM6tBDCzFmY2MXA2vcrM7t2/k5nlmNkMM1tqZuvM7JzA9k+Al4EBgf1WmtkV9dTwNnC6mcUH\nfh4FLHDObTGzlmY2J/D715jZBDOzQ72YmY0ws/waPz9qZr8PPG5rZm+a2RIzW7i/bpEjEet1ASIN\n0BeYV8dzvwXaADlACvCpmX3lnPt/wB3Ae865RwHMLBbAOXeimY0AHnXO5QVTgHNuoZkVACOAD4Fz\ngTcDT+8FTnPOFQUOCl8DQzn8s/s/A687594wsyxgrpllO+dKD/P1RHSmL2ElAahr3ZCzgInOuSrn\n3E7gNfzBDP4hoRvN7H4zy3HOVR5hHW8D55qZL/B79w/txADjzexb/IGfBaQfwe85M/B6S4EP8Pc9\n8wheT0ShL2FlFTCwjudi+P4BwQHlAM6514ET8Z+Jf2BmvzqoXUPtH9cfBqxxzn0X2H4N0BsY7Jzr\nC3wZxGsd6vfHACOdc30CX+2dc0sPo16RAxT6Ek7+DbQ3s2v3j5WbWXbguZnANebXGrgUmBJo08o5\nt8Y59xD+C78ja7xmAZBlZqmBtoccgw/4HP/Q6F3Av2psbw+sdc7tM7OTgaODeK0CoLOZxZhZZ+DC\nGs99Avwm8I4CM0sK4vVEDkmhL2HDObcPf2CfB6wxs1XA/gu2vwcS8b8b+Bz/UM8ngecmBi6srgKu\nBO6p8ZqL8V/MXWxmq4ErgqijGpgMnMZ/xvMBngOGBn7Pr4G5+58ws6fNbCVwLDApcNE4KfD7pwJf\nBfb/uMbrXQ90A9aa2RLgjfpqE6mPaWllEZHooTN9EZEootAXEYkiCn0RkSii0BcRiSIh94ncdu3a\nuezsbK/LEBEJK/PmzdvunKv3w4AhF/rZ2dnk5+fX31BERA4ws3XBtNPwjohIFFHoi4hEEYW+iEgU\nUeiLiEQRhb6ISBRR6IuIRBGFvohIFAm5efqHa+7aHXyyYjtJ8TF0SUsku20ymW2TSGkR53VpIiIh\nI2JCf/66nTzx8YofbE9LiiOrbTJZbZPIapN04HFm2yTSWyYQ3D0zREQiQ8itp5+Xl+cO9xO5zjlK\nyqtYX1jK+h0lrCssZW2Nx5t27aW6RneT4mPIbJPkPyAcODD4v3dMbUFsjEa/RCQ8mNk851xefe0i\n5kwfwMxomRBLv04p9OuU8oPnyyur2bCzlHU7SllfWMrawhLWF5ayqqCE6csKKK+sPtA2Lsbokpb0\n/YNCmyRyOrSiS1qi3iGISFiKqNCvT3ysj+7pLeme3vIHz1VXO7YUlbGusJR1hSUHDgzrdpQwf91O\nivdVHmib0sJ/YOnfKZV+HVM4OrM13dsl60AgIiEvqkL/UHw+o1PrRDq1TuS4Hm2/95xzjp2lFazZ\nXsLSLUUs3lTEok1FvPzFOsoq/O8O2iTHMzgrjSHZaRzXvR39O6Xg8+kgICKhRaEfBDOjTXL8gWDf\nr6rasbpgD/PX72Tu2p3kr93B1MVbAWjXMoGTerdjeO90RvTOIDVJs4hExHsRdSE3FGwrKuPTlduZ\nsayAWSsK2FVaQVyMMbx3OmcP7MTIfu1JitexVkQaV7AXchX6Taiq2vH1hl28v3Az7369mS1FZSTG\nxXD6gA784kdZHJPZWtcBRKRRKPRDTHW1Y+7aHUz+ehPvLthE8b5K+ndK4bLjsjhnYGcS42O8LlFE\nwphCP4SV7Kvk7a828uKcdSzbWkyb5HiuPqEblx2XRSt9glhEDoNCPww45/hizQ6embmKGcsKSE2M\n46rju3HF8dmkJir8RSR4Cv0w882GXTw5bSVTF28lpUUsY0/uxWXDskiI1bCPiNRPoR+mFm8q4pEP\nljJjWQFd2yRy++l9OCu3oy74isghBRv6WlwmxPTrlMLzVw7lxauHkhwfyw2vfMVlf/+SdYUlXpcm\nIhGg3tA3sxgzm2hmM83sMzPLraNduplNNbPpgbadAttHm9lsM/vSzG5q7A5EqhN7pTPl1ydy3zn9\n+Wr9LkY9PouJ01dSWVVd/84iInUI5kz/IiDWOTccuAt4rLZGzrkC59xI59yPgVVArpklAY8CZwDD\ngOvMLLNxSo98MT7j8mHZfHTzcE7uk8H/fLCMi5+dw9rtOusXkcMTTOgPA6aY2RBgHJBTV0Mzu9rM\nFgL9gc8CbRcD1cAzQAkwqJb9xphZvpnlFxQUNLwXEa5Dague/sVgnrx0EKu27eHMJz7h1S/XE2rX\nY0Qk9AU7pn8+MAr/WX9lXY2cc//rnMsF/gGMD2xuj/9s/w/AtDr2m+Scy3PO5aWnpwdbe9Q5e2An\nPrjpJAZltuaOtxZy8z+/Zm95lddliUgYCSb05wHOOTceGAwsATCzsWY2oY59dgF7gRVAV+BWYAMw\nAph7hDVHtY6pibx41bHcMrI37yzYyPlPzdZwj4gELZjQfxWIN7NZ+M/e91+MzQR67W9kZsPMbIaZ\nfYD/XcF459we4HZgeuDraefcpsbsQDTy+Yyxp/Ti+SuHsqWojLP/8imfrtjudVkiEgY0Tz/Mfbej\nlGv+kc+qgj384YJcfpLX1euSRMQDmqcfJbq2SeKN647juB5tue3Nb3jsw2W6wCsidVLoR4CUFnH8\n/YohXDKkK09OW8md73xLdbWCX0R+SHfziBBxMT4euiCXNsnxPDVjFWXlVTxy0VHExui4LiL/odCP\nIGbGbaf3ISk+hkc/XM7eiiomXDKI+FgFv4j4KQ0i0A0n9+Lu0f14/9st3PT6Aqo01CMiATrTj1BX\nn9AN5xwPTllCYnwMj1x4FD6fVuoUiXYK/Qh2zYndKdlXxeMfLScpPob7zumvJZpFopxCP8L9+pSe\nlJZX8uys1aQlxXPTyN5elyQiHlLoRzgz47dn9KGwpJwJH6+gS1oiF+sDXCJRS6EfBcyMhy7IZcvu\nMu54ayEdUxM5oVc7r8sSEQ9o9k6UiIvx8dQvjqFnRkt++dI8lm0p9rokEfGAQj+KpLSI47krh5AY\nH8N/v5DPrtJyr0sSkWam0I8yHVMTeeYXg9m8ey9jX/1Kc/hFooxCPwoNzkrj/nMH8MmK7fzPB8u8\nLkdEmpEu5EapS4dm8u3G3TwzcxW5nVM566iOXpckIs1AZ/pR7N6z+3N019b89l/fsL6w1OtyRKQZ\nKPSjWHysjycvHYQZjH11PuWV1V6XJCJNTKEf5bq2SeKRi47i6w27eeTfS70uR0SamEJfOH1ARy4/\nLou/fbqGaUu3el2OiDQhhb4AcMeZfenToRW3vbmQHSWavy8SqRT6AkCLuBge/+nRFO2t4M63F+o+\nuyIRSqEvB/TtmMLNo3rz/rdbeGfBRq/LEZEmoNCX7/nvE7uTl5XGPZMXsXHXXq/LEZFGptCX74nx\nGX/6ydFUVTvueEvDPCKRpt7QN7MYM5toZjPN7DMzy62jXYaZvR1oN9PMMgLbnzezBWY2w8yebewO\nSOPLbJvEraflMGt5AZMXbPK6HBFpRMGc6V8ExDrnhgN3AY/V0W4HMC7Qbjbw8xrP3eicG+Gcu/aI\nqpVmc9lx2QzKbM197y6icM8+r8sRkUYSTOgPA6aY2RBgHJBTWyPnXKVzbn3gx07A6sDj7cDDgTP9\nS2vb18zGmFm+meUXFBQ0rAfSJGJ8xsMXHsWefZU88N5ir8sRkUYS7Jj++cAo/Gf9lYdqaGa/Aiqc\nc5MBnHO/cc4dC5wHjDez5IP3cc5Ncs7lOefy0tPTG9QBaTq927fiuhE9eWfBJqYv2+Z1OSLSCIIJ\n/XmAc86NBwYDSwDMbKyZTajZ0MzuBboDY2p5nSpgH6BP/oSRX/24Bz0zWnLX299Ssu+Qx3sRCQPB\nhP6rQLyZzQLGAzcFtmcCvfY3MrMLgd/iPzBMN7MXAtsnmNk04C3gV865ikasX5pYQmwMD1+Yy6bd\ne3nsw+VelyMiR8hCbUpeXl6ey8/P97oMOcjv3l7I63O/472xJ9C3Y4rX5YjIQcxsnnMur752mqcv\nQbnttBxSWsRyz+RvNXdfJIwp9CUorZPi+e0ZfZi7didvzdcSDSLhSqEvQbt4cFcGZbbmofeXsHuv\nLs2IhCOFvgTN5zMeOHcAO0rKeXyqLuqKhCOFvjTIgM6p/OJHWbwwZy2LNu32uhwRaSCFvjTYLaNy\nSEuK5+53vqW6Whd1RcKJQl8aLDUxjjvO7Mv89bt4c94Gr8sRkQZQ6MthuWBQZ/Ky0njkg6UUl+mi\nrki4UOjLYfH5jHvP7k9hSTl/mbbS63JEJEgKfTlsuV1SueiYLvx99hrWbC/xuhwRCYJCX47Irafn\nEB/jY/yUJV6XIiJBUOjLEclo1YIbTu7FR0u28umK7V6XIyL1UOjLEbvqhGwy2yRx/3uLqKyq9roc\nETkEhb4csYTYGH53Zl+Wb93DK1+ur38HEfGMQl8axWn92zOsR1v+NHU5u0p1nxyRUKXQl0ZhZtxz\ndj+K9lbw549WeF2OiNRBoS+Npk+HFC4dmsmLn69jxdZir8sRkVoo9KVR3TyyN0nxMTwwZYlutiIS\nghT60qjatkzgxlN7M2t5ATOWFXhdjogcRKEvje6/fpRF93bJPDBlMRWawikSUhT60ujiY33ceVZf\nVheU8NLn67wuR0RqUOhLkzi5TwYn9mrHnz9awc4STeEUCRUKfWkSZsZdZ/WjuKyCP3+kWyuKhAqF\nvjSZnA6t+PmxWbz0xXpN4RQJEQp9aVI3BaZwPqhVOEVCQr2hb2YxZjbRzGaa2WdmlltHuwwzezvQ\nbqaZZQS2jzaz2Wb2pZnd1NgdkNDWJjmecaf0YubyAqYv2+Z1OSJRL5gz/YuAWOfccOAu4LE62u0A\nxgXazQZ+bmZJwKPAGcAw4Dozyzx4RzMbY2b5ZpZfUKC53ZHmsuOy6dYumQff0xROEa8FE/rDgClm\nNgQYB+TU1sg5V+mc27/EYidgdaDtYqAaeAYoAQbVsu8k51yecy4vPT294b2QkBYf6+POM/uyqqCE\nlzWFU8RTwY7pnw+Mwn/WX3mohmb2K6DCOTc5sKk9/rP9PwDTDrNOCXOn9PVP4XxcUzhFPBVM6M8D\nnHNuPDAYWAJgZmPNbELNhmZ2L9AdGBPYtALoCtwKbABGAHMbpXIJKzWncE74WKtwinglmNB/FYg3\ns1nAeGD/xdhMoNf+RmZ2IfBb/AeG6Wb2gnNuD3A7MD3w9bRzblMj1i9hJKdDK352rH8VzpXbNIVT\nxAsWaish5uXlufz8fK/LkCZSuGcfIx6dweCsNJ6/cqjX5YhEDDOb55zLq6+d5ulLs2rbMoFxp/Ri\nxjJN4RTxgkJfmp2mcIp4R6EvzU5TOEW8o9AXT5zSN4MTevqncOpG6iLNR6EvnjAz7hrdN7AKp6Zw\nijQXhb54puaN1DWFU6R5KPTFU/tvpD5eq3CKNAuFvnhq/xTO6csKmKEpnCJNTqEvnrvsuGyy2ybx\n4JQlmsIp0sQU+uI5/43U+7Fy2x5e+WJ9/TuIyGFT6EtIOLVvBsf3bMvjHy3XKpwiTUihLyHBzLhn\ndH+Kyyp55IOlXpcjErEU+hIycjq04sph2bw29zu+Wr/T63JEIpJCX0LKjSN7k9EqgXsmL6KqOrRW\ngBWJBAp9CSktE2K586x+LNy4m1e+1EVdkcam0JeQc/ZRHTmue1v+599LKdyzz+tyRCKKQl9Cjplx\n/7n9KS2v4o/v66KuSGNS6EtI6tW+FVef2I035m1g3rodXpcjEjEU+hKyfn1yLzqktODudxZRqU/q\nijQKhb6ErOSEWO4e3Y/Fm4t4STdbEWkUCn0JaWfmduCEnu147MPlbCsq87ockbCn0JeQtv+i7r6q\nau57d7HX5YiEPYW+hLzu6S0Z++OeTFm4mY8Wb/W6HJGwptCXsHDt8B7ktG/F3ZO/pbiswutyRMJW\nvaFvZjFmNtHMZprZZ2aWW0e7VDO7y8y2mdmIGttnmNnnge/3NWLtEkXiY308dGEuW4rKePSDZV6X\nIxK2gjnTvwiIdc4NB+4CHqujXQ9gKfB+Lc9d4pwb4Zy7t7YdzWyMmeWbWX5BQUEwdUsUOiYzjct+\nlMULn69jvhZkEzkswYT+MGCKmQ0BxgE5tTVyzs13zr0JHLxK1hbgZTObamYj69h3knMuzzmXl56e\n3oDyJdrcenofOqS04I5/LaS8UnP3RRoq2DH984FR+M/6KxvyC5xzlzjnjgeuAp5pWHki39cyIZYH\nzh3Asq3FTJq1yutyRMJOMKE/D3DOufHAYGAJgJmNNbMJDfhd1UBRw0sU+b5T+7XnrNyOPDFtJSu3\n7fG6HJGwEkzovwrEm9ksYDxwU2B7JtBrfyMzG2pm+cBo4Fkzeziw/XUz+xj4K3BFI9YuUezec/qR\nFB/Db974WuvuizSAORda/2Hy8vJcfn6+12VIGJi8YCPjXlvA7af34boRPbwuR8RTZjbPOZdXXzvN\n05ewdc7ATpwxoAOPT13O8q3FXpcjEhYU+hK2zIwHzhtAyxax3PLPr6nQSpwi9VLoS1hr1zKB8ecN\nYOHG3TwzQ7N5ROqj0Jewd0ZuR84e2Iknpq1g8SZNEBM5FIW+RIT7z+lPamI8t7zxtT60JXIICn2J\nCGnJ8Tx0QS5LNhfxp6nLvS5HJGQp9CVijOzXnkuHduXZWav4bOV2r8sRCUkKfYkod4/uR7d2ydz0\nzwXsLCn3uhyRkKPQl4iSFB/LE5cMYkdJObf/6xtC7cOHIl5T6EvEGdA5ldtP78OHi7fyypfrvS5H\nJKQo9CUiXXV8N07s1Y4H3lvMCn1aV+QAhb5EJJ/PeOwnA0mOj+XXry2grKLK65JEQoJCXyJWRqsW\nPHrxQJZsLmL8lCVelyMSEhT6EtF+3CeDMSd158XP1zF5wUavyxHxnEJfIt6tp+UwJDuNO95ayMpt\nGt+X6KbQl4gXF+PjLz87hqT4GH750nxK9jXojp8iEUWhL1GhfUoLJlwyiNUFe7jz7YWavy9RS6Ev\nUeP4nu24eWRv3lmwiRc/X+d1OSKeUOhLVLl+RE9O6ZPB/e8u5vPVhV6XI9LsFPoSVXw+4/FLjiar\nbRLXvzyfDTtLvS5JpFkp9CXqpLSI46+X5VFRVc1/vzCP0nJd2JXoodCXqNQ9vSVPXjqIZVuKuPUN\nLcwm0UOhL1FrRE4Gt5/ehykLN/PktJVelyPSLGK9LkDES2NO6s6yLcX8aepystomce7Rnb0uSaRJ\n1Xumb2YxZjbRzGaa2WdmlltHu1Qzu8vMtpnZiBrbrzGzT80s38x+2oi1ixwxM+OhC3M5tlsbbn3j\nG77QjB6JcMEM71wExDrnhgN3AY/V0a4HsBR4f/8GM+sCXA+cDJwCPGpmSUdUsUgjS4iNYdJ/5dG1\nTSJjXpzHqoI9Xpck0mSCCf1hwBQzGwKMA3Jqa+Scm++cexOoeUVsCDADSAMmASVAr4P3NbMxgXcC\n+QUFBQ3rgUgjSE2K47krhhLrM658bi6Fe/Z5XZJIkwj2Qu75wCj8Z/0Nnd/WF7gTGAssqq2Bc26S\ncy7POZeXnp7ewJcXaRyZbZP42+V5bC0q46rn57JHa/RIBAom9OcBzjk3HhgMLAEws7FmNqGefRcA\n2cBNQBn+A8Cyw65WpIkNykzjLz87hm83FXHti/nsq9TNVySyBBP6rwLxZjYLGI8/wAEyqTFUY2ZD\nzSwfGA08a2YPO+fWAH8F5uAf67/NOVfWmB0QaWwj+7XnkQuPYvbKQm58bQFV1ZrDL5HDQu1DKXl5\neS4/P9/rMkT430/X8MB7i7lkSFceuiAXM/O6JJE6mdk851xefe00T1+kDlef0I2dJeX8ZfpKUhLj\nuOOMPgp+CXsKfZFDuGVUb4rKKpg0azUxPuO203IU/BLWFPoih2Bm/P7s/lRWO56esQqfwW9GKfgl\nfCn0Rerh8xkPnjsA5xwTp6/CZ8bNI3sr+CUsKfRFguDzGePPy6W6Gp6cthIDblLwSxhS6IsEyecz\nHrrAv/TUE9NWUryvkrvP6ofPp+CX8KHQF2mA/cGfnBDL32evobiskj9ekEtsjFYpl/Cg0BdpIJ/P\nuHt0X1IT43j8o+XsKatkwqVHkxAb43VpIvXS6YnIYTAzxp3ai3tG9+Pfi7Zw1fNzKSqr8LoskXop\n9EWOwFUndOOxiwfyxeodXPz0HDbu2ut1SSKHpNAXOUIXDu7CP64ayqZdezl/4my+3bjb65JE6qTQ\nF2kEx/dsx5vXDSMuxsdPnp3DtKVbvS5JpFYKfZFGktOhFW9fP4zu6clc/Y98npqxklBb0FBEoS/S\niDJSWvDPa4/jrNyOPPLvZdzw6leUlutmLBI6FPoijSwpPpYnLx3EHWf04f2Fm7ngqc9YX1jqdVki\ngEJfpEmYGdcO78HzVw5l8+4yRj/5CR8s2uJ1WSIKfZGmdFLvdN694QSy2iZz7YvzuHfyt5RV6BaM\n4h2FvkgTy2ybxJvXHcdVx3fjH3PWceHTn7G6YI/XZUmUUuiLNIOE2BjuObsff70sjw079zL6yU95\n5Yv1mt0jzU6hL9KMRvZrz/vjTuTorq353dsLueK5uWzZXeZ1WRJFFPoizaxT60ReuvpY7junP1+s\nKWTU4zOZvGCjzvqlWSj0RTzg8xmXD8vm/XEn0TOjJeNeW8BVz89lw05N7ZSmpdAX8VC3dsm88cth\n3D26H1+s2cHIP83ir7NWU1lV7XVpEqEU+iIei/EZV5/Qjak3D2dYj7aM/78lnDtxNvlrd3hdmkSg\nekPfzGLMbKKZzTSzz8wst452yWb2ipnNMLNpZtYlsH2GmX0e+H5fY3dAJFJ0bp3I3y7P4+mfH0Ph\nnnIuemYOY1/9Sss1S6MK5s5ZFwGxzrnhZnYy8BgwqpZ2NwALnXM/M7OrgHuAMYHnLnHOrW2MgkUi\nmZlxRm5Hhuek88yMVTw7azVTF2/h2pN68MvhPUiM19255MgEM7wzDJhiZkOAcUBOPe1OB84GhgS2\nbwFeNrOpZjayth3NbIyZ5ZtZfkFBQcN6IBKBkuJjuXlUDh/fMpxT+7ZnwscrOPmxGbw+d73G++WI\nWH3TxMxsApACrAQeAZY653rU0m4y8B2wGHgZmOGcG1Tj+a6BbT/Yt6a8vDyXn5/f0H6IRLS5a3fw\n4JQlfP3dLrq1S+bGU3tx9lGd8PnM69IkRJjZPOdcXn3tgjnTnwc459x4YDCwJPALxgYOCDXbbXPO\nPQWcAcw+6HWqgaJgiheR7xuS3YZ3rh/GpP8aTEKsj3GvLeDMJz7hw0VbNL9fGiSYMf1XgVFmNguo\nAH4Z2J4J9KrRbgL+YZwZQDFwNYCZvQ60A/YBVzRK1SJRyMwY1b8Dp/Ztz7vfbOLPH61gzIvz6Nsx\nhetG9ODMAR2IjdGEPDm0eod3mpuGd0SCU1lVzdtfbeSZmatYVVBCZpskrh3enQuP6UKLOF3wjTbB\nDu8o9EXCXHW148PFW3l6xkq+3rCbdi0TuPy4LC49NpN2LRO8Lk+aiUJfJMo455izqpCnZ67ikxXb\niY/xcdZRHbl8WDZHd23tdXnSxIIN/WDG9EUkDJgZw3q2Y1jPdqzctocX56zlzXkbePurjQzs2pqf\nH5vJWbkdSU7Qf/topjN9kQhWXFbBW/M38o85a1ldUEJSfAxn5XbkJ0O6kpeVhpmmfEYKDe+IyAHO\nOeat28kb+Rt475tNlJRX0a1dMhcN7sL5gzrTqXWi1yXKEVLoi0itSssr+b+FW/hn/nd8uca/qNvg\nrDRGH9WRM3M70j6lhccVyuFQ6ItIvdYVlvDeN5t575vNLNlchBkMyWrD6IEdOb1/BzJ0AAgbCn0R\naZCV2/bwfws38943m1i+1X/j9qO6pHJKn/ac0jeD/p1SdA0ghCn0ReSwLd9azNTFW/l4yVa++m4X\nzkH7lARO7tOek/tk8KPubWjVIs7rMqUGhb6INIrte/Yxfek2pi3dxqzlBZSUVxHjMwZ2SeWEwBTR\nQZmtSYjVp4C9pNAXkUa3r7KK+et2MXvldj5duZ1vNuyi2kFiXAx52WkMyW5DXlYaR2e2Jilenwdo\nTgp9EWlyu/dW8MXqQmav3M7nq3ewfFsxzvlvAdmvYwqDs/wHgsFZaXRI1UXhpqTQF5Fmt3tvBfPX\n72Te2p3kr9vBgu92UVbhv+lLeqsEjuqcyoDOqeR2TiW3S6qmhzYiLcMgIs0uNTGOH+dk8OOcDAAq\nqqpZvKmI+et3snDjbr7duJvpy7ZRHTjXTG+VQG7nVHI6tCKnfSt6t29F9/RkrRLahBT6ItJk4mJ8\nDOzamoE1FnwrLa9k8aYiFm7czcKNu1m0sYhZywuoDBwJYnxGVtukAweB/QeC7LbJukdwI1Doi0iz\nSoqPJS+7DXnZbQ5sK6+sZm1hCcu2FLNiazHLthazdEsx/160hZoj0O1TEshum0y3dslkt0s+8Dir\nbZLeHQRJoS8inouP9R04q69pb3kVqwr2sGZ7CWu3l7C2sJS1hSVMXbyVwpLy77XNaJVAp9aJdE5L\npHNr/1en1v95nJIYqw+XodAXkRCWGB/DgMDF34MVlVWwbnspawr9B4SNO/eycddeFm8qYurirZRX\nVn+vfcuEWDqktiCjVYL/K8X/OL1VAu0DjzNSWtAywpeejuzeiUjESmkRR24X/yyggznn2L6nnE27\n/AeC/Qc+MAhOAAAF/ElEQVSELbvL2FZcRv66nWwr3veDAwNAUnzMgYNBWlI8bZLjSUuOp03gcc2f\n05LjaJkQXu8gFPoiEnHMjPRAcA+s465hzjmK9layrbiMbcX7/N+L9gUe76OguIz1O0pZ8N0udpaW\nU1FV+/T2uBgjLSmetKR4UhPjaNUilpTEOFIOfI8jJTGWlBZxtKrxOCXQNq6Zb2av0BeRqGRmpCbF\nkZoUR6+DriUczDnHnn2V7CypoLBkHztLy9lRUsHOknJ2lJb7v5eUU1xWyZaiMpZvK6ZobyXFZRUH\npqfWJSk+hpYJsbRMiOXGkb05Z2CnRuzlDyn0RUTqYWa0CpypZ7ZNCnq/6mpHSXklxWWVFJVVULS3\nkqK9FYHHFRSXVbJ7b8WBNmlJTb+InUJfRKSJ+Hz/OVh0IjTuTta8g0kiIuKpekPfzGLMbKKZzTSz\nz8wst452yWb2ipnNMLNpZtYlsP0aM/vUzPLN7KeN3QEREQleMGf6FwGxzrnhwF3AY3W0uwFY6Jwb\nAbwE3BMI/uuBk4FTgEfNLPgBMRERaVTBhP4wYIqZDQHGATn1tDsdOBsYEviaAaQBk4ASoNfBO5rZ\nmMA7gfyCgoIGd0JERIIT7Jj++cAo/Gf9lYdoNwboDlxRY1tf4E5gLLCotp2cc5Occ3nOubz09PQg\nSxIRkYYKJvTnAc45Nx4YDCwBMLOxZjbhoHbbnHNPAWcAs4EFQDZwE1CG/wCwrNGqFxGRBglmyuar\nwCgzmwVUAL8MbM/k+0M1E4CXzWwGUAxc7ZzbZmZ/BeYE9r3NOVfWWMWLiEjD6M5ZIiIRIGxvl2hm\nBcC6w9y9HbC9EcvxkvoSmtSX0KS+QJZzrt6LoiEX+kfCzPKDOdKFA/UlNKkvoUl9CZ4+kSsiEkUU\n+iIiUSTSQn+S1wU0IvUlNKkvoUl9CVJEjemLiMihRdqZvoiIHIJCX0Qkiij0RUSiSESEfrBr/oc6\nMysL3I9ghpmdb2ZDzWyWmc0xs0e9ru9QzKxFYD2mtWZ2RWDbaDObbWZfmtlNgW0ZZvZuoI/vmlmq\np4XXoo6+PG9mCwJ1PxvYVus9JEKJmSWZ2UtmNt3MvjCznNr+XYV6X+rox+/NbGmg5smBdmGRBWb2\nQaAvc8wst1n/Js65sP8Cfgo8G3h8MvCh1zUdZj/WHvTz10Bm4PEsYJjXNR6i9izgGmA8/lVWk4Cl\nQAr+NZ6W41+v6Wng0sA+9wO/87r2+voS2PY8MOKgdrcDdwQeXwVM8rr2OvrTI/B9DP41sn7w7yoc\n+lJLP36//+9To01YZQFwD3Btc/5NIuJMn+DX/A91lYGj/VtmNhioAjab2SNAPP77E4Qk59w659zf\n8C+sB/6/wWKgGngG/70UBvGfv9VlwLGEYJ9q6Qv4Pxb/cOCs69LAttruIRFynHOrAg87ATuo/d9V\nyPfloH6sBnYBN5jZJ2b268BzYZEFZnaWmeUDlwHv0Yx/k0gJfQh+zf+Q5Zzr6Zw7Cf+dxx4EWgB/\nwb/S6cte1naY2gOPAn8AptXYfiewD/iNF0UdDufcb5xzxwLnAePNLDnwVG33kAg5ZnYe/oPuU9T9\n7yrk+1KjHxOdc392/uUKRgK/MLP9AR/yWeCcmxKo/df4T4qa7W8SKaFf65r/YcyANUAc8BD++xKc\nCXzmZVENtALoCtwKbABGAHPx/60WOedeB87Cf9+FcFKF/4BVTu33kAg5ZnY1cDFwsXOugNr/XYV8\nXw7qR80wrwp8Lyb8smAXUEoz/k0i4sNZZhYHPId/zLgC+KVzboW3VTWMmWUA/8IfKNuAX+E/o3kY\n/w1opjrn7veuwkMLXGR6B/9b733AJ8D7wC2Bn59zzv3NzLoCL+A/sG0GrnHOlXhTde3q6MtOIBd/\nwDzknJsWuAj9MtCSGveQ8Kbq2plZHvAF/sCoxn+w+iMH/bsK9b7U0Y/p+M/oHfCMc+6f4ZAFZtYN\n/zWiMqAIuBH/MFSz/E0iIvRFRCQ4kTK8IyIiQVDoi4hEEYW+iEgUUeiLiEQRhb6ISBRR6IuIRBGF\nvohIFPn/RAPHFzpGYl0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc1b76bdc88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def model(X, Y, learning_rate=0.01, print_every=100, iteration=500, hidden_layers=[100], batch_size=128):\n",
    "    init(hidden_layers=hidden_layers, C=Y.shape[0], X_size=X.shape[0], learning_rate=learning_rate)\n",
    "    # costs will be returned for plotting\n",
    "    costs = []\n",
    "    X_full = X\n",
    "    Y_full = Y\n",
    "    np.random.seed(1)\n",
    "    for i in range(iteration):\n",
    "        permutation = np.random.permutation(X.shape[1])\n",
    "        X = X_full[:, permutation]\n",
    "        X = X[:, :batch_size]\n",
    "        Y = Y_full[:, permutation]\n",
    "        Y = Y[:, :batch_size]\n",
    "        \n",
    "        forwardpropagation_all(X)\n",
    "        Y_hat = cache['A'+str(hyper_parameters['L'])]\n",
    "        cost_value = cost(loss(Y_hat, Y))\n",
    "        costs.append(cost_value)\n",
    "        if i%print_every==0:\n",
    "            Y_predict = predict(Y_hat)\n",
    "            accu = accuracy(Y_predict, Y)\n",
    "            print(i,'> training error = ',cost_value,'; accu =',accu)\n",
    "        backpropagate_all(X, Y)\n",
    "        update_parameters()\n",
    "        if np.isnan(cost_value):\n",
    "            print(\"ERROR to nan!!!\")\n",
    "            break\n",
    "    return costs\n",
    "\n",
    "def test_model():\n",
    "    m = 10\n",
    "    X = np.random.randn(784,m)\n",
    "    Y = one_hot(np.random.randint(low=0, high=10, size=(m,1)), C=10)\n",
    "    #print(X[:,1])\n",
    "    #print(Y[:,1])\n",
    "    costs = model(X,Y,learning_rate=0.1, print_every=10, iteration=100,\n",
    "                 hidden_layers=[10,10])\n",
    "    plt.plot(costs)\n",
    "    plt.show()\n",
    "def test_model_1():\n",
    "    costs = model(mnist['X_train'], mnist['Y_train'], \n",
    "                  learning_rate=0.1, print_every=30, iteration=300,\n",
    "                  batch_size=128,\n",
    "                  hidden_layers=[784,1000])\n",
    "    plt.plot(costs)\n",
    "    plt.title(\"Cost Value\")\n",
    "    plt.show()\n",
    "\n",
    "test_model_1()\n",
    "#model(mnist['X_train'], mnist['Y_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache: [('A1', (784, 128)),\n",
      " ('A2', (1000, 128)),\n",
      " ('A3', (10, 128)),\n",
      " ('Z1', (784, 128)),\n",
      " ('Z2', (1000, 128)),\n",
      " ('Z3', (10, 128)),\n",
      " ('dA1', (784, 128)),\n",
      " ('dA2', (1000, 128)),\n",
      " ('dA3', (10, 128)),\n",
      " ('dW1', (784, 784)),\n",
      " ('dW2', (1000, 784)),\n",
      " ('dW3', (10, 1000)),\n",
      " ('dZ1', (784, 128)),\n",
      " ('dZ2', (1000, 128)),\n",
      " ('dZ3', (10, 128)),\n",
      " ('db1', (784, 1)),\n",
      " ('db2', (1000, 1)),\n",
      " ('db3', (10, 1))]\n",
      "parameters: [('W1', (784, 784)),\n",
      " ('W2', (1000, 784)),\n",
      " ('W3', (10, 1000)),\n",
      " ('b1', (784, 1)),\n",
      " ('b2', (1000, 1)),\n",
      " ('b3', (10, 1))]\n",
      "hyper_parameters: ['C', 'L', 'X_size', 'layers', 'learning_rate']\n"
     ]
    }
   ],
   "source": [
    "debug_show_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
