{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todo list\n",
    "\n",
    "1. one_hot √\n",
    "\n",
    "* mini-batch\n",
    "\n",
    "* normalization\n",
    "\n",
    "* train/dev/test set √\n",
    "\n",
    "* linear function √\n",
    "\n",
    "* sigmoid function\n",
    "\n",
    "* tanh function\n",
    "\n",
    "* relu function √\n",
    "\n",
    "* softmax function √\n",
    "\n",
    "* loss function √\n",
    "\n",
    "* cost function\n",
    "\n",
    "* regularization\n",
    "\n",
    "* drop out\n",
    "\n",
    "* batch normalization\n",
    "\n",
    "* momentum\n",
    "\n",
    "* exponentially moving average\n",
    "\n",
    "* Adam\n",
    "\n",
    "* all backpropagation of above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\"\"\" helper functions \"\"\"\n",
    "def names_in(dictionary):\n",
    "    \"\"\" list all names in a dictionary \"\"\"\n",
    "    print([name for name,_ in sorted(dictionary.items())])\n",
    "def names_shape_in(dictionary):\n",
    "    pprint([(name, val.shape) for name,val in sorted(dictionary.items())])\n",
    "\n",
    "def debug_show_all_variables():\n",
    "    print(\"cache: \", end='')\n",
    "    names_shape_in(cache)\n",
    "    print(\"parameters: \", end='')\n",
    "    names_shape_in(parameters)\n",
    "    print(\"hyper_parameters: \", end='')\n",
    "    names_in(hyper_parameters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_mnist():\n",
    "    import struct\n",
    "    from array import array\n",
    "    \"\"\" \n",
    "    load MNIST dataset into numpy array \n",
    "    MNIST dataset can be downloaded manually.\n",
    "    url: http://yann.lecun.com/exdb/mnist/\n",
    "    \"\"\"\n",
    "    ret = {}\n",
    "    with open('MNIST/train-images.idx3-ubyte', 'rb') as f:\n",
    "        magic, size, rows, cols = struct.unpack(\">IIII\", f.read(16))\n",
    "        assert(magic==2051)\n",
    "        ret['X_train'] = np.array(array(\"B\", f.read())).reshape(size,rows,cols)\n",
    "\n",
    "    with open('MNIST/t10k-images.idx3-ubyte', 'rb') as f:\n",
    "        magic, size, rows, cols = struct.unpack(\">IIII\", f.read(16))\n",
    "        assert(magic==2051)\n",
    "        ret['X_test'] = np.array(array(\"B\", f.read())).reshape(size,rows,cols)\n",
    "\n",
    "    with open('MNIST/train-labels.idx1-ubyte', 'rb') as f:\n",
    "        magic, size = struct.unpack(\">II\", f.read(8))\n",
    "        assert(magic==2049)\n",
    "        ret['Y_train'] = np.array(array(\"B\", f.read())).reshape(size,1)\n",
    "\n",
    "    with open('MNIST/t10k-labels.idx1-ubyte', 'rb') as f:\n",
    "        magic, size = struct.unpack(\">II\", f.read(8))\n",
    "        assert(magic==2049)\n",
    "        ret['Y_test'] = np.array(array(\"B\", f.read())).reshape(size,1)\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (50000, 28, 28) X_dev: (10000, 28, 28) X_test: (10000, 28, 28)\n",
      "Y_train: (50000, 1) Y_dev: (10000, 1) Y_test: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "mnist_original = load_mnist()\n",
    "\n",
    "\"\"\" random shuffle the training set \"\"\"\n",
    "np.random.seed(1)\n",
    "permutation = np.random.permutation(mnist_original['X_train'].shape[0])\n",
    "mnist_original['X_train'] = mnist_original['X_train'][permutation]\n",
    "mnist_original['Y_train'] = mnist_original['Y_train'][permutation]\n",
    "\n",
    "\"\"\" divide trainset into trainset and devset \"\"\"\n",
    "len_of_dev = 10000\n",
    "mnist_original['X_dev'] = mnist_original['X_train'][:len_of_dev]\n",
    "mnist_original['Y_dev'] = mnist_original['Y_train'][:len_of_dev]\n",
    "mnist_original['X_train'] = mnist_original['X_train'][len_of_dev:]\n",
    "mnist_original['Y_train'] = mnist_original['Y_train'][len_of_dev:]\n",
    "\n",
    "print('X_train:', mnist_original['X_train'].shape,\n",
    "      'X_dev:', mnist_original['X_dev'].shape,\n",
    "      'X_test:', mnist_original['X_test'].shape)\n",
    "print('Y_train:', mnist_original['Y_train'].shape,\n",
    "      'Y_dev:', mnist_original['Y_dev'].shape,\n",
    "      'Y_test:', mnist_original['Y_test'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def manually_validate_dataset(dataset):\n",
    "    random_train = np.random.randint(1, len(dataset['X_train']))-1\n",
    "    random_dev = np.random.randint(1, len(dataset['X_dev']))-1\n",
    "    random_test = np.random.randint(1, len(dataset['X_test']))-1\n",
    "    print(dataset['Y_train'][random_train], dataset['Y_dev'][random_dev], dataset['Y_test'][random_test])\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1,3, sharey=True, figsize=[10,3])\n",
    "    ax1.imshow(dataset['X_train'][random_train], cmap='gray')\n",
    "    ax2.imshow(dataset['X_dev'][random_dev], cmap='gray')\n",
    "    ax3.imshow(dataset['X_test'][random_test], cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "#manually_validate_dataset(mnist_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ X = \n",
    "\\begin{bmatrix}\n",
    "\\vert & & \\vert & & \\vert \\\\\n",
    "x^{(1)} & ... & x^{(i)} & ... & x^{(m)} \\\\\n",
    "\\vert & & \\vert & & \\vert\n",
    "\\end{bmatrix} \n",
    "\\quad \\quad\n",
    "Y = \n",
    "\\begin{bmatrix}\n",
    "\\vert & & \\vert & & \\vert \\\\\n",
    "y_{one\\_hot}^{(1)} & ... & y_{one\\_hot}^{(i)} & ... & y_{one\\_hot}^{(m)} \\\\\n",
    "\\vert & & \\vert & & \\vert \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 50000) (10, 50000)\n"
     ]
    }
   ],
   "source": [
    "mnist = {}\n",
    "\n",
    "\"\"\" X is 28*28 image \"\"\"\n",
    "def flatten(X):\n",
    "    \"\"\" prepare X to (nx, m) shape \"\"\"\n",
    "    X = X.reshape(-1, 28*28).T\n",
    "    return X\n",
    "\n",
    "mnist['X_train'] = flatten(mnist_original['X_train'])\n",
    "mnist['X_dev'] = flatten(mnist_original['X_dev'])\n",
    "mnist['X_test'] = flatten(mnist_original['X_test'])\n",
    "\n",
    "\"\"\" Y is label 0-9 \"\"\"\n",
    "def one_hot(Y, C):\n",
    "    \"\"\" prepare Y to (1, m) shape \"\"\"\n",
    "    assert(Y.shape[1]==1)\n",
    "    Y_ret = np.zeros((Y.shape[0], C))\n",
    "    Y_ret[np.arange(Y.shape[0]), Y.reshape(-1).astype(int)] = 1\n",
    "    Y_ret = Y_ret.T\n",
    "    return Y_ret\n",
    "\n",
    "def test_one_hot():\n",
    "    Y = np.ones((5,1))\n",
    "    Y = one_hot(Y, 10)\n",
    "    assert(Y[0,0]==0)\n",
    "    assert(Y[1,0]==1)\n",
    "    assert(Y[2,1]==0)\n",
    "\n",
    "def back_one_hot(Y):\n",
    "    \"\"\" convert one hot Y back to real number \"\"\"\n",
    "    Y_ret = np.repeat( [np.arange(Y.shape[0])], repeats=Y.shape[1], axis=0 )\n",
    "    assert(Y_ret.shape == Y.T.shape)\n",
    "    Y_ret = Y_ret[Y.T.astype(bool)]\n",
    "    return Y_ret.reshape(-1,1)\n",
    "\n",
    "mnist['Y_train'] = one_hot(mnist_original['Y_train'], 10)\n",
    "mnist['Y_dev'] = one_hot(mnist_original['Y_dev'], 10)\n",
    "mnist['Y_test'] = one_hot(mnist_original['Y_test'], 10)\n",
    "\n",
    "print(mnist['X_train'].shape, mnist['Y_train'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" init cache, parameters and hyper_parameters \"\"\"\n",
    "def init(X_size, hidden_layers=[10,10,5], C=10, learning_rate=0.01):\n",
    "    global cache, parameters, hyper_parameters\n",
    "    cache = {}\n",
    "    parameters = {}\n",
    "    hyper_parameters = {}\n",
    "    initialize_hyper_parameters(X_size, hidden_layers, C, learning_rate)\n",
    "    initialize_parameters()\n",
    "\n",
    "def initialize_hyper_parameters(X_size, hidden_layers, C, learning_rate):\n",
    "    global hyper_parameters\n",
    "    hyper_parameters['X_size'] = X_size\n",
    "    # layers of network, include the last softmax layer\n",
    "    # hidden layers use ReLU activation function, and the last layer use Softmax function\n",
    "    hyper_parameters['layers'] = hidden_layers + [C]\n",
    "    # L layers in total\n",
    "    hyper_parameters['L'] = len(hyper_parameters['layers'])\n",
    "    # Class numbers 0-9 is 10\n",
    "    hyper_parameters['C'] = C\n",
    "    hyper_parameters['learning_rate'] = learning_rate\n",
    "\n",
    "def initialize_parameters():\n",
    "    \"\"\" init W b or any other parameters in every layer \"\"\"\n",
    "    global parameters, hyper_parameters\n",
    "    layers = hyper_parameters['layers']\n",
    "    x_size = hyper_parameters['X_size']\n",
    "    cells_prev = x_size\n",
    "    for layer_idx, cells in enumerate(layers):\n",
    "        parameters['W'+str(layer_idx+1)] = np.random.randn(cells, cells_prev) * 0.01\n",
    "        parameters['b'+str(layer_idx+1)] = np.zeros((cells, 1))\n",
    "        cells_prev = layers[layer_idx]\n",
    "\n",
    "def test_init():\n",
    "    init(50)\n",
    "    debug_show_all_variables()\n",
    "#test_init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "ReLu = max(0, x)\n",
    "\\quad \\quad\n",
    "Softmax = \\frac{\\exp(Z)}{\\sum_i^n{\\exp(Z)}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "Z = W \\dot X + b\n",
    "\\quad \\quad\n",
    "A = active(Z)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(X):\n",
    "    return X * (X > 0)\n",
    "\n",
    "def test_ReLU():\n",
    "    X = np.array([[1., 2., -2., -3.], [-2, -2, 1, 0.1]])\n",
    "    Y = np.array([[1., 2., 0., 0.], [0, 0, 1, 0.1]])\n",
    "    bias = np.sum(np.abs(Y - ReLU(X)))\n",
    "    assert(bias<0.0001)\n",
    "#test_ReLU()\n",
    "\n",
    "def softmax(X):\n",
    "    s = np.sum(np.exp(X), axis=0)\n",
    "    return np.exp(X) / s\n",
    "\n",
    "def test_softmax():\n",
    "    X = np.array([-3.44,1.16,-0.81,3.91])\n",
    "    Y = np.array([0.0006, 0.0596, 0.0083, 0.9315])\n",
    "    bias = np.sum(np.abs(Y - softmax(X)))\n",
    "    assert(bias<0.0001)\n",
    "    X = np.array([[1.,2,3,4],[2,2,3,4],[5,5,5,4]])\n",
    "    Y = np.array([[ 0.01714783,  0.0452785 ,  0.10650698,  0.33333333],\n",
    "       [ 0.04661262,  0.0452785 ,  0.10650698,  0.33333333],\n",
    "       [ 0.93623955,  0.909443  ,  0.78698604,  0.33333333]])\n",
    "    bias = np.sum(np.abs(Y - softmax(X)))\n",
    "    assert(bias<1e-5)\n",
    "#test_softmax()\n",
    "\n",
    "def forward_propagation(X):\n",
    "    global cache, parameters, hyper_parameters\n",
    "    layers = hyper_parameters['layers']\n",
    "    A = X\n",
    "    for layer_idx, cells in enumerate(layers):\n",
    "        Z = np.dot(parameters['W'+str(layer_idx+1)], A)\n",
    "        if layer_idx<len(layers)-1:\n",
    "            \"\"\" normal layers use relu \"\"\"\n",
    "            A = ReLU(Z)\n",
    "        else:\n",
    "            \"\"\" last layers use softmax \"\"\"\n",
    "            A = softmax(Z)\n",
    "        cache['Z'+str(layer_idx+1)] = Z\n",
    "        cache['A'+str(layer_idx+1)] = A\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "L(\\hat{y}, y) = -\\frac{1}{m} \\sum_i^m{(y_i\\log(\\hat{y}_i) + (1-y_i)\\log(1-\\hat{y}_i))}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "L(\\hat{y}, y) = -\\frac{1}{m} \\sum_i^m \\sum_j^C (y_j^{(i)}\\log(\\hat{y}_j^{(i)}))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where m is batch size, C is class number (10 classes), (i) is the i-th example, j is the vertical bit of result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(Y_hat, Y):\n",
    "    m = Y.shape[1]\n",
    "    A = np.multiply(Y, np.log(Y_hat))\n",
    "    C = -np.sum(A) / m\n",
    "    return C\n",
    "\n",
    "def test_loss():\n",
    "    Y = np.asarray([[0., 1., 1.], [1., 0., 0.]])\n",
    "    aL = np.array([[.8, .9, .4], [.2, .1, .6]])\n",
    "    assert(loss(aL, Y) - 0.87702971 < 1e-7)\n",
    "    \n",
    "    z = np.array([[1.,2,3,4],[2,2,3,4],[5,5,5,4]])\n",
    "    y = np.array([[0.,0,1],[0,0,1],[0,0,1],[1,0,0]]).T\n",
    "    bias = 0.37474097876709611 - loss(softmax(z), y)\n",
    "    assert(np.abs(bias)<1e-5)\n",
    "#test_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(Y_hat):\n",
    "    return np.argmax(Y_hat,axis=0).reshape(-1,1)\n",
    "\n",
    "def test_predict():\n",
    "    Y = np.array([[0.9,0.01,0.01,0.01,0.02,0.01,0.01,0.01,0.01,0.01],\n",
    "                  [0.01,0.01,0.01,0.02,0.9,0.01,0.01,0.01,0.01,0.01]\n",
    "                 ]).T\n",
    "    print(Y.shape)\n",
    "    Y_result = predict(Y)\n",
    "    print(Y_result)\n",
    "    Z = np.array([0,4]).reshape(2,1)\n",
    "    assert(np.sum(Y_result != Z)==0)\n",
    "#test_predict()\n",
    "\n",
    "def accuracy(Y_predict, Y):\n",
    "    return np.sum(np.equal(Y_predict, back_one_hot(Y))) / Y_predict.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already know L formula above:\n",
    "$$\n",
    "L = -\\frac{1}{m} \\sum_i^{m}[y^{(i)} log(\\hat{y}^{(i)}) + (1-y^{(i)}) log(1-\\hat{y}^{(i)})]\n",
    "$$\n",
    "\n",
    "$$\n",
    "A^{[L]} = \\hat{Y} = \n",
    "\\begin{bmatrix}\n",
    "\\hat{y}^{(1)} & ... & \\hat{y}^{(m)} \n",
    "\\end{bmatrix}\n",
    "\\quad, \\quad\n",
    "Y =\n",
    "\\begin{bmatrix}\n",
    "y^{(1)} & ... & y^{(m)}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "So:\n",
    "$$\n",
    "\"dAL\" = \\frac{\\partial L}{\\partial A^{[L]}}\n",
    "= - \\frac{1}{m} ( \\frac{Y}{A^{[L]}} - \\frac{1-Y}{1-A^{[L]}} )\n",
    "= \\frac{A^{[L]}-Y}{m A^{[L]}(1-A^{[L]})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def backpropagate_cost(Y, AL):\n",
    "    #m = Y.shape[1] #why not 1/m?\n",
    "    m=1\n",
    "    return -1/m * (np.divide(Y, AL) - np.divide(1-Y,1-AL))\n",
    "\n",
    "def test_backpropagation_cost(epslon=1e-7):\n",
    "    \"\"\" TODO: implement the gradient check \"\"\"\n",
    "#test_backpropagation_cost()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\"dZL\"\n",
    "= \\frac{\\partial L}{\\partial Z^{[L]}}\n",
    "= \\frac{\\partial L}{\\partial A^{[L]}} \\frac{\\partial A^{[L]}}{\\partial Z^{[L]}}\n",
    "= \"dAL\" * \\frac{\\partial A^{[L]}}{\\partial Z^{[L]}}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial A^{[L]}}{\\partial Z^{[L]}}\n",
    "= A^{[L]} - (A^{[L]})^{2}\n",
    "$$\n",
    "$$\n",
    "\"dZL\" = \"dAL\" * (\"AL\" - \"AL\"^{2})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagate_softmax(AL, dAL):\n",
    "    return np.multiply(dAL, (AL-np.power(AL,2)))\n",
    "\n",
    "def test_backpropagation_softmax():\n",
    "    z = np.array([[1.,2,3,4],[2,2,3,4],[5,5,5,4]])\n",
    "    y = np.array([[0.,0,1],[0,0,1],[0,0,1],[1,0,0]]).T\n",
    "    a = softmax(z)\n",
    "    l = loss(a, y)\n",
    "    da = backpropagate_cost(y, a)\n",
    "    dz = backpropagate_softmax(a, da)\n",
    "    result = np.array([[ 0.01714783,0.0452785,0.10650698,-0.66666667],\n",
    "                     [ 0.04661262,0.0452785,0.10650698,0.33333333],\n",
    "                     [-0.06376045,-0.090557,-0.21301396,0.33333333]])\n",
    "    assert(np.sum(np.abs(result - dz))<1e-6)\n",
    "#test_backpropagation_softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def backpropagate_linear(dZ, W, A_prev):\n",
    "    m = dZ.shape[1]\n",
    "    dA = np.dot(W.T, dZ)\n",
    "    dW = 1/m * np.dot(dZ, A_prev.T)\n",
    "    db = 1/m * np.sum(dZ, axis=1).reshape(-1,1)\n",
    "    return dA, dW, db\n",
    "\n",
    "def backpropagate_ReLU(dA, Z):\n",
    "    return np.multiply(dA, (Z>=0).astype(np.float32))\n",
    "\n",
    "def test_backpropagation_linear():\n",
    "    \"\"\" TODO \"\"\"\n",
    "    A_prev = np.array([[1,2],[2,2],[3,2]])\n",
    "    W = \n",
    "    pass\n",
    "\n",
    "def test_backpropagation_ReLU():\n",
    "    \"\"\" TODO \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backpropagate_all():\n",
    "    global mnist, cache, parameters, hyper_parameters\n",
    "    # 1. Last layer has coss function and softmax function\n",
    "    L = str(hyper_parameters['L'])\n",
    "    cache['dA'+L] = backpropagate_cost(mnist['Y_train'],cache['A'+L])\n",
    "    AL = cache['A'+L]\n",
    "    dAL = cache['dA'+L]\n",
    "    cache['dZ'+L] = backpropagate_softmax(AL, dAL)\n",
    "\n",
    "    # 2. Layers in between are similar: linear and ReLU\n",
    "    for i in np.arange(start=hyper_parameters['L']-1, stop=0, step=-1):\n",
    "        L = str(i)\n",
    "        cache['dA'+L], cache['dW'+str(i+1)], cache['db'+str(i+1)] = backpropagate_linear(cache['dZ'+str(i+1)], \n",
    "                                                                           parameters['W'+str(i+1)], \n",
    "                                                                           cache['A'+L])\n",
    "        cache['dZ'+L] = backpropagate_ReLU(cache['dA'+L], cache['Z'+L])\n",
    "\n",
    "    # 3. First layer has different parameter.\n",
    "    _, cache['dW1'], cache['db1'] = backpropagate_linear(cache['dZ1'], parameters['W1'], mnist['X_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters():\n",
    "    global cache, parameters\n",
    "    for i in range(hyper_parameters['L']):\n",
    "        L = str(i+1)\n",
    "        parameters['W'+L] -= cache['dW'+L] * hyper_parameters['learning_rate']\n",
    "        parameters['b'+L] -= cache['db'+L] * hyper_parameters['learning_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 > 3.60142930093 ; accu: 0.10662\n",
      "1 > 10.1497381687 ; accu: 0.20242\n",
      "2 > 22.6295464351 ; accu: 0.1757\n",
      "3 > 11.1491189949 ; accu: 0.22128\n",
      "4 > 4.49205669956 ; accu: 0.20412\n",
      "5 > 2.26513506491 ; accu: 0.16568\n",
      "6 > 2.16754719378 ; accu: 0.23076\n",
      "7 > 2.0562889961 ; accu: 0.30354\n",
      "8 > 1.97480817826 ; accu: 0.28832\n",
      "9 > 1.8522214557 ; accu: 0.30446\n",
      "10 > 1.77600917343 ; accu: 0.3342\n",
      "11 > 3.19505777056 ; accu: 0.29446\n",
      "12 > 5.22995157641 ; accu: 0.1263\n",
      "13 > 6.18417790274 ; accu: 0.20616\n",
      "14 > 2.20976338922 ; accu: 0.20908\n",
      "15 > 2.05161577518 ; accu: 0.31778\n",
      "16 > 2.11779477663 ; accu: 0.26952\n",
      "17 > 1.937347358 ; accu: 0.29524\n",
      "18 > 2.05581713001 ; accu: 0.2624\n",
      "19 > 1.9971302847 ; accu: 0.246\n",
      "20 > 2.01954391376 ; accu: 0.25066\n",
      "21 > 1.83482643227 ; accu: 0.31578\n",
      "22 > 1.9195396734 ; accu: 0.27142\n",
      "23 > 1.76081143961 ; accu: 0.31652\n",
      "24 > 1.71360316115 ; accu: 0.36064\n",
      "25 > 1.5566262188 ; accu: 0.39964\n",
      "26 > 2.21935654167 ; accu: 0.39012\n",
      "27 > 3.68112808289 ; accu: 0.32994\n",
      "28 > 4.05900419912 ; accu: 0.26832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liusida/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:4: RuntimeWarning: invalid value encountered in true_divide\n",
      "/home/liusida/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:2: RuntimeWarning: invalid value encountered in greater\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/liusida/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:9: RuntimeWarning: invalid value encountered in greater_equal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 > nan ; accu: 0.09942\n",
      "30 > nan ; accu: 0.09942\n",
      "31 > nan ; accu: 0.09942\n",
      "32 > nan ; accu: 0.09942\n",
      "33 > nan ; accu: 0.09942\n",
      "34 > nan ; accu: 0.09942\n",
      "35 > nan ; accu: 0.09942\n",
      "36 > nan ; accu: 0.09942\n",
      "37 > nan ; accu: 0.09942\n",
      "38 > nan ; accu: 0.09942\n",
      "39 > nan ; accu: 0.09942\n",
      "40 > nan ; accu: 0.09942\n",
      "41 > nan ; accu: 0.09942\n",
      "42 > nan ; accu: 0.09942\n",
      "43 > nan ; accu: 0.09942\n",
      "44 > nan ; accu: 0.09942\n",
      "45 > nan ; accu: 0.09942\n",
      "46 > nan ; accu: 0.09942\n",
      "47 > nan ; accu: 0.09942\n",
      "48 > nan ; accu: 0.09942\n",
      "49 > nan ; accu: 0.09942\n",
      "50 > nan ; accu: 0.09942\n",
      "51 > nan ; accu: 0.09942\n",
      "52 > nan ; accu: 0.09942\n",
      "53 > nan ; accu: 0.09942\n",
      "54 > nan ; accu: 0.09942\n",
      "55 > nan ; accu: 0.09942\n",
      "56 > nan ; accu: 0.09942\n",
      "57 > nan ; accu: 0.09942\n",
      "58 > nan ; accu: 0.09942\n",
      "59 > nan ; accu: 0.09942\n",
      "60 > nan ; accu: 0.09942\n",
      "61 > nan ; accu: 0.09942\n",
      "62 > nan ; accu: 0.09942\n",
      "63 > nan ; accu: 0.09942\n",
      "64 > nan ; accu: 0.09942\n",
      "65 > nan ; accu: 0.09942\n",
      "66 > nan ; accu: 0.09942\n",
      "67 > nan ; accu: 0.09942\n",
      "68 > nan ; accu: 0.09942\n",
      "69 > nan ; accu: 0.09942\n",
      "70 > nan ; accu: 0.09942\n",
      "71 > nan ; accu: 0.09942\n",
      "72 > nan ; accu: 0.09942\n",
      "73 > nan ; accu: 0.09942\n",
      "74 > nan ; accu: 0.09942\n",
      "75 > nan ; accu: 0.09942\n",
      "76 > nan ; accu: 0.09942\n",
      "77 > nan ; accu: 0.09942\n",
      "78 > nan ; accu: 0.09942\n",
      "79 > nan ; accu: 0.09942\n",
      "80 > nan ; accu: 0.09942\n",
      "81 > nan ; accu: 0.09942\n",
      "82 > nan ; accu: 0.09942\n",
      "83 > nan ; accu: 0.09942\n",
      "84 > nan ; accu: 0.09942\n",
      "85 > nan ; accu: 0.09942\n",
      "86 > nan ; accu: 0.09942\n",
      "87 > nan ; accu: 0.09942\n",
      "88 > nan ; accu: 0.09942\n",
      "89 > nan ; accu: 0.09942\n",
      "90 > nan ; accu: 0.09942\n",
      "91 > nan ; accu: 0.09942\n",
      "92 > nan ; accu: 0.09942\n",
      "93 > nan ; accu: 0.09942\n",
      "94 > nan ; accu: 0.09942\n",
      "95 > nan ; accu: 0.09942\n",
      "96 > nan ; accu: 0.09942\n",
      "97 > nan ; accu: 0.09942\n",
      "98 > nan ; accu: 0.09942\n",
      "99 > nan ; accu: 0.09942\n"
     ]
    }
   ],
   "source": [
    "def model(X, Y, learning_rate=0.01):\n",
    "    init(hidden_layers=[100], C=Y.shape[0], X_size=X.shape[0], learning_rate=learning_rate)\n",
    "    for i in range(100):\n",
    "        Y_hat = forward_propagation(X)\n",
    "        cost = loss(Y_hat, Y)\n",
    "        if i%1==0:\n",
    "            Y_predict = predict(Y_hat)\n",
    "            accu = accuracy(Y_predict, Y)\n",
    "            print(i,'>',cost,'; accu:',accu)\n",
    "        backpropagate_all()\n",
    "        update_parameters()\n",
    "    \n",
    "model(mnist['X_train'], mnist['Y_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache: [('A1', (100, 50000)),\n",
      " ('A2', (10, 50000)),\n",
      " ('Z1', (100, 50000)),\n",
      " ('Z2', (10, 50000)),\n",
      " ('dA1', (100, 50000)),\n",
      " ('dA2', (10, 50000)),\n",
      " ('dW1', (100, 784)),\n",
      " ('dW2', (10, 100)),\n",
      " ('dZ1', (100, 50000)),\n",
      " ('dZ2', (10, 50000)),\n",
      " ('db1', (100, 1)),\n",
      " ('db2', (10, 1))]\n",
      "parameters: [('W1', (100, 784)), ('W2', (10, 100)), ('b1', (100, 1)), ('b2', (10, 1))]\n",
      "hyper_parameters: ['C', 'L', 'X_size', 'layers', 'learning_rate']\n"
     ]
    }
   ],
   "source": [
    "debug_show_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
