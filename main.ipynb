{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todo list\n",
    "\n",
    "1. one_hot √\n",
    "\n",
    "* mini-batch\n",
    "\n",
    "* normalization\n",
    "\n",
    "* train/dev/test set √\n",
    "\n",
    "* linear function √\n",
    "\n",
    "* sigmoid function\n",
    "\n",
    "* tanh function\n",
    "\n",
    "* relu function √\n",
    "\n",
    "* softmax function √\n",
    "\n",
    "* loss function √\n",
    "\n",
    "* cost function ?\n",
    "\n",
    "* regularization\n",
    "\n",
    "* drop out\n",
    "\n",
    "* batch normalization\n",
    "\n",
    "* momentum\n",
    "\n",
    "* exponentially moving average\n",
    "\n",
    "* Adam\n",
    "\n",
    "* all backpropagation of above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\"\"\" helper functions \"\"\"\n",
    "def names_in(dictionary):\n",
    "    \"\"\" list all names in a dictionary \"\"\"\n",
    "    print([name for name,_ in sorted(dictionary.items())])\n",
    "def names_shape_in(dictionary):\n",
    "    pprint([(name, val.shape) for name,val in sorted(dictionary.items())])\n",
    "\n",
    "def debug_show_all_variables():\n",
    "    print(\"cache: \", end='')\n",
    "    names_shape_in(cache)\n",
    "    print(\"parameters: \", end='')\n",
    "    names_shape_in(parameters)\n",
    "    print(\"hyper_parameters: \", end='')\n",
    "    names_in(hyper_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_mnist():\n",
    "    import struct\n",
    "    from array import array\n",
    "    \"\"\" \n",
    "    load MNIST dataset into numpy array \n",
    "    MNIST dataset can be downloaded manually.\n",
    "    url: http://yann.lecun.com/exdb/mnist/\n",
    "    \"\"\"\n",
    "    ret = {}\n",
    "    with open('MNIST/train-images.idx3-ubyte', 'rb') as f:\n",
    "        magic, size, rows, cols = struct.unpack(\">IIII\", f.read(16))\n",
    "        assert(magic==2051)\n",
    "        ret['X_train'] = np.array(array(\"B\", f.read())).reshape(size,rows,cols)\n",
    "\n",
    "    with open('MNIST/t10k-images.idx3-ubyte', 'rb') as f:\n",
    "        magic, size, rows, cols = struct.unpack(\">IIII\", f.read(16))\n",
    "        assert(magic==2051)\n",
    "        ret['X_test'] = np.array(array(\"B\", f.read())).reshape(size,rows,cols)\n",
    "\n",
    "    with open('MNIST/train-labels.idx1-ubyte', 'rb') as f:\n",
    "        magic, size = struct.unpack(\">II\", f.read(8))\n",
    "        assert(magic==2049)\n",
    "        ret['Y_train'] = np.array(array(\"B\", f.read())).reshape(size,1)\n",
    "\n",
    "    with open('MNIST/t10k-labels.idx1-ubyte', 'rb') as f:\n",
    "        magic, size = struct.unpack(\">II\", f.read(8))\n",
    "        assert(magic==2049)\n",
    "        ret['Y_test'] = np.array(array(\"B\", f.read())).reshape(size,1)\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (50000, 28, 28) X_dev: (10000, 28, 28) X_test: (10000, 28, 28)\n",
      "Y_train: (50000, 1) Y_dev: (10000, 1) Y_test: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "mnist_original = load_mnist()\n",
    "\n",
    "\"\"\" random shuffle the training set \"\"\"\n",
    "np.random.seed(1)\n",
    "permutation = np.random.permutation(mnist_original['X_train'].shape[0])\n",
    "mnist_original['X_train'] = mnist_original['X_train'][permutation]\n",
    "mnist_original['Y_train'] = mnist_original['Y_train'][permutation]\n",
    "\n",
    "\"\"\" divide trainset into trainset and devset \"\"\"\n",
    "len_of_dev = 10000\n",
    "mnist_original['X_dev'] = mnist_original['X_train'][:len_of_dev]\n",
    "mnist_original['Y_dev'] = mnist_original['Y_train'][:len_of_dev]\n",
    "mnist_original['X_train'] = mnist_original['X_train'][len_of_dev:]\n",
    "mnist_original['Y_train'] = mnist_original['Y_train'][len_of_dev:]\n",
    "\n",
    "print('X_train:', mnist_original['X_train'].shape,\n",
    "      'X_dev:', mnist_original['X_dev'].shape,\n",
    "      'X_test:', mnist_original['X_test'].shape)\n",
    "print('Y_train:', mnist_original['Y_train'].shape,\n",
    "      'Y_dev:', mnist_original['Y_dev'].shape,\n",
    "      'Y_test:', mnist_original['Y_test'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def manually_validate_dataset(dataset):\n",
    "    random_train = np.random.randint(1, len(dataset['X_train']))-1\n",
    "    random_dev = np.random.randint(1, len(dataset['X_dev']))-1\n",
    "    random_test = np.random.randint(1, len(dataset['X_test']))-1\n",
    "    print(dataset['Y_train'][random_train], dataset['Y_dev'][random_dev], dataset['Y_test'][random_test])\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1,3, sharey=True, figsize=[10,3])\n",
    "    ax1.imshow(dataset['X_train'][random_train], cmap='gray')\n",
    "    ax2.imshow(dataset['X_dev'][random_dev], cmap='gray')\n",
    "    ax3.imshow(dataset['X_test'][random_test], cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "#manually_validate_dataset(mnist_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ X = \n",
    "\\begin{bmatrix}\n",
    "\\vert & & \\vert & & \\vert \\\\\n",
    "x^{(1)} & ... & x^{(i)} & ... & x^{(m)} \\\\\n",
    "\\vert & & \\vert & & \\vert\n",
    "\\end{bmatrix} \n",
    "\\quad \\quad\n",
    "Y = \n",
    "\\begin{bmatrix}\n",
    "\\vert & & \\vert & & \\vert \\\\\n",
    "y_{one\\_hot}^{(1)} & ... & y_{one\\_hot}^{(i)} & ... & y_{one\\_hot}^{(m)} \\\\\n",
    "\\vert & & \\vert & & \\vert \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 50000) (10, 50000)\n"
     ]
    }
   ],
   "source": [
    "mnist = {}\n",
    "\n",
    "\"\"\" X is 28*28 image \"\"\"\n",
    "def flatten(X):\n",
    "    \"\"\" prepare X to (nx, m) shape \"\"\"\n",
    "    X = X.reshape(-1, 28*28).T\n",
    "    return X\n",
    "\n",
    "def normalize(X):\n",
    "    u = 0\n",
    "    sigma = 255\n",
    "    return (X-u) / sigma\n",
    "\n",
    "mnist['X_train'] = normalize(flatten(mnist_original['X_train']))\n",
    "mnist['X_dev'] = normalize(flatten(mnist_original['X_dev']))\n",
    "mnist['X_test'] = normalize(flatten(mnist_original['X_test']))\n",
    "\n",
    "\"\"\" Y is label 0-9 \"\"\"\n",
    "def one_hot(Y, C):\n",
    "    \"\"\" prepare Y to (1, m) shape \"\"\"\n",
    "    assert(Y.shape[1]==1)\n",
    "    Y_ret = np.zeros((Y.shape[0], C))\n",
    "    Y_ret[np.arange(Y.shape[0]), Y.reshape(-1).astype(int)] = 1\n",
    "    Y_ret = Y_ret.T\n",
    "    return Y_ret\n",
    "\n",
    "def back_one_hot(Y):\n",
    "    \"\"\" convert one hot Y back to real number \"\"\"\n",
    "    Y_ret = np.repeat( [np.arange(Y.shape[0])], repeats=Y.shape[1], axis=0 )\n",
    "    assert(Y_ret.shape == Y.T.shape)\n",
    "    Y_ret = Y_ret[Y.T.astype(bool)]\n",
    "    return Y_ret.reshape(-1,1)\n",
    "\n",
    "mnist['Y_train'] = one_hot(mnist_original['Y_train'], 10)\n",
    "mnist['Y_dev'] = one_hot(mnist_original['Y_dev'], 10)\n",
    "mnist['Y_test'] = one_hot(mnist_original['Y_test'], 10)\n",
    "\n",
    "print(mnist['X_train'].shape, mnist['Y_train'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" init cache, parameters and hyper_parameters \"\"\"\n",
    "def init(X_size, hidden_layers=[10,10,5], C=10, learning_rate=0.01, regularization=0.1):\n",
    "    global cache, parameters, hyper_parameters\n",
    "    cache = {}\n",
    "    parameters = {}\n",
    "    hyper_parameters = {}\n",
    "    initialize_hyper_parameters(X_size, hidden_layers, C, learning_rate, regularization)\n",
    "    initialize_parameters()\n",
    "\n",
    "def initialize_hyper_parameters(X_size, hidden_layers, C, learning_rate, regularization):\n",
    "    global hyper_parameters\n",
    "    hyper_parameters['X_size'] = X_size\n",
    "    # layers of network, include the last softmax layer\n",
    "    # hidden layers use ReLU activation function, and the last layer use Softmax function\n",
    "    hyper_parameters['layers'] = hidden_layers + [C]\n",
    "    # L layers in total\n",
    "    hyper_parameters['L'] = len(hyper_parameters['layers'])\n",
    "    # Class numbers 0-9 is 10\n",
    "    hyper_parameters['C'] = C\n",
    "    hyper_parameters['learning_rate'] = learning_rate\n",
    "    hyper_parameters['regularization'] = regularization\n",
    "    # tiny_float to avoid divide by zero\n",
    "    hyper_parameters['tiny_float'] = 1e-7\n",
    "\n",
    "def initialize_parameters():\n",
    "    \"\"\" init W b or any other parameters in every layer \"\"\"\n",
    "    global parameters, hyper_parameters\n",
    "    layers = hyper_parameters['layers']\n",
    "    x_size = hyper_parameters['X_size']\n",
    "    cells_prev = x_size\n",
    "    for layer_idx, cells in enumerate(layers):\n",
    "        parameters['W'+str(layer_idx+1)] = np.random.randn(cells, cells_prev) * 0.01\n",
    "        parameters['b'+str(layer_idx+1)] = np.zeros((cells, 1))\n",
    "        cells_prev = layers[layer_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "ReLu = max(0, x)\n",
    "\\quad \\quad\n",
    "Softmax = \\frac{\\exp(Z)}{\\sum_i^n{\\exp(Z)}}\n",
    "$$\n",
    "***\n",
    "$$\n",
    "Z = W X + b\n",
    "\\quad \\quad\n",
    "A = active(Z)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ReLU(X):\n",
    "    return X * (X > 0)\n",
    "\n",
    "def softmax(X):\n",
    "    s = np.sum(np.exp(X), axis=0)\n",
    "    return np.exp(X) / s\n",
    "\n",
    "def forward_propagation_each_layer(W, A_prev, b, activation_function=ReLU):\n",
    "    Z = np.dot(W, A_prev) + b\n",
    "    A = activation_function(Z)\n",
    "    return Z,A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is logistic regression loss:\n",
    "$$\n",
    "L(\\hat{y}, y) = -\\frac{1}{m} \\sum_i^m{(y_i\\log(\\hat{y}_i) + (1-y_i)\\log(1-\\hat{y}_i))}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is multi-class classification loss:\n",
    "$$\n",
    "L(\\hat{y}, y) = -\\frac{1}{m} \\sum_i^m \\sum_j^C (y_j^{(i)}\\log(\\hat{y}_j^{(i)}))\n",
    "$$\n",
    "where m is batch size, C is class number (10 classes), (i) is the i-th example, j is the vertical bit of result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(Y_hat, Y):\n",
    "    A = np.multiply(Y, np.log(Y_hat+hyper_parameters['tiny_float']))\n",
    "    B = np.multiply(1-Y, np.log(1-Y_hat+hyper_parameters['tiny_float']))\n",
    "    Loss = -A-B\n",
    "    #why not use Loss = -np.sum(A+B) / m?\n",
    "    return Loss\n",
    "\n",
    "def cost(loss):\n",
    "    return np.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(Y_hat):\n",
    "    return np.argmax(Y_hat,axis=0).reshape(-1,1)\n",
    "\n",
    "def accuracy(Y_predict, Y):\n",
    "    return np.sum(np.equal(Y_predict, back_one_hot(Y))) / Y_predict.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This formula is wrong, This is the logistic regression(binary classifier) Loss\n",
    "\n",
    "\n",
    "We already know L formula above:\n",
    "$$\n",
    "L = -\\frac{1}{m} \\sum_i^{m}[y^{(i)} log(\\hat{y}^{(i)}) + (1-y^{(i)}) log(1-\\hat{y}^{(i)})]\n",
    "$$\n",
    "***\n",
    "$$\n",
    "A^{[L]} = \\hat{Y} = \n",
    "\\begin{bmatrix}\n",
    "\\hat{y}^{(1)} & ... & \\hat{y}^{(m)} \n",
    "\\end{bmatrix}\n",
    "\\quad, \\quad\n",
    "Y =\n",
    "\\begin{bmatrix}\n",
    "y^{(1)} & ... & y^{(m)}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "***\n",
    "So:\n",
    "$$\n",
    "\\textbf{dAL} = \\frac{\\partial L}{\\partial A^{[L]}}\n",
    "= - \\frac{1}{m} ( \\frac{Y}{A^{[L]}} - \\frac{1-Y}{1-A^{[L]}} )\n",
    "= \\frac{A^{[L]}-Y}{m A^{[L]}(1-A^{[L]})}\n",
    "$$\n",
    "\n",
    "in which **dAL** represents the Python Variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def backpropagate_cost(Y, AL):\n",
    "    #m = Y.shape[1] #why not 1/m?\n",
    "    #dAL = -np.divide(Y, AL) #why not use this formula?\n",
    "    dAL = - (np.divide(Y, AL+hyper_parameters['tiny_float']) - np.divide(1 - Y, 1 - AL+hyper_parameters['tiny_float']))\n",
    "    return dAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\textbf{dZL}\n",
    "= \\frac{\\partial L}{\\partial Z^{[L]}}\n",
    "= \\frac{\\partial L}{\\partial A^{[L]}} \\frac{\\partial A^{[L]}}{\\partial Z^{[L]}}\n",
    "= \\textbf{dAL} * \\frac{\\partial A^{[L]}}{\\partial Z^{[L]}}\n",
    "$$\n",
    "***\n",
    "$$\n",
    "\\frac{\\partial A^{[L]}}{\\partial Z^{[L]}}\n",
    "= A^{[L]} - (A^{[L]})^{2}\n",
    "$$\n",
    "***\n",
    "$$\n",
    "\\textbf{dZL} = \\textbf{dAL} * (\\textbf{AL} - \\textbf{AL}^{2})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backpropagate_softmax(AL, dAL, Y=None, ZL=None):\n",
    "    dZ = np.multiply(dAL, (AL-np.power(AL,2)))\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\because\n",
    "$$\n",
    "$$\n",
    "Z^{[l]} = W^{[l]} * A^{[l-1]} + b^{[l]}\n",
    "$$\n",
    "***\n",
    "$$\n",
    "\\textbf{dZ} = \\frac{\\partial L}{\\partial Z^{[l]}}\n",
    "$$\n",
    "***\n",
    "$$\n",
    "\\therefore\n",
    "$$\n",
    "$$\n",
    "\\textbf{dA\\_prev} \n",
    "= \\frac{\\partial L}{\\partial A^{[l-1]}}\n",
    "= \\frac{\\partial L}{\\partial Z^{[l]}} \\frac{\\partial Z^{[l]}}{\\partial A^{[l-1]}}\n",
    "= \\textbf{dZ} * W^{[l]}\n",
    "$$\n",
    "***\n",
    "$$\n",
    "\\textbf{dW}\n",
    "= \\frac{\\partial L}{\\partial W^{[l]}}\n",
    "= \\frac{\\partial L}{\\partial Z^{[l]}} \\frac{\\partial Z^{[l]}}{\\partial W^{[l]}}\n",
    "= \\textbf{dZ} * A^{[l-1]}\n",
    "$$\n",
    "***\n",
    "$$\n",
    "\\textbf{db}\n",
    "= \\frac{\\partial L}{\\partial b^{[l]}}\n",
    "= \\frac{\\partial L}{\\partial Z^{[l]}} \\frac{\\partial Z^{[l]}}{\\partial b^{[l]}}\n",
    "= \\textbf{dZ} * 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backpropagate_linear(dZ, W, A_prev):\n",
    "    m = dZ.shape[1] # I still don't know why move 1/m here from backpropagate_cost.\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    dW = 1/m * np.dot(dZ, A_prev.T)\n",
    "    db = 1/m * np.sum(dZ, axis=1).reshape(-1,1)\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\because\n",
    "$$\n",
    "\n",
    "$$\n",
    "A^{[l]} = ReLU(Z^{[l]}) = max(0, Z^{[l]})\n",
    "$$\n",
    "***\n",
    "$$\n",
    "\\therefore\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\textbf{dZ}\n",
    "= \\frac{\\partial L}{\\partial Z^{[l]}}\n",
    "= \\frac{\\partial L}{\\partial A^{[l]}} \\frac{\\partial A^{[l]}}{\\partial Z^{[l]}}\n",
    "= \\textbf{dA} * ReLU^{-1}(Z^{[l]})\n",
    "= \\textbf{dA} * \n",
    "\\begin{bmatrix}\n",
    "1 (z \\geq 0)\\\\\n",
    "0 (z < 0)\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def backpropagate_ReLU(dA, Z):\n",
    "    return np.multiply(dA, (Z>=0).astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" neural network logic level \"\"\"\n",
    "\n",
    "def forwardpropagation_all(X):\n",
    "    global cache, parameters, hyper_parameters\n",
    "    layers = hyper_parameters['layers']\n",
    "    A_prev = X\n",
    "    for layer_idx, cells in enumerate(layers):\n",
    "        W = parameters['W'+str(layer_idx+1)]\n",
    "        b = parameters['b'+str(layer_idx+1)]\n",
    "        if layer_idx==len(layers)-1:\n",
    "            # Last layer's activation function should be softmax\n",
    "            Z, A = forward_propagation_each_layer(W, A_prev, b, softmax)\n",
    "        else:\n",
    "            Z, A = forward_propagation_each_layer(W, A_prev, b)\n",
    "        cache['Z'+str(layer_idx+1)] = Z\n",
    "        cache['A'+str(layer_idx+1)] = A\n",
    "        A_prev = A\n",
    "\n",
    "def backpropagate_all(X, Y):\n",
    "    global mnist, cache, parameters, hyper_parameters\n",
    "    # 1. Last layer has coss function and softmax function\n",
    "    L = str(hyper_parameters['L'])\n",
    "    cache['dA'+L] = backpropagate_cost(Y,cache['A'+L])\n",
    "    AL = cache['A'+L]\n",
    "    dAL = cache['dA'+L]\n",
    "    cache['dZ'+L] = backpropagate_softmax(AL, dAL)\n",
    "\n",
    "    # 2. Layers in between are similar: linear and ReLU\n",
    "    for i in np.arange(start=hyper_parameters['L']-1, stop=0, step=-1):\n",
    "        L = str(i)\n",
    "        cache['dA'+L], cache['dW'+str(i+1)], cache['db'+str(i+1)] = backpropagate_linear(cache['dZ'+str(i+1)], \n",
    "                                                                           parameters['W'+str(i+1)], \n",
    "                                                                           cache['A'+L])\n",
    "        cache['dZ'+L] = backpropagate_ReLU(cache['dA'+L], cache['Z'+L])\n",
    "\n",
    "    # 3. First layer has different parameter.\n",
    "    _, cache['dW1'], cache['db1'] = backpropagate_linear(cache['dZ1'], parameters['W1'], X)\n",
    "\n",
    "def update_parameters():\n",
    "    global cache, parameters\n",
    "    m = cache['dZ1'].shape[1]\n",
    "    for i in range(hyper_parameters['L']):\n",
    "        L = str(i+1)\n",
    "        parameters['W'+L] -= cache['dW'+L] * hyper_parameters['learning_rate'] \\\n",
    "        + hyper_parameters['regularization'] * parameters['W'+L] * hyper_parameters['learning_rate'] / m\n",
    "        parameters['b'+L] -= cache['db'+L] * hyper_parameters['learning_rate']\n",
    "        \n",
    "def cost_with_regularization(cost):\n",
    "    global parameters, hyper_parameters\n",
    "    if hyper_parameters['regularization']==0:\n",
    "        return cost\n",
    "    s = 0\n",
    "    for i in range(hyper_parameters['L']):\n",
    "        L = str(i+1)\n",
    "        s += np.sum(np.mean(np.dot(parameters['W'+L].T, parameters['W'+L]), axis=1), keepdims=False)\n",
    "    \n",
    "    cost += hyper_parameters['regularization'] / 2 * s\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 > 1 -th iter, training error =  0.0308154673848 ; accu_train =  0.953125\n",
      " > dev error =  0.03624720335 ; accu_dev =  0.9374\n",
      "1000 > 2 -th iter, training error =  0.0242003642807 ; accu_train =  0.9765625\n",
      " > dev error =  0.0229473583357 ; accu_dev =  0.9607\n",
      "1500 > 3 -th iter, training error =  0.0170215872048 ; accu_train =  0.96875\n",
      " > dev error =  0.0164491044366 ; accu_dev =  0.9707\n",
      "2000 > 5 -th iter, training error =  0.0139467133097 ; accu_train =  0.9609375\n",
      " > dev error =  0.0177605589023 ; accu_dev =  0.9686\n",
      "2500 > 6 -th iter, training error =  0.00285418106803 ; accu_train =  1.0\n",
      " > dev error =  0.0138554752856 ; accu_dev =  0.9784\n",
      "3000 > 7 -th iter, training error =  0.00638742196485 ; accu_train =  0.984375\n",
      " > dev error =  0.0140489880351 ; accu_dev =  0.9769\n",
      "3500 > 8 -th iter, training error =  0.00511877647788 ; accu_train =  0.984375\n",
      " > dev error =  0.0140035250418 ; accu_dev =  0.9781\n",
      "4000 > 10 -th iter, training error =  0.00339154685975 ; accu_train =  1.0\n",
      " > dev error =  0.0134825024685 ; accu_dev =  0.9797\n",
      "4500 > 11 -th iter, training error =  0.00606061356319 ; accu_train =  0.9921875\n",
      " > dev error =  0.014523495056 ; accu_dev =  0.9781\n",
      "5000 > 12 -th iter, training error =  0.00219695537881 ; accu_train =  1.0\n",
      " > dev error =  0.0131346683412 ; accu_dev =  0.9806\n",
      "5500 > 14 -th iter, training error =  0.000554327590105 ; accu_train =  1.0\n",
      " > dev error =  0.0132789223034 ; accu_dev =  0.9818\n",
      "6000 > 15 -th iter, training error =  0.000111836739748 ; accu_train =  1.0\n",
      " > dev error =  0.0135740734907 ; accu_dev =  0.9809\n",
      "6500 > 16 -th iter, training error =  0.000213619912166 ; accu_train =  1.0\n",
      " > dev error =  0.0138869192047 ; accu_dev =  0.9809\n",
      "7000 > 17 -th iter, training error =  0.000217328463164 ; accu_train =  1.0\n",
      " > dev error =  0.0139858014878 ; accu_dev =  0.9823\n",
      "7500 > 19 -th iter, training error =  0.000460886837926 ; accu_train =  1.0\n",
      " > dev error =  0.01422717355 ; accu_dev =  0.9809\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEFCAYAAAAPCDf9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0BcRSSIKfRGRJKLQFxFJIv8fmRS3BKOaqh4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efe2dffb3c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def model(X_full, Y_full, X_dev, Y_dev, \n",
    "          learning_rate=0.01, print_every=100, iteration=500, \n",
    "          hidden_layers=[100], batch_size=128, regularization=0):\n",
    "    init(hidden_layers=hidden_layers, C=Y_full.shape[0], X_size=X_full.shape[0], \n",
    "         learning_rate=learning_rate, regularization=regularization)\n",
    "    # costs will be returned for plotting\n",
    "    costs = []\n",
    "    costs_index = []\n",
    "    costs_dev = []\n",
    "    costs_dev_index = []\n",
    "    m = X_full.shape[1]\n",
    "    step = 0\n",
    "    np.random.seed(1)\n",
    "    for i in range(iteration):\n",
    "        permutation = np.random.permutation(m)\n",
    "        X_permutated = X_full[:, permutation]\n",
    "        Y_permutated = Y_full[:, permutation]\n",
    "        for j in range( m // batch_size ):\n",
    "            X = X_permutated[:, j*batch_size:(j+1)*batch_size]\n",
    "            Y = Y_permutated[:, j*batch_size:(j+1)*batch_size]\n",
    "            if X.shape[1]==batch_size:\n",
    "                forwardpropagation_all(X)\n",
    "                Y_hat = cache['A'+str(hyper_parameters['L'])]\n",
    "                cost_value = cost_with_regularization(cost(loss(Y_hat, Y)))\n",
    "                costs.append(cost_value)\n",
    "                costs_index.append(step)\n",
    "                backpropagate_all(X, Y)\n",
    "                update_parameters()\n",
    "                \n",
    "                step += 1\n",
    "                if step%print_every==0:\n",
    "                    Y_predict = predict(Y_hat)\n",
    "                    accu = accuracy(Y_predict, Y)\n",
    "\n",
    "                    forwardpropagation_all(X_dev)\n",
    "                    Y_hat_dev = cache['A'+str(hyper_parameters['L'])]\n",
    "                    cost_dev_value = cost_with_regularization(cost(loss(Y_hat_dev, Y_dev)))\n",
    "                    costs_dev.append(cost_dev_value)\n",
    "                    costs_dev_index.append(step)\n",
    "                    Y_predict_dev = predict(Y_hat_dev)\n",
    "                    accu_dev = accuracy(Y_predict_dev, Y_dev)\n",
    "\n",
    "                    print(step,'>',i,'-th iter, training error = ',cost_value,'; accu_train = ',accu)\n",
    "                    print(' > dev error = ',cost_dev_value,'; accu_dev = ', accu_dev)\n",
    "                if np.isnan(cost_value):\n",
    "                    print(\"ERROR to nan!!!\")\n",
    "                    break\n",
    "            else:\n",
    "                print(\"Batch_Size ERROR!!!\")\n",
    "    return costs, costs_index, costs_dev, costs_dev_index\n",
    "\n",
    "def test_model():\n",
    "    m = 10\n",
    "    X = np.random.randn(784,m)\n",
    "    Y = one_hot(np.random.randint(low=0, high=10, size=(m,1)), C=10)\n",
    "    #print(X[:,1])\n",
    "    #print(Y[:,1])\n",
    "    costs = model(X,Y,learning_rate=0.1, print_every=10, iteration=100,\n",
    "                 hidden_layers=[10,10])\n",
    "    plt.plot(costs)\n",
    "    plt.show()\n",
    "def test_model_1():\n",
    "    costs, costs_index, costs_dev, costs_dev_index = \\\n",
    "    model(mnist['X_train'], mnist['Y_train'], mnist['X_dev'], mnist['Y_dev'],\n",
    "                  learning_rate=0.3, print_every=500, iteration=20,\n",
    "                  batch_size=128, regularization=0,\n",
    "                  hidden_layers=[784,100])\n",
    "    \n",
    "    plt.plot(costs_index, costs)\n",
    "    plt.plot(costs_dev_index, costs_dev)\n",
    "    plt.title(\"Cost Value\")\n",
    "    plt.show()\n",
    "\n",
    "test_model_1()\n",
    "#model(mnist['X_train'], mnist['Y_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache: [('A1', (784, 32)),\n",
      " ('A2', (1000, 32)),\n",
      " ('A3', (1000, 32)),\n",
      " ('A4', (10, 32)),\n",
      " ('Z1', (784, 32)),\n",
      " ('Z2', (1000, 32)),\n",
      " ('Z3', (1000, 32)),\n",
      " ('Z4', (10, 32)),\n",
      " ('dA1', (784, 32)),\n",
      " ('dA2', (1000, 32)),\n",
      " ('dA3', (1000, 32)),\n",
      " ('dA4', (10, 32)),\n",
      " ('dW1', (784, 784)),\n",
      " ('dW2', (1000, 784)),\n",
      " ('dW3', (1000, 1000)),\n",
      " ('dW4', (10, 1000)),\n",
      " ('dZ1', (784, 32)),\n",
      " ('dZ2', (1000, 32)),\n",
      " ('dZ3', (1000, 32)),\n",
      " ('dZ4', (10, 32)),\n",
      " ('db1', (784, 1)),\n",
      " ('db2', (1000, 1)),\n",
      " ('db3', (1000, 1)),\n",
      " ('db4', (10, 1))]\n",
      "parameters: [('W1', (784, 784)),\n",
      " ('W2', (1000, 784)),\n",
      " ('W3', (1000, 1000)),\n",
      " ('W4', (10, 1000)),\n",
      " ('b1', (784, 1)),\n",
      " ('b2', (1000, 1)),\n",
      " ('b3', (1000, 1)),\n",
      " ('b4', (10, 1))]\n",
      "hyper_parameters: ['C', 'L', 'X_size', 'layers', 'learning_rate', 'regularization', 'tiny_float']\n"
     ]
    }
   ],
   "source": [
    "debug_show_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy:  0.9824\n"
     ]
    }
   ],
   "source": [
    "forwardpropagation_all(mnist['X_test'])\n",
    "Y_hat_test = cache['A'+str(hyper_parameters['L'])]\n",
    "Y_predict_test = predict(Y_hat_test)\n",
    "accu_test = accuracy(Y_predict_test, mnist['Y_test'])\n",
    "print(\"Final accuracy: \", accu_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
