{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todo list\n",
    "\n",
    "1. one_hot √\n",
    "\n",
    "* mini-batch\n",
    "\n",
    "* normalization\n",
    "\n",
    "* train/dev/test set √\n",
    "\n",
    "* linear function √\n",
    "\n",
    "* sigmoid function\n",
    "\n",
    "* tanh function\n",
    "\n",
    "* relu function √\n",
    "\n",
    "* softmax function √\n",
    "\n",
    "* loss function √\n",
    "\n",
    "* cost function\n",
    "\n",
    "* regularization\n",
    "\n",
    "* drop out\n",
    "\n",
    "* batch normalization\n",
    "\n",
    "* momentum\n",
    "\n",
    "* exponentially moving average\n",
    "\n",
    "* Adam\n",
    "\n",
    "* all backpropagation of above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import struct\n",
    "from array import array\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_mnist():\n",
    "    \"\"\" \n",
    "    load MNIST dataset into numpy array \n",
    "    MNIST dataset can be downloaded manually.\n",
    "    url: http://yann.lecun.com/exdb/mnist/\n",
    "    \"\"\"\n",
    "    ret = {}\n",
    "    with open('MNIST/train-images.idx3-ubyte', 'rb') as f:\n",
    "        magic, size, rows, cols = struct.unpack(\">IIII\", f.read(16))\n",
    "        assert(magic==2051)\n",
    "        ret['X_train'] = np.array(array(\"B\", f.read())).reshape(size,rows,cols)\n",
    "\n",
    "    with open('MNIST/t10k-images.idx3-ubyte', 'rb') as f:\n",
    "        magic, size, rows, cols = struct.unpack(\">IIII\", f.read(16))\n",
    "        assert(magic==2051)\n",
    "        ret['X_test'] = np.array(array(\"B\", f.read())).reshape(size,rows,cols)\n",
    "\n",
    "    with open('MNIST/train-labels.idx1-ubyte', 'rb') as f:\n",
    "        magic, size = struct.unpack(\">II\", f.read(8))\n",
    "        assert(magic==2049)\n",
    "        ret['Y_train'] = np.array(array(\"B\", f.read())).reshape(size,1)\n",
    "\n",
    "    with open('MNIST/t10k-labels.idx1-ubyte', 'rb') as f:\n",
    "        magic, size = struct.unpack(\">II\", f.read(8))\n",
    "        assert(magic==2049)\n",
    "        ret['Y_test'] = np.array(array(\"B\", f.read())).reshape(size,1)\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (50000, 28, 28) X_dev: (10000, 28, 28) X_test: (10000, 28, 28)\n",
      "Y_train: (50000, 1) Y_dev: (10000, 1) Y_test: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "mnist_original = load_mnist()\n",
    "\n",
    "\"\"\" random shuffle the training set \"\"\"\n",
    "permutation = np.random.permutation(mnist_original['X_train'].shape[0])\n",
    "mnist_original['X_train'] = mnist_original['X_train'][permutation]\n",
    "mnist_original['Y_train'] = mnist_original['Y_train'][permutation]\n",
    "\n",
    "\"\"\" divide trainset into trainset and devset \"\"\"\n",
    "len_of_dev = 10000\n",
    "mnist_original['X_dev'] = mnist_original['X_train'][:len_of_dev]\n",
    "mnist_original['Y_dev'] = mnist_original['Y_train'][:len_of_dev]\n",
    "mnist_original['X_train'] = mnist_original['X_train'][len_of_dev:]\n",
    "mnist_original['Y_train'] = mnist_original['Y_train'][len_of_dev:]\n",
    "\n",
    "print('X_train:', mnist_original['X_train'].shape,\n",
    "      'X_dev:', mnist_original['X_dev'].shape,\n",
    "      'X_test:', mnist_original['X_test'].shape)\n",
    "print('Y_train:', mnist_original['Y_train'].shape,\n",
    "      'Y_dev:', mnist_original['Y_dev'].shape,\n",
    "      'Y_test:', mnist_original['Y_test'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def manually_validate_dataset(dataset):\n",
    "    random_train = np.random.randint(1, len(dataset['X_train']))-1\n",
    "    random_dev = np.random.randint(1, len(dataset['X_dev']))-1\n",
    "    random_test = np.random.randint(1, len(dataset['X_test']))-1\n",
    "    print(dataset['Y_train'][random_train], dataset['Y_dev'][random_dev], dataset['Y_test'][random_test])\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1,3, sharey=True, figsize=[10,3])\n",
    "    ax1.imshow(dataset['X_train'][random_train], cmap='gray')\n",
    "    ax2.imshow(dataset['X_dev'][random_dev], cmap='gray')\n",
    "    ax3.imshow(dataset['X_test'][random_test], cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "#manually_validate_dataset(mnist_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ X = \n",
    "\\begin{bmatrix}\n",
    "\\vert & & \\vert & & \\vert \\\\\n",
    "x^{(1)} & ... & x^{(i)} & ... & x^{(m)} \\\\\n",
    "\\vert & & \\vert & & \\vert\n",
    "\\end{bmatrix} \n",
    "\\quad \\quad\n",
    "Y = \n",
    "\\begin{bmatrix}\n",
    "\\vert & & \\vert & & \\vert \\\\\n",
    "y_{one\\_hot}^{(1)} & ... & y_{one\\_hot}^{(i)} & ... & y_{one\\_hot}^{(m)} \\\\\n",
    "\\vert & & \\vert & & \\vert \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 50000) (10, 50000)\n"
     ]
    }
   ],
   "source": [
    "mnist = {}\n",
    "\n",
    "\"\"\" X is 28*28 image \"\"\"\n",
    "def flatten(X):\n",
    "    \"\"\" prepare X to (nx, m) shape \"\"\"\n",
    "    X = X.reshape(-1, 28*28).T\n",
    "    return X\n",
    "\n",
    "mnist['X_train'] = flatten(mnist_original['X_train'])\n",
    "mnist['X_dev'] = flatten(mnist_original['X_dev'])\n",
    "mnist['X_test'] = flatten(mnist_original['X_test'])\n",
    "\n",
    "\"\"\" Y is label 0-9 \"\"\"\n",
    "def one_hot(Y, C):\n",
    "    \"\"\" prepare Y to (1, m) shape \"\"\"\n",
    "    assert(Y.shape[1]==1)\n",
    "    Y_ret = np.zeros((Y.shape[0], C))\n",
    "    Y_ret[np.arange(Y.shape[0]), Y.reshape(-1).astype(int)] = 1\n",
    "    Y_ret = Y_ret.T\n",
    "    return Y_ret\n",
    "\n",
    "def test_one_hot():\n",
    "    Y = np.ones((5,1))\n",
    "    Y = one_hot(Y, 10)\n",
    "    assert(Y[0,0]==0)\n",
    "    assert(Y[1,0]==1)\n",
    "    assert(Y[2,1]==0)\n",
    "\n",
    "def back_one_hot(Y):\n",
    "    \"\"\" convert one hot Y back to real number \"\"\"\n",
    "    Y_ret = np.repeat( [np.arange(Y.shape[0])], repeats=Y.shape[1], axis=0 )\n",
    "    assert(Y_ret.shape == Y.T.shape)\n",
    "    Y_ret = Y_ret[Y.T.astype(bool)]\n",
    "    return Y_ret.reshape(-1,1)\n",
    "\n",
    "mnist['Y_train'] = one_hot(mnist_original['Y_train'], 10)\n",
    "mnist['Y_dev'] = one_hot(mnist_original['Y_dev'], 10)\n",
    "mnist['Y_test'] = one_hot(mnist_original['Y_test'], 10)\n",
    "\n",
    "print(mnist['X_train'].shape, mnist['Y_train'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 : (1, 784)\n",
      "W2 : (2, 1)\n",
      "W3 : (3, 2)\n",
      "W4 : (10, 3)\n",
      "b1 : (1, 1)\n",
      "b2 : (2, 1)\n",
      "b3 : (3, 1)\n",
      "b4 : (10, 1)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" layers of network, include the last softmax layer \"\"\"\n",
    "layers = [1, 2, 3, mnist['Y_train'].shape[0]]\n",
    "\n",
    "def initialize_parameters(layers, x_size):\n",
    "    \"\"\" init W b or any other parameters in every layer \"\"\"\n",
    "    parameters = {}\n",
    "    cells_prev = x_size\n",
    "    for layer_idx, cells in enumerate(layers):\n",
    "        parameters['W'+str(layer_idx+1)] = np.random.randn(cells, cells_prev) * 0.01\n",
    "        parameters['b'+str(layer_idx+1)] = np.zeros((cells, 1))\n",
    "        cells_prev = layers[layer_idx]\n",
    "    return parameters\n",
    "\n",
    "parameters = initialize_parameters(layers, mnist['X_train'].shape[0])\n",
    "for key, value in sorted(parameters.items()):\n",
    "    print(key, ':', value.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "ReLu = max(0, x)\n",
    "\\quad \\quad\n",
    "Softmax = \\frac{\\exp(Z)}{\\sum_i^n{\\exp(Z)}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "Z = W \\dot X + b\n",
    "\\quad \\quad\n",
    "A = active(Z)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 50000)\n"
     ]
    }
   ],
   "source": [
    "def ReLU(X):\n",
    "    return X * (X > 0)\n",
    "\n",
    "def test_ReLU():\n",
    "    X = np.array([1.,2.,-2.,-3.])\n",
    "    Y = np.array([1.,2.,0.,0.])\n",
    "    bias = np.sum(np.abs(Y - ReLU(X)))\n",
    "    assert(bias<0.0001)\n",
    "#test_ReLU()\n",
    "\n",
    "def softmax(X):\n",
    "    s = np.sum(np.exp(X))\n",
    "    return np.exp(X) / s\n",
    "\n",
    "def test_softmax():\n",
    "    X = np.array([-3.44,1.16,-0.81,3.91])\n",
    "    Y = np.array([0.0006, 0.0596, 0.0083, 0.9315])\n",
    "    bias = np.sum(np.abs(Y - softmax(X)))\n",
    "    assert(bias<0.0001)\n",
    "#test_softmax()\n",
    "\n",
    "def forward_propagation(X, layers, parameters):\n",
    "    A = X\n",
    "    for layer_idx, cells in enumerate(layers):\n",
    "        Z = np.dot(parameters['W'+str(layer_idx+1)], A)\n",
    "        if layer_idx<len(layers)-1:\n",
    "            \"\"\" normal layers use relu \"\"\"\n",
    "            A = ReLU(Z)\n",
    "        else:\n",
    "            \"\"\" last layers use softmax \"\"\"\n",
    "            A = softmax(Z)\n",
    "    return A\n",
    "\n",
    "Y_hat = forward_propagation(mnist['X_train'], layers, parameters)\n",
    "print(Y_hat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "L(\\hat{y}, y) = -\\frac{1}{m} \\sum_i^m{(y_i\\log(\\hat{y}_i) + (1-y_i)\\log(1-\\hat{y}_i))}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3122381664312115"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loss(Y_hat, Y):\n",
    "    A = (np.multiply(Y, np.log(Y_hat)))\n",
    "    B = (np.multiply(1-Y, np.log(1-Y_hat)))\n",
    "    C = -np.mean(A+B)\n",
    "    return C\n",
    "\n",
    "def test_loss():\n",
    "    Y = np.asarray([[1, 1, 1]])\n",
    "    aL = np.array([[.8,.9,0.4]])\n",
    "    assert(loss(aL, Y) - 0.414931599615 < 0.0001)\n",
    "#test_loss()\n",
    "L = loss(Y_hat, mnist['Y_train'])\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.091800000000000007"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict(Y_hat):\n",
    "    return np.argmax(Y_hat,axis=0).reshape(-1,1)\n",
    "\n",
    "def test_predict():\n",
    "    Y = np.array([[0.9,0.01,0.01,0.01,0.02,0.01,0.01,0.01,0.01,0.01],\n",
    "                  [0.01,0.01,0.01,0.02,0.9,0.01,0.01,0.01,0.01,0.01]\n",
    "                 ]).T\n",
    "    print(Y.shape)\n",
    "    Y_result = predict(Y)\n",
    "    print(Y_result)\n",
    "    Z = np.array([0,4]).reshape(2,1)\n",
    "    assert(np.sum(Y_result != Z)==0)\n",
    "#test_predict()\n",
    "\n",
    "Y_predict = predict(Y_hat)\n",
    "accurate = np.sum(np.equal(Y_predict, back_one_hot(mnist['Y_train']))) / Y_predict.shape[0]\n",
    "accurate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
