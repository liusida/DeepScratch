{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this notebook, I am going to implement a deep network from numpy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Python Standard Library struct and array\n",
    "# for dealing with reading dataset from file\n",
    "import struct\n",
    "from array import array\n",
    "from time import time\n",
    "\n",
    "# Numpy for calculating\n",
    "import numpy as np\n",
    "\n",
    "# To draw the training and dev cost value curve\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Test Cases\n",
    "import test\n",
    "\n",
    "# Some Useful Helper Functions\n",
    "import helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. Preparing Data\n",
    "\n",
    "## Load MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_mnist():\n",
    "    \"\"\" \n",
    "    load MNIST dataset into numpy array \n",
    "    MNIST dataset can be downloaded manually.\n",
    "    url: http://yann.lecun.com/exdb/mnist/\n",
    "    \"\"\"\n",
    "    ret = {}\n",
    "    with open('MNIST/train-images.idx3-ubyte', 'rb') as f:\n",
    "        magic, size, rows, cols = struct.unpack(\">IIII\", f.read(16))\n",
    "        assert(magic==2051)\n",
    "        ret['X_train'] = np.array(array(\"B\", f.read())).reshape(size,rows,cols)\n",
    "\n",
    "    with open('MNIST/t10k-images.idx3-ubyte', 'rb') as f:\n",
    "        magic, size, rows, cols = struct.unpack(\">IIII\", f.read(16))\n",
    "        assert(magic==2051)\n",
    "        ret['X_test'] = np.array(array(\"B\", f.read())).reshape(size,rows,cols)\n",
    "\n",
    "    with open('MNIST/train-labels.idx1-ubyte', 'rb') as f:\n",
    "        magic, size = struct.unpack(\">II\", f.read(8))\n",
    "        assert(magic==2049)\n",
    "        ret['Y_train'] = np.array(array(\"B\", f.read())).reshape(size,1)\n",
    "\n",
    "    with open('MNIST/t10k-labels.idx1-ubyte', 'rb') as f:\n",
    "        magic, size = struct.unpack(\">II\", f.read(8))\n",
    "        assert(magic==2049)\n",
    "        ret['Y_test'] = np.array(array(\"B\", f.read())).reshape(size,1)\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle and divide the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle_divide_dataset( dataset, len_of_dev=10000 ):\n",
    "    \"\"\"\n",
    "    Shuffle and divide the dataset\n",
    "    \n",
    "    len_of_dev: 10,000 is a reasonable number for dev set.\n",
    "                Dev dataset with this size is big enough to measure variance problem.\n",
    "    \"\"\"       \n",
    "    assert('X_train' in dataset)\n",
    "    assert(len(dataset)==4)\n",
    "    \n",
    "    \"\"\" random shuffle the training set \"\"\"\n",
    "    np.random.seed(1)\n",
    "    permutation = np.random.permutation(dataset['X_train'].shape[0])\n",
    "    dataset['X_train'] = dataset['X_train'][permutation]\n",
    "    dataset['Y_train'] = dataset['Y_train'][permutation]\n",
    "\n",
    "    \"\"\" divide trainset into trainset and devset \"\"\"\n",
    "    dataset['X_dev'] = dataset['X_train'][:len_of_dev]\n",
    "    dataset['Y_dev'] = dataset['Y_train'][:len_of_dev]\n",
    "    dataset['X_train'] = dataset['X_train'][len_of_dev:]\n",
    "    dataset['Y_train'] = dataset['Y_train'][len_of_dev:]\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually check the dataset by random visualize some of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def manually_validate_mnist_dataset(dataset):\n",
    "    \"\"\"Manually check the dataset by random visualize some of them\"\"\"\n",
    "    random_train = np.random.randint(1, len(dataset['X_train']))-1\n",
    "    random_dev = np.random.randint(1, len(dataset['X_dev']))-1\n",
    "    random_test = np.random.randint(1, len(dataset['X_test']))-1\n",
    "    print(dataset['Y_train'][random_train], dataset['Y_dev'][random_dev], dataset['Y_test'][random_test])\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1,3, sharey=True, figsize=[10,3])\n",
    "    ax1.imshow(dataset['X_train'][random_train], cmap='gray')\n",
    "    ax2.imshow(dataset['X_dev'][random_dev], cmap='gray')\n",
    "    mappable = ax3.imshow(dataset['X_test'][random_test], cmap='gray')\n",
    "    fig.colorbar(mappable)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize the dataset\n",
    "\n",
    "$$\n",
    "z = \\frac{x - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "#### Notice: the MNIST gray images have a lot of areas of black (0), relatively few areas of white(255), so the standardization has a result of roughly (-0.4, 2.8).\n",
    "#### Maybe we can also use divide 256 to scale the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standardize( dataset ):\n",
    "    \"\"\"use standard sccore to normalize input dataset\"\"\"\n",
    "    assert('X_train' in dataset)\n",
    "    mu = np.mean(dataset['X_train'], keepdims=False)\n",
    "    sigma = np.std(dataset['X_train'], keepdims=False)\n",
    "    for key, val in dataset.items():\n",
    "        if key[:2]=='X_':\n",
    "            dataset[key] = ( dataset[key] - mu ) / sigma\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess dataset into vectors\n",
    "#### Notice: Andrew's course use the format of vector above, but tensorflow does it in it's transpose way.\n",
    "## Flat Images(X) into vectors, and stack them into matrix\n",
    "\n",
    "#### Input Format: X (m, width, height); Output Format:\n",
    "$$ X = \n",
    "\\begin{bmatrix}\n",
    "\\vert & & \\vert & & \\vert \\\\\n",
    "x^{(1)} & ... & x^{(i)} & ... & x^{(m)} \\\\\n",
    "\\vert & & \\vert & & \\vert\n",
    "\\end{bmatrix} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flat_stack( dataset ):\n",
    "    \"\"\"input dataset format: (m, width, height)\"\"\"\n",
    "    for key, val in dataset.items():\n",
    "        if key[:2]=='X_':\n",
    "            width = dataset[key].shape[1]\n",
    "            height = dataset[key].shape[2]\n",
    "            dataset[key] = dataset[key].reshape(-1, width*height).T\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encode Labels(Y)\n",
    "#### Input Format: Y (m, label_number); output Format:\n",
    "$$\n",
    "Y = \n",
    "\\begin{bmatrix}\n",
    "\\vert & & \\vert & & \\vert \\\\\n",
    "y_{one\\_hot}^{(1)} & ... & y_{one\\_hot}^{(i)} & ... & y_{one\\_hot}^{(m)} \\\\\n",
    "\\vert & & \\vert & & \\vert \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot( dataset ):\n",
    "    min_label_number = np.min(dataset['Y_train'], keepdims=False)\n",
    "    max_label_number = np.max(dataset['Y_train'], keepdims=False)\n",
    "    C = max_label_number - min_label_number + 1\n",
    "    for key, val in dataset.items():\n",
    "        if key[:2]=='Y_':\n",
    "            # all label number should be trained in Y_train\n",
    "            assert(min_label_number <= np.min(dataset[key], keepdims=False))\n",
    "            assert(max_label_number >= np.max(dataset[key], keepdims=False))\n",
    "            Y = dataset[key]\n",
    "            Y_onehot = np.zeros((C, Y.shape[0]))\n",
    "            Y_onehot[Y.reshape(-1).astype(int), np.arange(Y.shape[0])] = 1\n",
    "            dataset[key] = Y_onehot\n",
    "    return dataset\n",
    "\n",
    "def back_one_hot(Y_onehot):\n",
    "    \"\"\" This is an inverse function of one hot, in case we need to interpret the result. \"\"\"\n",
    "    Y = np.repeat( [np.arange(Y_onehot.shape[0])], repeats=Y_onehot.shape[1], axis=0 )\n",
    "    assert(Y.shape == Y_onehot.T.shape)\n",
    "    Y = Y[Y_onehot.T.astype(bool)]\n",
    "    return Y.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init cache, parameters, hyper_parameters\n",
    "\n",
    "**cache** is used for store results in steps, such as Z1, A1, Z2, A2 and so on.\n",
    "\n",
    "**parameters** are the model itself, consist of W and b, which can be adjusted during training.\n",
    "\n",
    "**hyper_parameters** are the settings for training.\n",
    "\n",
    "_(I am still thinking should the structure of the network be a parameter? Or should there be structure, parameters, training conditions?)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init(X_size, hidden_layers=[10,10,5], C=10, learning_rate=0.01, regularization=0.1):\n",
    "    \"\"\" init cache, parameters and hyper_parameters \"\"\"\n",
    "    global cache, parameters, hyper_parameters\n",
    "    cache = {}\n",
    "    parameters = {}\n",
    "    hyper_parameters = {}\n",
    "\n",
    "    \"\"\" init hyper_parameters \"\"\"\n",
    "    hyper_parameters['X_size'] = X_size\n",
    "    # layers of network, include the last softmax layer\n",
    "    # hidden layers use ReLU activation function, and the last layer use Softmax function\n",
    "    hyper_parameters['layers'] = hidden_layers + [C]\n",
    "    # L layers in total\n",
    "    hyper_parameters['L'] = len(hyper_parameters['layers'])\n",
    "    # Class numbers 0-9 is 10\n",
    "    hyper_parameters['C'] = C\n",
    "    hyper_parameters['learning_rate'] = learning_rate\n",
    "    hyper_parameters['regularization'] = regularization\n",
    "    # tiny_float to avoid divide by zero\n",
    "    hyper_parameters['tiny_float'] = 1e-7\n",
    "    \n",
    "    \"\"\" init W b or any other parameters in every layer \"\"\"\n",
    "    layers = hyper_parameters['layers']\n",
    "    x_size = hyper_parameters['X_size']\n",
    "    cells_prev = x_size\n",
    "    for layer_idx, cells in enumerate(layers):\n",
    "        parameters['W'+str(layer_idx+1)] = np.random.randn(cells, cells_prev) * 0.01\n",
    "        parameters['b'+str(layer_idx+1)] = np.zeros((cells, 1))\n",
    "        cells_prev = layers[layer_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Part 2. Neural Network calculations\n",
    "\n",
    "## Core calculation functions and their backpropagations\n",
    "_(I cannot split the backpropagation into seperated independent steps, since I cannot understand Matrix-by-Matrix derivatives.)_\n",
    "\n",
    "_(For example, the linear matrix function: $Y=WX+B$. What is $\\frac{\\partial Y}{\\partial X}$? I assume that will be a four-rank tensor, but I cannot understand that.)_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "ReLu = max(0, x)\n",
    "$$\n",
    "***\n",
    "$$\n",
    "\\frac{\\partial Loss}{\\partial Z} = \n",
    "\\frac{\\partial Loss}{\\partial A} \\cdot \\frac{\\partial A}{\\partial Z} =\n",
    "\\frac{\\partial Loss}{\\partial A} \\cdot (Z >= 0)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    A = Z * ( Z>=0 )\n",
    "    return A\n",
    "def relu_backpropagation(Z, A, dL_dA):\n",
    "    \"\"\" No need for A \"\"\"\n",
    "    dL_dZ = np.multiply(dL_dA, (Z>=0).astype(np.float32))\n",
    "    return dL_dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Softmax = \\frac{\\exp(Z)}{\\sum_i^n{\\exp(Z)}}\n",
    "$$\n",
    "***\n",
    "$$\n",
    "\\frac{\\partial Loss}{\\partial Z} = \n",
    "\\frac{\\partial Loss}{\\partial A} \\cdot \\frac{\\partial A}{\\partial Z} =\n",
    "\\frac{\\partial Loss}{\\partial A} \\cdot (A-A^2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    exp_Z = np.exp(Z)\n",
    "    A = exp_Z / np.sum(exp_Z, axis=0)\n",
    "    return A\n",
    "def softmax_backpropagation(Z, A, dL_dA):\n",
    "    \"\"\" No need for Z \"\"\"\n",
    "    dL_dZ = np.multiply(dL_dA, (A-np.power(A,2)))\n",
    "    return dL_dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Z = W A_{prev} + B\n",
    "$$\n",
    "***\n",
    "$$\n",
    "\\frac{\\partial Loss}{\\partial A_{prev}} = \n",
    "\\frac{\\partial Loss}{\\partial Z} \\cdot \\frac{\\partial Z}{\\partial A_{prev}} =\n",
    "\\frac{\\partial Loss}{\\partial Z} \\cdot A_{prev}^{\\intercal}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial Loss}{\\partial W} = \n",
    "\\frac{\\partial Loss}{\\partial Z} \\cdot \\frac{\\partial Z}{\\partial W} =\n",
    "W^\\intercal \\cdot \\frac{\\partial Loss}{\\partial Z}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial Loss}{\\partial B} = \n",
    "\\frac{\\partial Loss}{\\partial Z} \\cdot \\frac{\\partial Z}{\\partial B} =\n",
    "\\frac{\\partial Loss}{\\partial Z} \\cdot 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear(A_prev, W, B):\n",
    "    Z = np.dot(W, A_prev) + B\n",
    "    return Z\n",
    "def linear_backpropagation(A_prev, W, B, dL_dZ):\n",
    "    \"\"\" No need for B \"\"\"\n",
    "    m = dL_dZ.shape[1] # I still don't know why move 1/m here from backpropagate_cost.\n",
    "    dL_dA_prev = np.dot(W.T, dL_dZ)\n",
    "    dL_dW = 1/m * np.dot(dL_dZ, A_prev.T)\n",
    "    dL_db = 1/m * np.sum(dL_dZ, axis=1).reshape(-1,1)\n",
    "    return dL_dA_prev, dL_dW, dL_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Loss = -\\frac{1}{m} \\sum_i^m \\sum_j^C {(y_j^{(i)}\\log(\\hat{y}_j^{(i)}) + (1-y_j^{(i)})\\log(1-\\hat{y}_j^{(i)}))}\n",
    "$$\n",
    "#### Notice: if I omit the 1e-10 thing, the A should reach 0.0 or 1.0, therefore the result of log or divide will give a runtime error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(A, Y):\n",
    "    \"\"\" where A is Y_hat, loss is L in every dL \"\"\"\n",
    "    cost = np.multiply(Y, np.log(A+1e-10)) + np.multiply(1-Y, np.log(1 - A+1e-10))\n",
    "    loss = -np.mean(np.sum(cost, axis=0), keepdims=False)\n",
    "    return loss\n",
    "def loss_backpropagation(A, Y):\n",
    "    dL_dA = - (np.divide(Y, A+1e-10) - np.divide(1 - Y, 1 - A+1e-10))\n",
    "    return dL_dA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "accuracy = \\frac{\\sum_i^m (y^{[i]}==\\hat{y}^{[i]})}{m}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(Y_hat):\n",
    "    return np.argmax(Y_hat, axis=0).reshape(-1,1)\n",
    "\n",
    "def accuracy(Y_predict, Y):\n",
    "    return np.sum(np.equal(Y_predict, back_one_hot(Y))) / Y_predict.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Loss_{reg} = Loss + \\frac{\\lambda}{2}(\\sum_l^L W^{[l] \\intercal} \\cdot W^{[l]})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Delta \\frac{\\partial L}{\\partial W^{[l]}} = \\frac{\\lambda}{m}W^{[l]}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_with_regularization(loss):\n",
    "    global parameters, hyper_parameters\n",
    "    if hyper_parameters['regularization']==0:\n",
    "        return loss\n",
    "    s = 0\n",
    "    for i in range(hyper_parameters['L']):\n",
    "        L = str(i+1)\n",
    "        s += np.sum(np.mean(np.dot(parameters['W'+L].T, parameters['W'+L]), axis=1), keepdims=False)\n",
    "    \n",
    "    loss += hyper_parameters['regularization'] / 2 * s\n",
    "    return loss\n",
    "\n",
    "def dL_dW_incremental_with_regularization(L):\n",
    "    global cache, parameters, hyper_parameters\n",
    "    if hyper_parameters['regularization']==0:\n",
    "        return 0\n",
    "    m = cache['Z1'].shape[1]\n",
    "    return hyper_parameters['regularization'] * parameters['W'+L] / m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3. Neural network logic\n",
    "\n",
    "```\n",
    "Init all parameters ---> forward propagation ---+---> compute loss\n",
    "                     ^            +             |\n",
    "                     |            |             +---> compute accuracy\n",
    "                     |            v\n",
    "                     |     back propagation\n",
    "                     |            +\n",
    "                     |            |\n",
    "                     |            v\n",
    "                     |     update parameters\n",
    "                     |            +\n",
    "                     |            |\n",
    "                     +------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" neural network logic level \"\"\"\n",
    "\n",
    "def forwardpropagation_all(X, Y):\n",
    "    global cache, parameters, hyper_parameters\n",
    "    layers = hyper_parameters['layers']\n",
    "    A_prev = X\n",
    "    for layer_idx, cells in enumerate(layers):\n",
    "        W = parameters['W'+str(layer_idx+1)]\n",
    "        b = parameters['b'+str(layer_idx+1)]\n",
    "        Z = linear(A_prev, W, b)\n",
    "        if layer_idx==len(layers)-1:\n",
    "            # Last layer's activation function should be softmax\n",
    "            A = softmax(Z)\n",
    "        else:\n",
    "            A = relu(Z)\n",
    "        cache['Z'+str(layer_idx+1)] = Z\n",
    "        cache['A'+str(layer_idx+1)] = A\n",
    "        A_prev = A\n",
    "    return loss_with_regularization(loss(A, Y))\n",
    "\n",
    "def backpropagate_all(X, Y):\n",
    "    global cache, parameters, hyper_parameters\n",
    "    # 1. Last layer has coss function and softmax function\n",
    "    L = str(hyper_parameters['L'])\n",
    "    cache['dA'+L] = loss_backpropagation(cache['A'+L], Y)\n",
    "    AL = cache['A'+L]\n",
    "    dAL = cache['dA'+L]\n",
    "    cache['dZ'+L] = softmax_backpropagation(None, AL, dAL)\n",
    "\n",
    "    # 2. Layers in between are similar: linear and ReLU\n",
    "    for i in np.arange(start=hyper_parameters['L']-1, stop=0, step=-1):\n",
    "        L = str(i)\n",
    "        cache['dA'+L], cache['dW'+str(i+1)], cache['db'+str(i+1)] = \\\n",
    "        linear_backpropagation(cache['A'+L], parameters['W'+str(i+1)], None, cache['dZ'+str(i+1)])\n",
    "        cache['dZ'+L] = relu_backpropagation(cache['Z'+L], None, cache['dA'+L])\n",
    "\n",
    "    # 3. First layer has different parameter.\n",
    "    t = time()\n",
    "    _, cache['dW1'], cache['db1'] = linear_backpropagation(X, parameters['W1'], None, cache['dZ1'])\n",
    "    timer['tmp'] += time()-t\n",
    "\n",
    "def update_parameters():\n",
    "    global cache, parameters\n",
    "    for i in range(hyper_parameters['L']):\n",
    "        L = str(i+1)\n",
    "        parameters['W'+L] -= (cache['dW'+L]+ dL_dW_incremental_with_regularization(L)) * hyper_parameters['learning_rate']        \n",
    "        parameters['b'+L] -= cache['db'+L] * hyper_parameters['learning_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_dev, Y_dev, \n",
    "          learning_rate=0.01, print_every=100, iteration=500, \n",
    "          hidden_layers=[100], batch_size=128, regularization=0):\n",
    "    \n",
    "    init(hidden_layers=hidden_layers, C=Y_train.shape[0], X_size=X_train.shape[0], \n",
    "         learning_rate=learning_rate, regularization=regularization)\n",
    "    \n",
    "    # losses will be returned for plotting\n",
    "    losses = [[],[],[],[]]\n",
    "    m = X_train.shape[1]\n",
    "    step = 0\n",
    "    np.random.seed(1)\n",
    "    t0 = time()\n",
    "    for i in range(iteration):\n",
    "        permutation = np.random.permutation(m)\n",
    "        X_permutated = X_train[:, permutation]\n",
    "        Y_permutated = Y_train[:, permutation]\n",
    "        for j in range( m // batch_size ):\n",
    "            X = X_permutated[:, j*batch_size:(j+1)*batch_size]\n",
    "            Y = Y_permutated[:, j*batch_size:(j+1)*batch_size]\n",
    "            if X.shape[1]==batch_size:\n",
    "                t = time()\n",
    "                loss_value = forwardpropagation_all(X, Y)\n",
    "                timer['forwardpropagation_all'] += time()-t\n",
    "                losses[0].append(loss_value)\n",
    "                losses[1].append(step)\n",
    "                t = time()\n",
    "                backpropagate_all(X, Y)\n",
    "                timer['backpropagate_all'] += time()-t\n",
    "                t = time()\n",
    "                update_parameters()\n",
    "                timer['update_parameters'] += time()-t\n",
    "                \n",
    "                if step%print_every==0:\n",
    "                    Y_hat = cache['A'+str(hyper_parameters['L'])]\n",
    "                    Y_predict = predict(Y_hat)\n",
    "                    accu = accuracy(Y_predict, Y)\n",
    "\n",
    "                    loss_dev_value = forwardpropagation_all(X_dev, Y_dev)\n",
    "                    Y_hat_dev = cache['A'+str(hyper_parameters['L'])]\n",
    "                    losses[2].append(loss_dev_value)\n",
    "                    losses[3].append(step)\n",
    "                    Y_predict_dev = predict(Y_hat_dev)\n",
    "                    accu_dev = accuracy(Y_predict_dev, Y_dev)\n",
    "\n",
    "                    print(step,'>',i,'-th iter, training loss = ',loss_value,'; accu_train = ',accu)\n",
    "                    print(' > dev loss = ',loss_dev_value,'; accu_dev = ', accu_dev)\n",
    "                step += 1\n",
    "                \n",
    "            else:\n",
    "                print(\"Batch_Size ERROR!!!\")\n",
    "        if np.isnan(loss_value):\n",
    "            print(\"ERROR to nan!!!\")\n",
    "            break\n",
    "    timer['total'] = time() - t0\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 > 0 -th iter, training loss =  3.25148105444 ; accu_train =  0.171875\n",
      " > dev loss =  3.24787931303 ; accu_dev =  0.0994\n",
      "400 > 0 -th iter, training loss =  0.526907145796 ; accu_train =  0.90625\n",
      " > dev loss =  0.405185749369 ; accu_dev =  0.93\n",
      "800 > 1 -th iter, training loss =  0.161732741339 ; accu_train =  0.96875\n",
      " > dev loss =  0.290992617632 ; accu_dev =  0.9498\n",
      "1200 > 1 -th iter, training loss =  0.0553791380761 ; accu_train =  1.0\n",
      " > dev loss =  0.204445636661 ; accu_dev =  0.9662\n",
      "1600 > 2 -th iter, training loss =  0.115765936277 ; accu_train =  0.984375\n",
      " > dev loss =  0.191407358802 ; accu_dev =  0.9658\n",
      "2000 > 2 -th iter, training loss =  0.321134005337 ; accu_train =  0.96875\n",
      " > dev loss =  0.185684879835 ; accu_dev =  0.9683\n",
      "2400 > 3 -th iter, training loss =  0.212927292581 ; accu_train =  0.96875\n",
      " > dev loss =  0.161517727563 ; accu_dev =  0.9723\n",
      "2800 > 3 -th iter, training loss =  0.0221132073942 ; accu_train =  1.0\n",
      " > dev loss =  0.147732874362 ; accu_dev =  0.9742\n",
      "3200 > 4 -th iter, training loss =  0.0570597715542 ; accu_train =  0.984375\n",
      " > dev loss =  0.150481000278 ; accu_dev =  0.9743\n",
      "3600 > 4 -th iter, training loss =  0.108341540245 ; accu_train =  0.96875\n",
      " > dev loss =  0.138029205175 ; accu_dev =  0.9768\n",
      "4000 > 5 -th iter, training loss =  0.0553970849336 ; accu_train =  0.984375\n",
      " > dev loss =  0.140574453956 ; accu_dev =  0.9771\n",
      "4400 > 5 -th iter, training loss =  0.0193642785681 ; accu_train =  1.0\n",
      " > dev loss =  0.125718326902 ; accu_dev =  0.9793\n",
      "4800 > 6 -th iter, training loss =  0.0258928411317 ; accu_train =  1.0\n",
      " > dev loss =  0.130945095288 ; accu_dev =  0.979\n",
      "5200 > 6 -th iter, training loss =  0.0251022169335 ; accu_train =  1.0\n",
      " > dev loss =  0.131094640992 ; accu_dev =  0.9789\n",
      "5600 > 7 -th iter, training loss =  0.0150337945608 ; accu_train =  1.0\n",
      " > dev loss =  0.130249155811 ; accu_dev =  0.9796\n",
      "6000 > 7 -th iter, training loss =  0.0393741468678 ; accu_train =  1.0\n",
      " > dev loss =  0.123508163519 ; accu_dev =  0.9797\n",
      "6400 > 8 -th iter, training loss =  0.0213354848285 ; accu_train =  1.0\n",
      " > dev loss =  0.123449054965 ; accu_dev =  0.9811\n",
      "6800 > 8 -th iter, training loss =  0.0106488801678 ; accu_train =  1.0\n",
      " > dev loss =  0.127918644951 ; accu_dev =  0.9796\n",
      "7200 > 9 -th iter, training loss =  0.0112592452952 ; accu_train =  1.0\n",
      " > dev loss =  0.130299572448 ; accu_dev =  0.9811\n",
      "7600 > 9 -th iter, training loss =  0.00172645831733 ; accu_train =  1.0\n",
      " > dev loss =  0.128092816774 ; accu_dev =  0.9808\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEFCAYAAADpIfy5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc4351e96d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total': 114.05971622467041, 'update_parameters': 24.177534580230713, 'forwardpropagation_all': 28.848888874053955, 'backpropagate_all': 52.24351382255554, 'tmp': 21.835800170898438}\n"
     ]
    }
   ],
   "source": [
    "timer = {'forwardpropagation_all':0, 'backpropagate_all':0, 'update_parameters':0, 'tmp':0}\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"test model using random data\"\"\"\n",
    "    m = 10\n",
    "    X = np.random.randn(784,m)\n",
    "    Y = one_hot(np.random.randint(low=0, high=10, size=(m,1)), C=10)\n",
    "    #print(X[:,1])\n",
    "    #print(Y[:,1])\n",
    "    costs = model(X,Y,learning_rate=0.1, print_every=10, iteration=100,\n",
    "                 hidden_layers=[10,10])\n",
    "    plt.plot(costs[1], costs[0])\n",
    "    plt.show()\n",
    "    \n",
    "def main():\n",
    "    \"\"\"test model using real MNIST dataset\"\"\"\n",
    "    mnist = load_mnist()\n",
    "    mnist = shuffle_divide_dataset(mnist)\n",
    "    mnist = standardize(mnist)\n",
    "    mnist = flat_stack(mnist)\n",
    "    mnist = one_hot(mnist)\n",
    "    \n",
    "    costs = \\\n",
    "    model(mnist['X_train'], mnist['Y_train'], mnist['X_dev'], mnist['Y_dev'],\n",
    "                  learning_rate=0.1, print_every=400, iteration=10,\n",
    "                  batch_size=64, regularization=0,\n",
    "                  hidden_layers=[784,784])\n",
    "    \n",
    "    plt.plot(costs[1], costs[0])\n",
    "    plt.plot(costs[3], costs[2])\n",
    "    plt.title(\"Cost Value\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(timer)\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
