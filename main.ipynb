{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this notebook, I am going to implement a deep network from numpy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Python Standard Library struct and array\n",
    "# for dealing with reading dataset from file\n",
    "import struct\n",
    "from array import array\n",
    "\n",
    "# Numpy for calculating\n",
    "import numpy as np\n",
    "\n",
    "# To draw the training and dev cost value curve\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Test Cases\n",
    "import test\n",
    "\n",
    "# Some Useful Helper Functions\n",
    "import helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_mnist():\n",
    "    \"\"\" \n",
    "    load MNIST dataset into numpy array \n",
    "    MNIST dataset can be downloaded manually.\n",
    "    url: http://yann.lecun.com/exdb/mnist/\n",
    "    \"\"\"\n",
    "    ret = {}\n",
    "    with open('MNIST/train-images.idx3-ubyte', 'rb') as f:\n",
    "        magic, size, rows, cols = struct.unpack(\">IIII\", f.read(16))\n",
    "        assert(magic==2051)\n",
    "        ret['X_train'] = np.array(array(\"B\", f.read())).reshape(size,rows,cols)\n",
    "\n",
    "    with open('MNIST/t10k-images.idx3-ubyte', 'rb') as f:\n",
    "        magic, size, rows, cols = struct.unpack(\">IIII\", f.read(16))\n",
    "        assert(magic==2051)\n",
    "        ret['X_test'] = np.array(array(\"B\", f.read())).reshape(size,rows,cols)\n",
    "\n",
    "    with open('MNIST/train-labels.idx1-ubyte', 'rb') as f:\n",
    "        magic, size = struct.unpack(\">II\", f.read(8))\n",
    "        assert(magic==2049)\n",
    "        ret['Y_train'] = np.array(array(\"B\", f.read())).reshape(size,1)\n",
    "\n",
    "    with open('MNIST/t10k-labels.idx1-ubyte', 'rb') as f:\n",
    "        magic, size = struct.unpack(\">II\", f.read(8))\n",
    "        assert(magic==2049)\n",
    "        ret['Y_test'] = np.array(array(\"B\", f.read())).reshape(size,1)\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle and divide the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle_divide_dataset( dataset, len_of_dev=10000 ):\n",
    "    \"\"\"\n",
    "    Shuffle and divide the dataset\n",
    "    \n",
    "    len_of_dev: 10,000 is a reasonable number for dev set.\n",
    "                Dev dataset with this size is big enough to measure variance problem.\n",
    "    \"\"\"       \n",
    "    assert('X_train' in dataset)\n",
    "    assert(len(dataset)==4)\n",
    "    \n",
    "    \"\"\" random shuffle the training set \"\"\"\n",
    "    np.random.seed(1)\n",
    "    permutation = np.random.permutation(dataset['X_train'].shape[0])\n",
    "    dataset['X_train'] = dataset['X_train'][permutation]\n",
    "    dataset['Y_train'] = dataset['Y_train'][permutation]\n",
    "\n",
    "    \"\"\" divide trainset into trainset and devset \"\"\"\n",
    "    dataset['X_dev'] = dataset['X_train'][:len_of_dev]\n",
    "    dataset['Y_dev'] = dataset['Y_train'][:len_of_dev]\n",
    "    dataset['X_train'] = dataset['X_train'][len_of_dev:]\n",
    "    dataset['Y_train'] = dataset['Y_train'][len_of_dev:]\n",
    "\n",
    "    print('X_train:', dataset['X_train'].shape,\n",
    "          'X_dev:', dataset['X_dev'].shape,\n",
    "          'X_test:', dataset['X_test'].shape)\n",
    "    print('Y_train:', dataset['Y_train'].shape,\n",
    "          'Y_dev:', dataset['Y_dev'].shape,\n",
    "          'Y_test:', dataset['Y_test'].shape)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually check the dataset by random visualize some of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def manually_validate_mnist_dataset(dataset):\n",
    "    \"\"\"Manually check the dataset by random visualize some of them\"\"\"\n",
    "    random_train = np.random.randint(1, len(dataset['X_train']))-1\n",
    "    random_dev = np.random.randint(1, len(dataset['X_dev']))-1\n",
    "    random_test = np.random.randint(1, len(dataset['X_test']))-1\n",
    "    print(dataset['Y_train'][random_train], dataset['Y_dev'][random_dev], dataset['Y_test'][random_test])\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1,3, sharey=True, figsize=[10,3])\n",
    "    ax1.imshow(dataset['X_train'][random_train], cmap='gray')\n",
    "    ax2.imshow(dataset['X_dev'][random_dev], cmap='gray')\n",
    "    mappable = ax3.imshow(dataset['X_test'][random_test], cmap='gray')\n",
    "    fig.colorbar(mappable)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize the dataset\n",
    "\n",
    "$$\n",
    "z = \\frac{x - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "#### Notice: the MNIST gray images have a lot of areas of black (0), relatively few areas of white(255), so the standardization has a result of roughly (-0.4, 2.8).\n",
    "#### Maybe we can also use divide 256 to scale the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standardize( dataset ):\n",
    "    \"\"\"use standard sccore to normalize input dataset\"\"\"\n",
    "    assert('X_train' in dataset)\n",
    "    mu = np.mean(dataset['X_train'], keepdims=False)\n",
    "    sigma = np.std(dataset['X_train'], keepdims=False)\n",
    "    for key, val in dataset.items():\n",
    "        if key[:2]=='X_':\n",
    "            dataset[key] = ( dataset[key] - mu ) / sigma\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess dataset into vectors\n",
    "#### Notice: Andrew's course use the format of vector above, but tensorflow does it in it's transpose way.\n",
    "## Flat Images(X) into vectors, and stack them into matrix\n",
    "\n",
    "#### Input Format: X (m, width, height); Output Format:\n",
    "$$ X = \n",
    "\\begin{bmatrix}\n",
    "\\vert & & \\vert & & \\vert \\\\\n",
    "x^{(1)} & ... & x^{(i)} & ... & x^{(m)} \\\\\n",
    "\\vert & & \\vert & & \\vert\n",
    "\\end{bmatrix} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flat_stack( dataset ):\n",
    "    \"\"\"input dataset format: (m, width, height)\"\"\"\n",
    "    for key, val in dataset.items():\n",
    "        if key[:2]=='X_':\n",
    "            width = dataset[key].shape[1]\n",
    "            height = dataset[key].shape[2]\n",
    "            dataset[key] = dataset[key].reshape(-1, width*height).T\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encode Labels(Y)\n",
    "#### Input Format: Y (m, label_number); output Format:\n",
    "$$\n",
    "Y = \n",
    "\\begin{bmatrix}\n",
    "\\vert & & \\vert & & \\vert \\\\\n",
    "y_{one\\_hot}^{(1)} & ... & y_{one\\_hot}^{(i)} & ... & y_{one\\_hot}^{(m)} \\\\\n",
    "\\vert & & \\vert & & \\vert \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot( dataset ):\n",
    "    min_label_number = np.min(dataset['Y_train'], keepdims=False)\n",
    "    max_label_number = np.max(dataset['Y_train'], keepdims=False)\n",
    "    C = max_label_number - min_label_number + 1\n",
    "    for key, val in dataset.items():\n",
    "        if key[:2]=='Y_':\n",
    "            # all label number should be trained in Y_train\n",
    "            assert(min_label_number <= np.min(dataset[key], keepdims=False))\n",
    "            assert(max_label_number >= np.max(dataset[key], keepdims=False))\n",
    "            Y = dataset[key]\n",
    "            Y_onehot = np.zeros((C, Y.shape[0]))\n",
    "            Y_onehot[Y.reshape(-1).astype(int), np.arange(Y.shape[0])] = 1\n",
    "            dataset[key] = Y_onehot\n",
    "    return dataset\n",
    "\n",
    "def back_one_hot(Y_onehot):\n",
    "    \"\"\" This is an inverse function of one hot, in case we need to interpret the result. \"\"\"\n",
    "    Y = np.repeat( [np.arange(Y_onehot.shape[0])], repeats=Y_onehot.shape[1], axis=0 )\n",
    "    assert(Y.shape == Y_onehot.T.shape)\n",
    "    Y = Y[Y_onehot.T.astype(bool)]\n",
    "    return Y.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init cache, parameters, hyper_parameters\n",
    "\n",
    "**cache** is used for store results in steps, such as Z1, A1, Z2, A2 and so on.\n",
    "\n",
    "**parameters** are the model itself, consist of W and b, which can be adjusted during training.\n",
    "\n",
    "**hyper_parameters** are the settings for training.\n",
    "\n",
    "_(I am still thinking should the structure of the network be a parameter? Or should there be structure, parameters, training conditions?)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init(X_size, hidden_layers=[10,10,5], C=10, learning_rate=0.01, regularization=0.1):\n",
    "    \"\"\" init cache, parameters and hyper_parameters \"\"\"\n",
    "    global cache, parameters, hyper_parameters\n",
    "    cache = {}\n",
    "    parameters = {}\n",
    "    hyper_parameters = {}\n",
    "\n",
    "    \"\"\" init hyper_parameters \"\"\"\n",
    "    hyper_parameters['X_size'] = X_size\n",
    "    # layers of network, include the last softmax layer\n",
    "    # hidden layers use ReLU activation function, and the last layer use Softmax function\n",
    "    hyper_parameters['layers'] = hidden_layers + [C]\n",
    "    # L layers in total\n",
    "    hyper_parameters['L'] = len(hyper_parameters['layers'])\n",
    "    # Class numbers 0-9 is 10\n",
    "    hyper_parameters['C'] = C\n",
    "    hyper_parameters['learning_rate'] = learning_rate\n",
    "    hyper_parameters['regularization'] = regularization\n",
    "    # tiny_float to avoid divide by zero\n",
    "    hyper_parameters['tiny_float'] = 1e-7\n",
    "    \n",
    "    \"\"\" init W b or any other parameters in every layer \"\"\"\n",
    "    layers = hyper_parameters['layers']\n",
    "    x_size = hyper_parameters['X_size']\n",
    "    cells_prev = x_size\n",
    "    for layer_idx, cells in enumerate(layers):\n",
    "        parameters['W'+str(layer_idx+1)] = np.random.randn(cells, cells_prev) * 0.01\n",
    "        parameters['b'+str(layer_idx+1)] = np.zeros((cells, 1))\n",
    "        cells_prev = layers[layer_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "# after this line, the code is still waiting for refactory\n",
    "# I paste those code just for testing, make sure I don't have any \n",
    "# mistake above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ReLU(X):\n",
    "    return X * (X > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    s = np.sum(np.exp(X), axis=0)\n",
    "    return np.exp(X) / s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation_each_layer(W, A_prev, b, activation_function=ReLU):\n",
    "    Z = np.dot(W, A_prev) + b\n",
    "    A = activation_function(Z)\n",
    "    return Z,A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(Y_hat, Y):\n",
    "    A = np.multiply(Y, np.log(Y_hat+hyper_parameters['tiny_float']))\n",
    "    B = np.multiply(1-Y, np.log(1-Y_hat+hyper_parameters['tiny_float']))\n",
    "    Loss = -A-B\n",
    "    #why not use Loss = -np.sum(A+B) / m?\n",
    "    return Loss\n",
    "\n",
    "def cost(loss):\n",
    "    return np.mean(loss)\n",
    "\n",
    "def cost_with_regularization(cost):\n",
    "    global parameters, hyper_parameters\n",
    "    if hyper_parameters['regularization']==0:\n",
    "        return cost\n",
    "    s = 0\n",
    "    for i in range(hyper_parameters['L']):\n",
    "        L = str(i+1)\n",
    "        s += np.sum(np.mean(np.dot(parameters['W'+L].T, parameters['W'+L]), axis=1), keepdims=False)\n",
    "    \n",
    "    cost += hyper_parameters['regularization'] / 2 * s\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(Y_hat):\n",
    "    return np.argmax(Y_hat,axis=0).reshape(-1,1)\n",
    "\n",
    "def accuracy(Y_predict, Y):\n",
    "    return np.sum(np.equal(Y_predict, back_one_hot(Y))) / Y_predict.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backpropagate_cost(Y, AL):\n",
    "    #m = Y.shape[1] #why not 1/m?\n",
    "    #dAL = -np.divide(Y, AL) #why not use this formula?\n",
    "    dAL = - (np.divide(Y, AL+hyper_parameters['tiny_float']) - np.divide(1 - Y, 1 - AL+hyper_parameters['tiny_float']))\n",
    "    return dAL\n",
    "\n",
    "def backpropagate_softmax(AL, dAL, Y=None, ZL=None):\n",
    "    dZ = np.multiply(dAL, (AL-np.power(AL,2)))\n",
    "    return dZ\n",
    "\n",
    "def backpropagate_linear(dZ, W, A_prev):\n",
    "    m = dZ.shape[1] # I still don't know why move 1/m here from backpropagate_cost.\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    dW = 1/m * np.dot(dZ, A_prev.T)\n",
    "    db = 1/m * np.sum(dZ, axis=1).reshape(-1,1)\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def backpropagate_ReLU(dA, Z):\n",
    "    return np.multiply(dA, (Z>=0).astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" neural network logic level \"\"\"\n",
    "\n",
    "def forwardpropagation_all(X):\n",
    "    global cache, parameters, hyper_parameters\n",
    "    layers = hyper_parameters['layers']\n",
    "    A_prev = X\n",
    "    for layer_idx, cells in enumerate(layers):\n",
    "        W = parameters['W'+str(layer_idx+1)]\n",
    "        b = parameters['b'+str(layer_idx+1)]\n",
    "        if layer_idx==len(layers)-1:\n",
    "            # Last layer's activation function should be softmax\n",
    "            Z, A = forward_propagation_each_layer(W, A_prev, b, softmax)\n",
    "        else:\n",
    "            Z, A = forward_propagation_each_layer(W, A_prev, b)\n",
    "        cache['Z'+str(layer_idx+1)] = Z\n",
    "        cache['A'+str(layer_idx+1)] = A\n",
    "        A_prev = A\n",
    "\n",
    "def backpropagate_all(X, Y):\n",
    "    global cache, parameters, hyper_parameters\n",
    "    # 1. Last layer has coss function and softmax function\n",
    "    L = str(hyper_parameters['L'])\n",
    "    cache['dA'+L] = backpropagate_cost(Y,cache['A'+L])\n",
    "    AL = cache['A'+L]\n",
    "    dAL = cache['dA'+L]\n",
    "    cache['dZ'+L] = backpropagate_softmax(AL, dAL)\n",
    "\n",
    "    # 2. Layers in between are similar: linear and ReLU\n",
    "    for i in np.arange(start=hyper_parameters['L']-1, stop=0, step=-1):\n",
    "        L = str(i)\n",
    "        cache['dA'+L], cache['dW'+str(i+1)], cache['db'+str(i+1)] = backpropagate_linear(cache['dZ'+str(i+1)], \n",
    "                                                                           parameters['W'+str(i+1)], \n",
    "                                                                           cache['A'+L])\n",
    "        cache['dZ'+L] = backpropagate_ReLU(cache['dA'+L], cache['Z'+L])\n",
    "\n",
    "    # 3. First layer has different parameter.\n",
    "    _, cache['dW1'], cache['db1'] = backpropagate_linear(cache['dZ1'], parameters['W1'], X)\n",
    "\n",
    "def update_parameters():\n",
    "    global cache, parameters\n",
    "    m = cache['dZ1'].shape[1]\n",
    "    for i in range(hyper_parameters['L']):\n",
    "        L = str(i+1)\n",
    "        parameters['W'+L] -= cache['dW'+L] * hyper_parameters['learning_rate'] \\\n",
    "        + hyper_parameters['regularization'] * parameters['W'+L] * hyper_parameters['learning_rate'] / m\n",
    "        parameters['b'+L] -= cache['db'+L] * hyper_parameters['learning_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X_full, Y_full, X_dev, Y_dev, \n",
    "          learning_rate=0.01, print_every=100, iteration=500, \n",
    "          hidden_layers=[100], batch_size=128, regularization=0):\n",
    "    init(hidden_layers=hidden_layers, C=Y_full.shape[0], X_size=X_full.shape[0], \n",
    "         learning_rate=learning_rate, regularization=regularization)\n",
    "    # costs will be returned for plotting\n",
    "    costs = []\n",
    "    costs_index = []\n",
    "    costs_dev = []\n",
    "    costs_dev_index = []\n",
    "    m = X_full.shape[1]\n",
    "    step = 0\n",
    "    np.random.seed(1)\n",
    "    for i in range(iteration):\n",
    "        permutation = np.random.permutation(m)\n",
    "        X_permutated = X_full[:, permutation]\n",
    "        Y_permutated = Y_full[:, permutation]\n",
    "        for j in range( m // batch_size ):\n",
    "            X = X_permutated[:, j*batch_size:(j+1)*batch_size]\n",
    "            Y = Y_permutated[:, j*batch_size:(j+1)*batch_size]\n",
    "            if X.shape[1]==batch_size:\n",
    "                forwardpropagation_all(X)\n",
    "                Y_hat = cache['A'+str(hyper_parameters['L'])]\n",
    "                cost_value = cost_with_regularization(cost(loss(Y_hat, Y)))\n",
    "                costs.append(cost_value)\n",
    "                costs_index.append(step)\n",
    "                backpropagate_all(X, Y)\n",
    "                update_parameters()\n",
    "                \n",
    "                step += 1\n",
    "                if step%print_every==0:\n",
    "                    Y_predict = predict(Y_hat)\n",
    "                    accu = accuracy(Y_predict, Y)\n",
    "\n",
    "                    forwardpropagation_all(X_dev)\n",
    "                    Y_hat_dev = cache['A'+str(hyper_parameters['L'])]\n",
    "                    cost_dev_value = cost_with_regularization(cost(loss(Y_hat_dev, Y_dev)))\n",
    "                    costs_dev.append(cost_dev_value)\n",
    "                    costs_dev_index.append(step)\n",
    "                    Y_predict_dev = predict(Y_hat_dev)\n",
    "                    accu_dev = accuracy(Y_predict_dev, Y_dev)\n",
    "\n",
    "                    print(step,'>',i,'-th iter, training error = ',cost_value,'; accu_train = ',accu)\n",
    "                    print(' > dev error = ',cost_dev_value,'; accu_dev = ', accu_dev)\n",
    "                if np.isnan(cost_value):\n",
    "                    print(\"ERROR to nan!!!\")\n",
    "                    break\n",
    "            else:\n",
    "                print(\"Batch_Size ERROR!!!\")\n",
    "    return costs, costs_index, costs_dev, costs_dev_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (50000, 28, 28) X_dev: (10000, 28, 28) X_test: (10000, 28, 28)\n",
      "Y_train: (50000, 1) Y_dev: (10000, 1) Y_test: (10000, 1)\n",
      "100 > 0 -th iter, training error =  0.0707874748674 ; accu_train =  0.859375\n",
      " > dev error =  0.0592800287817 ; accu_dev =  0.8905\n",
      "200 > 0 -th iter, training error =  0.0397765790065 ; accu_train =  0.9453125\n",
      " > dev error =  0.0331147758066 ; accu_dev =  0.9408\n",
      "300 > 0 -th iter, training error =  0.0281383620182 ; accu_train =  0.9453125\n",
      " > dev error =  0.0281435353728 ; accu_dev =  0.9528\n",
      "400 > 1 -th iter, training error =  0.0207596016396 ; accu_train =  0.9453125\n",
      " > dev error =  0.0253027252948 ; accu_dev =  0.9572\n",
      "500 > 1 -th iter, training error =  0.0168401238311 ; accu_train =  0.9609375\n",
      " > dev error =  0.0233806247853 ; accu_dev =  0.9603\n",
      "600 > 1 -th iter, training error =  0.00579511963313 ; accu_train =  0.9921875\n",
      " > dev error =  0.0178080258046 ; accu_dev =  0.9705\n",
      "700 > 1 -th iter, training error =  0.0183387479383 ; accu_train =  0.9609375\n",
      " > dev error =  0.0187389458595 ; accu_dev =  0.9675\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEFCAYAAAAPCDf9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFCWN+H+Hh7DOJQqjXgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2f64ffca58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def test_model():\n",
    "    \"\"\"test model using random data\"\"\"\n",
    "    m = 10\n",
    "    X = np.random.randn(784,m)\n",
    "    Y = one_hot(np.random.randint(low=0, high=10, size=(m,1)), C=10)\n",
    "    #print(X[:,1])\n",
    "    #print(Y[:,1])\n",
    "    costs = model(X,Y,learning_rate=0.1, print_every=10, iteration=100,\n",
    "                 hidden_layers=[10,10])\n",
    "    plt.plot(costs)\n",
    "    plt.show()\n",
    "    \n",
    "def test_model_1():\n",
    "    \"\"\"test model using real MNIST dataset\"\"\"\n",
    "    mnist = load_mnist()\n",
    "    mnist = shuffle_divide_dataset(mnist)\n",
    "    mnist = standardize(mnist)\n",
    "    mnist = flat_stack(mnist)\n",
    "    mnist = one_hot(mnist)\n",
    "    \n",
    "    costs, costs_index, costs_dev, costs_dev_index = \\\n",
    "    model(mnist['X_train'], mnist['Y_train'], mnist['X_dev'], mnist['Y_dev'],\n",
    "                  learning_rate=0.3, print_every=100, iteration=2,\n",
    "                  batch_size=128, regularization=0,\n",
    "                  hidden_layers=[784,100])\n",
    "    \n",
    "    plt.plot(costs_index, costs)\n",
    "    plt.plot(costs_dev_index, costs_dev)\n",
    "    plt.title(\"Cost Value\")\n",
    "    plt.show()\n",
    "    \n",
    "test_model_1()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
