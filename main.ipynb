{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this notebook, I am going to implement a deep network from numpy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Python Standard Library struct and array\n",
    "# for dealing with reading dataset from file\n",
    "import struct\n",
    "from array import array\n",
    "from time import time\n",
    "\n",
    "# Numpy for calculating\n",
    "import numpy as np\n",
    "\n",
    "# To draw the training and dev cost value curve\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Test Cases\n",
    "import test\n",
    "\n",
    "# Some Useful Helper Functions\n",
    "import helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. Preparing Data\n",
    "\n",
    "## Load MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_mnist():\n",
    "    \"\"\" \n",
    "    load MNIST dataset into numpy array \n",
    "    MNIST dataset can be downloaded manually.\n",
    "    url: http://yann.lecun.com/exdb/mnist/\n",
    "    \"\"\"\n",
    "    ret = {}\n",
    "    with open('MNIST/train-images.idx3-ubyte', 'rb') as f:\n",
    "        magic, size, rows, cols = struct.unpack(\">IIII\", f.read(16))\n",
    "        assert(magic==2051)\n",
    "        ret['X_train'] = np.array(array(\"B\", f.read())).reshape(size,rows,cols)\n",
    "\n",
    "    with open('MNIST/t10k-images.idx3-ubyte', 'rb') as f:\n",
    "        magic, size, rows, cols = struct.unpack(\">IIII\", f.read(16))\n",
    "        assert(magic==2051)\n",
    "        ret['X_test'] = np.array(array(\"B\", f.read())).reshape(size,rows,cols)\n",
    "\n",
    "    with open('MNIST/train-labels.idx1-ubyte', 'rb') as f:\n",
    "        magic, size = struct.unpack(\">II\", f.read(8))\n",
    "        assert(magic==2049)\n",
    "        ret['Y_train'] = np.array(array(\"B\", f.read())).reshape(size,1)\n",
    "\n",
    "    with open('MNIST/t10k-labels.idx1-ubyte', 'rb') as f:\n",
    "        magic, size = struct.unpack(\">II\", f.read(8))\n",
    "        assert(magic==2049)\n",
    "        ret['Y_test'] = np.array(array(\"B\", f.read())).reshape(size,1)\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle and divide the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle_divide_dataset( dataset, len_of_dev=10000 ):\n",
    "    \"\"\"\n",
    "    Shuffle and divide the dataset\n",
    "    \n",
    "    len_of_dev: 10,000 is a reasonable number for dev set.\n",
    "                Dev dataset with this size is big enough to measure variance problem.\n",
    "    \"\"\"       \n",
    "    assert('X_train' in dataset)\n",
    "    assert(len(dataset)==4)\n",
    "    \n",
    "    \"\"\" random shuffle the training set \"\"\"\n",
    "    np.random.seed(1)\n",
    "    permutation = np.random.permutation(dataset['X_train'].shape[0])\n",
    "    dataset['X_train'] = dataset['X_train'][permutation]\n",
    "    dataset['Y_train'] = dataset['Y_train'][permutation]\n",
    "\n",
    "    \"\"\" divide trainset into trainset and devset \"\"\"\n",
    "    dataset['X_dev'] = dataset['X_train'][:len_of_dev]\n",
    "    dataset['Y_dev'] = dataset['Y_train'][:len_of_dev]\n",
    "    dataset['X_train'] = dataset['X_train'][len_of_dev:]\n",
    "    dataset['Y_train'] = dataset['Y_train'][len_of_dev:]\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually check the dataset by random visualize some of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def manually_validate_mnist_dataset(dataset):\n",
    "    \"\"\"Manually check the dataset by random visualize some of them\"\"\"\n",
    "    random_train = np.random.randint(1, len(dataset['X_train']))-1\n",
    "    random_dev = np.random.randint(1, len(dataset['X_dev']))-1\n",
    "    random_test = np.random.randint(1, len(dataset['X_test']))-1\n",
    "    print(dataset['Y_train'][random_train], dataset['Y_dev'][random_dev], dataset['Y_test'][random_test])\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1,3, sharey=True, figsize=[10,3])\n",
    "    ax1.imshow(dataset['X_train'][random_train], cmap='gray')\n",
    "    ax2.imshow(dataset['X_dev'][random_dev], cmap='gray')\n",
    "    mappable = ax3.imshow(dataset['X_test'][random_test], cmap='gray')\n",
    "    fig.colorbar(mappable)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize the dataset\n",
    "\n",
    "$$\n",
    "z = \\frac{x - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "#### Notice: the MNIST gray images have a lot of areas of black (0), relatively few areas of white(255), so the standardization has a result of roughly (-0.4, 2.8).\n",
    "#### Maybe we can also use divide 256 to scale the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standardize( dataset ):\n",
    "    \"\"\"use standard sccore to normalize input dataset\"\"\"\n",
    "    assert('X_train' in dataset)\n",
    "    mu = np.mean(dataset['X_train'], keepdims=False)\n",
    "    sigma = np.std(dataset['X_train'], keepdims=False)\n",
    "    for key, val in dataset.items():\n",
    "        if key[:2]=='X_':\n",
    "            dataset[key] = ( dataset[key] - mu ) / sigma\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess dataset into vectors\n",
    "#### Notice: Andrew's course use the format of vector above, but tensorflow does it in it's transpose way.\n",
    "## Flat Images(X) into vectors, and stack them into matrix\n",
    "\n",
    "#### Input Format: X (m, width, height); Output Format:\n",
    "$$ X = \n",
    "\\begin{bmatrix}\n",
    "\\vert & & \\vert & & \\vert \\\\\n",
    "x^{(1)} & ... & x^{(i)} & ... & x^{(m)} \\\\\n",
    "\\vert & & \\vert & & \\vert\n",
    "\\end{bmatrix} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flat_stack( dataset ):\n",
    "    \"\"\"input dataset format: (m, width, height)\"\"\"\n",
    "    for key, val in dataset.items():\n",
    "        if key[:2]=='X_':\n",
    "            width = dataset[key].shape[1]\n",
    "            height = dataset[key].shape[2]\n",
    "            dataset[key] = dataset[key].reshape(-1, width*height).T\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encode Labels(Y)\n",
    "#### Input Format: Y (m, label_number); output Format:\n",
    "$$\n",
    "Y = \n",
    "\\begin{bmatrix}\n",
    "\\vert & & \\vert & & \\vert \\\\\n",
    "y_{one\\_hot}^{(1)} & ... & y_{one\\_hot}^{(i)} & ... & y_{one\\_hot}^{(m)} \\\\\n",
    "\\vert & & \\vert & & \\vert \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot( dataset ):\n",
    "    min_label_number = np.min(dataset['Y_train'], keepdims=False)\n",
    "    max_label_number = np.max(dataset['Y_train'], keepdims=False)\n",
    "    C = max_label_number - min_label_number + 1\n",
    "    for key, val in dataset.items():\n",
    "        if key[:2]=='Y_':\n",
    "            # all label number should be trained in Y_train\n",
    "            assert(min_label_number <= np.min(dataset[key], keepdims=False))\n",
    "            assert(max_label_number >= np.max(dataset[key], keepdims=False))\n",
    "            Y = dataset[key]\n",
    "            Y_onehot = np.zeros((C, Y.shape[0]))\n",
    "            Y_onehot[Y.reshape(-1).astype(int), np.arange(Y.shape[0])] = 1\n",
    "            dataset[key] = Y_onehot\n",
    "    return dataset\n",
    "\n",
    "def back_one_hot(Y_onehot):\n",
    "    \"\"\" This is an inverse function of one hot, in case we need to interpret the result. \"\"\"\n",
    "    Y = np.repeat( [np.arange(Y_onehot.shape[0])], repeats=Y_onehot.shape[1], axis=0 )\n",
    "    assert(Y.shape == Y_onehot.T.shape)\n",
    "    Y = Y[Y_onehot.T.astype(bool)]\n",
    "    return Y.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init cache, parameters, hyper_parameters\n",
    "\n",
    "**cache** is used for store results in steps, such as Z1, A1, Z2, A2 and so on.\n",
    "\n",
    "**parameters** are the model itself, consist of W and b, which can be adjusted during training.\n",
    "\n",
    "**hyper_parameters** are the settings for training.\n",
    "\n",
    "_(I am still thinking should the structure of the network be a parameter? Or should there be structure, parameters, training conditions?)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init(X_size, hidden_layers=[10,10,5], C=10, learning_rate=0.01, \n",
    "         regularization=0.1, keep_parameters=True, keep_prop=1):\n",
    "    \"\"\" init cache, parameters and hyper_parameters \"\"\"\n",
    "    global cache, parameters, hyper_parameters\n",
    "\n",
    "    #if the layers changed, there is no way to keep parameters.\n",
    "    if 'hyper_parameters' in globals():\n",
    "        if hyper_parameters['layers']!=hidden_layers+[C]:\n",
    "            print(hyper_parameters['layers'],hidden_layers+[C])\n",
    "            keep_parameters=False\n",
    "            \n",
    "    cache = {}\n",
    "    hyper_parameters = {}\n",
    "\n",
    "    \"\"\" init hyper_parameters \"\"\"\n",
    "    hyper_parameters['X_size'] = X_size\n",
    "    # layers of network, include the last softmax layer\n",
    "    # hidden layers use ReLU activation function, and the last layer use Softmax function\n",
    "    hyper_parameters['layers'] = hidden_layers + [C]\n",
    "    # L layers in total\n",
    "    hyper_parameters['L'] = len(hyper_parameters['layers'])\n",
    "    # Class numbers 0-9 is 10\n",
    "    hyper_parameters['C'] = C\n",
    "    hyper_parameters['learning_rate'] = learning_rate\n",
    "    hyper_parameters['regularization'] = regularization\n",
    "    hyper_parameters['keep_prop'] = keep_prop\n",
    "    \n",
    "    \"\"\" init W b or any other parameters in every layer \"\"\"\n",
    "    layers = hyper_parameters['layers']\n",
    "    x_size = hyper_parameters['X_size']\n",
    "    if not keep_parameters or 'parameters' not in globals():\n",
    "        parameters = {}\n",
    "        print(\"re-init parameters.\")\n",
    "        cells_prev = x_size\n",
    "        for layer_idx, cells in enumerate(layers):\n",
    "            parameters['W'+str(layer_idx+1)] = np.random.randn(cells, cells_prev) * 0.01\n",
    "            parameters['b'+str(layer_idx+1)] = np.zeros((cells, 1))\n",
    "            cells_prev = layers[layer_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Part 2. Neural Network calculations\n",
    "\n",
    "## Core calculation functions and their backpropagations\n",
    "_(I cannot split the backpropagation into seperated independent steps, since I cannot understand Matrix-by-Matrix derivatives.)_\n",
    "\n",
    "_(For example, the linear matrix function: $Y=WX+B$. What is $\\frac{\\partial Y}{\\partial X}$? I assume that will be a four-rank tensor, but I cannot understand that.)_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "ReLu = max(0, x)\n",
    "$$\n",
    "***\n",
    "$$\n",
    "\\frac{\\partial Loss}{\\partial Z} = \n",
    "\\frac{\\partial Loss}{\\partial A} \\cdot \\frac{\\partial A}{\\partial Z} =\n",
    "\\frac{\\partial Loss}{\\partial A} \\cdot (Z >= 0)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    A = Z * ( Z>=0 )\n",
    "    return A\n",
    "def relu_backpropagation(Z, A, dL_dA):\n",
    "    \"\"\" No need for A \"\"\"\n",
    "    dL_dZ = np.multiply(dL_dA, (Z>=0).astype(np.float32))\n",
    "    return dL_dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Softmax = \\frac{\\exp(Z)}{\\sum_i^n{\\exp(Z)}}\n",
    "$$\n",
    "\n",
    "***\n",
    "I am not sure about the formula of derivative of Softmax with m examples batched. If m=1, I have this formula:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial Loss}{\\partial Z} = \n",
    "\\frac{\\partial Loss}{\\partial A} \\cdot \\frac{\\partial A}{\\partial Z} =\n",
    "\\frac{\\partial Loss}{\\partial A} \\cdot A * (I-A^{\\intercal})\n",
    "$$\n",
    "\n",
    "***\n",
    "It's lucky that I noticed tensorflow combines softmax and loss functions together into one softmax_and_cross_entropy function, so I just use that way to calculate, skipping calculating dL_dA:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial Loss}{\\partial Z} = \n",
    "\\hat{Y} - Y\n",
    "$$\n",
    "\n",
    "But there are some strange notation after that, I don't know why it should look like this, I added them to pass the gradient check:\n",
    "$$\n",
    "\\frac{n}{(n-1)m}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    Z = Z - np.max(Z, axis=0) # This line is avoid exp overflow, since [1,2] and [1001,1002] have the same softmax result.\n",
    "    exp_Z = np.exp(Z)\n",
    "    A = exp_Z / np.sum(exp_Z, axis=0)\n",
    "    return A\n",
    "def softmax_loss_backpropagation(A, Y):\n",
    "    \"\"\"\n",
    "    Directly calculate dL_dZ from A and Y, combining softmax and loss functions.\n",
    "    It's quite difficult to calculate them seperately.\n",
    "    In fact, I can't calculate derivative of softmax with mini-batches.\n",
    "    \"\"\"\n",
    "    m = A.shape[1]\n",
    "    n = A.shape[0]\n",
    "    dL_dZ = (A - Y) * n / (n-1) / m\n",
    "    return dL_dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Z = W A_{prev} + B\n",
    "$$\n",
    "***\n",
    "$$\n",
    "\\frac{\\partial Loss}{\\partial A_{prev}} = \n",
    "\\frac{\\partial Loss}{\\partial Z} \\cdot \\frac{\\partial Z}{\\partial A_{prev}} =\n",
    "\\frac{\\partial Loss}{\\partial Z} \\cdot A_{prev}^{\\intercal}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial Loss}{\\partial W} = \n",
    "\\frac{\\partial Loss}{\\partial Z} \\cdot \\frac{\\partial Z}{\\partial W} =\n",
    "W^\\intercal \\cdot \\frac{\\partial Loss}{\\partial Z}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial Loss}{\\partial B} = \n",
    "\\frac{\\partial Loss}{\\partial Z} \\cdot \\frac{\\partial Z}{\\partial B} =\n",
    "\\frac{\\partial Loss}{\\partial Z} \\cdot 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear(A_prev, W, B):\n",
    "    Z = np.dot(W, A_prev) + B\n",
    "    return Z\n",
    "def linear_backpropagation(A_prev, W, B, dL_dZ):\n",
    "    \"\"\" No need for B \"\"\"\n",
    "    m = dL_dZ.shape[1]\n",
    "    dL_dA_prev = np.dot(W.T, dL_dZ)\n",
    "    dL_dW = 1 * np.dot(dL_dZ, A_prev.T)\n",
    "    dL_db = 1 * np.sum(dL_dZ, axis=1).reshape(-1,1)\n",
    "    return dL_dA_prev, dL_dW, dL_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Loss = -\\frac{1}{m} \\sum_i^m \\sum_j^C {(y_j^{(i)}\\log(\\hat{y}_j^{(i)}) + (1-y_j^{(i)})\\log(1-\\hat{y}_j^{(i)}))}\n",
    "$$\n",
    "#### Notice: if I omit the 1e-10 thing, the A should reach 0.0 or 1.0, therefore the result of log or divide will give a runtime error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TINY = 1e-10\n",
    "def loss(A, Y):\n",
    "    \"\"\" where A is Y_hat, loss is L in every dL \"\"\"\n",
    "    cost = np.multiply(Y, np.log(A+TINY)) + np.multiply(1-Y, np.log(1 - A+TINY))\n",
    "    loss = -np.mean(np.sum(cost, axis=0), keepdims=False)\n",
    "    return loss\n",
    "def loss_backpropagation(A, Y):\n",
    "    m = A.shape[1]\n",
    "    dL_dA = - (np.divide(Y, A+TINY) - np.divide(1 - Y, 1 - A+TINY)) / m\n",
    "    return dL_dA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "accuracy = \\frac{\\sum_i^m (y^{[i]}==\\hat{y}^{[i]})}{m}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(Y_hat):\n",
    "    return np.argmax(Y_hat, axis=0).reshape(-1,1)\n",
    "\n",
    "def accuracy(Y_predict, Y):\n",
    "    return np.sum(np.equal(Y_predict, back_one_hot(Y))) / Y_predict.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3. Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 Regularization\n",
    "$$\n",
    "Loss_{reg} = Loss + \\frac{\\lambda}{2}(\\sum_l^L W^{[l] \\intercal} \\cdot W^{[l]})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Delta \\frac{\\partial L}{\\partial W^{[l]}} = \\frac{\\lambda}{m}W^{[l]}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_with_regularization(loss):\n",
    "    global parameters, hyper_parameters\n",
    "    if hyper_parameters['regularization']==0:\n",
    "        return loss\n",
    "    s = 0\n",
    "    for i in range(hyper_parameters['L']):\n",
    "        L = str(i+1)\n",
    "        s += np.sum(np.mean(np.dot(parameters['W'+L].T, parameters['W'+L]), axis=1), keepdims=False)\n",
    "    \n",
    "    loss += hyper_parameters['regularization'] / 2 * s\n",
    "    return loss\n",
    "\n",
    "def dL_dW_incremental_with_regularization(L):\n",
    "    global cache, parameters, hyper_parameters\n",
    "    if hyper_parameters['regularization']==0:\n",
    "        return 0\n",
    "    m = cache['Z1'].shape[1]\n",
    "    return hyper_parameters['regularization'] * parameters['W'+L] / m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dropout(A, layer):\n",
    "    global cache\n",
    "    if layer>0 and layer<hyper_parameters['L'] and hyper_parameters['keep_prop']<1:\n",
    "        cache['dropout'+str(layer)] = np.random.rand(A.shape[0], A.shape[1]) < hyper_parameters['keep_prop']\n",
    "        A = A * cache['dropout'+str(layer)] / hyper_parameters['keep_prop']\n",
    "    return A\n",
    "def dropout_backpropagation(A, layer, dA):\n",
    "    \"\"\" No need for A \"\"\"\n",
    "    global cache\n",
    "    if layer>0 and layer<hyper_parameters['L'] and hyper_parameters['keep_prop']<1:\n",
    "        dA = dA * cache['dropout'+str(layer)]\n",
    "    return dA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4. Neural network logic\n",
    "\n",
    "```\n",
    "Init all parameters ---> forward propagation ---+---> compute loss\n",
    "                     ^            +             |\n",
    "                     |            |             +---> compute accuracy\n",
    "                     |            v\n",
    "                     |     back propagation\n",
    "                     |            +\n",
    "                     |            |\n",
    "                     |            v\n",
    "                     |     update parameters\n",
    "                     |            +\n",
    "                     |            |\n",
    "                     +------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" neural network logic level \"\"\"\n",
    "\n",
    "def forwardpropagation_all(X, Y, with_dropout=True):\n",
    "    global cache, parameters, hyper_parameters\n",
    "    layers = hyper_parameters['layers']\n",
    "    A_prev = X\n",
    "    for layer_idx, cells in enumerate(layers):\n",
    "        W = parameters['W'+str(layer_idx+1)]\n",
    "        b = parameters['b'+str(layer_idx+1)]\n",
    "        Z = linear(A_prev, W, b)\n",
    "        if layer_idx==len(layers)-1:\n",
    "            # Last layer's activation function should be softmax\n",
    "            A = softmax(Z)\n",
    "        else:\n",
    "            A = relu(Z)\n",
    "        if with_dropout:\n",
    "            A = dropout(A, layer_idx+1)\n",
    "        cache['Z'+str(layer_idx+1)] = Z\n",
    "        cache['A'+str(layer_idx+1)] = A\n",
    "        A_prev = A\n",
    "    return loss_with_regularization(loss(A, Y))\n",
    "\n",
    "def backpropagate_all(X, Y, with_dropout=True):\n",
    "    global cache, parameters, hyper_parameters\n",
    "    # 1. Last layer has coss function and softmax function\n",
    "    L = str(hyper_parameters['L'])\n",
    "    \"\"\"\n",
    "    cache['dA'+L] = loss_backpropagation(cache['A'+L], Y)\n",
    "    dAL = cache['dA'+L]\n",
    "    cache['dZ'+L] = softmax_backpropagation(None, AL, dAL)\n",
    "    Instead, We calculate dZL directly from A and Y\n",
    "    \"\"\"\n",
    "    AL = cache['A'+L]\n",
    "    cache['dZ'+L] = softmax_loss_backpropagation(AL, Y)\n",
    "\n",
    "    # 2. Layers in between are similar: linear and ReLU\n",
    "    for i in np.arange(start=hyper_parameters['L']-1, stop=0, step=-1):\n",
    "        L = str(i)\n",
    "        cache['dA'+L], cache['dW'+str(i+1)], cache['db'+str(i+1)] = \\\n",
    "        linear_backpropagation(cache['A'+L], parameters['W'+str(i+1)], None, cache['dZ'+str(i+1)])\n",
    "        if with_dropout:\n",
    "            cache['dA'+L] = dropout_backpropagation(None, i, cache['dA'+L])\n",
    "        cache['dZ'+L] = relu_backpropagation(cache['Z'+L], None, cache['dA'+L])\n",
    "\n",
    "    # 3. First layer has different parameter.\n",
    "    t = time()\n",
    "    _, cache['dW1'], cache['db1'] = linear_backpropagation(X, parameters['W1'], None, cache['dZ1'])\n",
    "    timer['tmp'] += time()-t\n",
    "\n",
    "def update_parameters():\n",
    "    global cache, parameters\n",
    "    for i in range(hyper_parameters['L']):\n",
    "        L = str(i+1)\n",
    "        parameters['W'+L] -= (cache['dW'+L]+ dL_dW_incremental_with_regularization(L)) * hyper_parameters['learning_rate']        \n",
    "        parameters['b'+L] -= cache['db'+L] * hyper_parameters['learning_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_dev, Y_dev, \n",
    "          learning_rate=0.01, print_every=100, iteration=500, \n",
    "          hidden_layers=[100], batch_size=128, regularization=0,\n",
    "          keep_parameters=True, keep_prop=1):\n",
    "    \n",
    "    init(hidden_layers=hidden_layers, C=Y_train.shape[0], X_size=X_train.shape[0], \n",
    "         learning_rate=learning_rate, regularization=regularization, \n",
    "         keep_parameters=keep_parameters, keep_prop=keep_prop)\n",
    "    \n",
    "    # losses will be returned for plotting\n",
    "    losses = [[],[],[],[]]\n",
    "    m = X_train.shape[1]\n",
    "    step = 0\n",
    "    np.random.seed(1)\n",
    "    t0 = time()\n",
    "    for i in range(iteration):\n",
    "        permutation = np.random.permutation(m)\n",
    "        X_permutated = X_train[:, permutation]\n",
    "        Y_permutated = Y_train[:, permutation]\n",
    "        for j in range( m // batch_size ):\n",
    "            X = X_permutated[:, j*batch_size:(j+1)*batch_size]\n",
    "            Y = Y_permutated[:, j*batch_size:(j+1)*batch_size]\n",
    "            if X.shape[1]==batch_size:\n",
    "                t = time()\n",
    "                loss_value = forwardpropagation_all(X, Y)\n",
    "                timer['forwardpropagation_all'] += time()-t\n",
    "                t = time()\n",
    "                backpropagate_all(X, Y)\n",
    "                timer['backpropagate_all'] += time()-t\n",
    "                t = time()\n",
    "                update_parameters()\n",
    "                timer['update_parameters'] += time()-t\n",
    "                if step%(print_every//10+1)==0:\n",
    "                    losses[0].append(loss_value)\n",
    "                    losses[1].append(step)\n",
    "                if step%print_every==0:\n",
    "                    Y_hat = cache['A'+str(hyper_parameters['L'])]\n",
    "                    Y_predict = predict(Y_hat)\n",
    "                    accu = accuracy(Y_predict, Y)\n",
    "\n",
    "                    loss_dev_value = forwardpropagation_all(X_dev, Y_dev, with_dropout=False)\n",
    "                    Y_hat_dev = cache['A'+str(hyper_parameters['L'])]\n",
    "                    losses[2].append(loss_dev_value)\n",
    "                    losses[3].append(step)\n",
    "                    Y_predict_dev = predict(Y_hat_dev)\n",
    "                    accu_dev = accuracy(Y_predict_dev, Y_dev)\n",
    "\n",
    "                    print(step,'>',i,'-th iter, training loss = ',loss_value,'; accu_train = ',accu)\n",
    "                    print(' > dev loss = ',loss_dev_value,'; accu_dev = ', accu_dev, '\\n')\n",
    "                step += 1\n",
    "                \n",
    "            else:\n",
    "                print(\"Batch_Size ERROR!!!\")\n",
    "        if np.isnan(loss_value):\n",
    "            print(\"ERROR to nan!!!\")\n",
    "            break\n",
    "    timer['total'] = time() - t0\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re-init parameters.\n",
      "0 > 0 -th iter, training loss =  3.25384142622 ; accu_train =  0.0625\n",
      " > dev loss =  3.24895153358 ; accu_dev =  0.109 \n",
      "\n",
      "1000 > 2 -th iter, training loss =  0.116089411778 ; accu_train =  0.9921875\n",
      " > dev loss =  0.220779724182 ; accu_dev =  0.96 \n",
      "\n",
      "2000 > 5 -th iter, training loss =  0.166677898506 ; accu_train =  0.9765625\n",
      " > dev loss =  0.165315784555 ; accu_dev =  0.9716 \n",
      "\n",
      "3000 > 7 -th iter, training loss =  0.0835638952957 ; accu_train =  0.984375\n",
      " > dev loss =  0.131650488784 ; accu_dev =  0.9767 \n",
      "\n",
      "4000 > 10 -th iter, training loss =  0.0841584745819 ; accu_train =  0.9921875\n",
      " > dev loss =  0.123457976118 ; accu_dev =  0.9782 \n",
      "\n",
      "5000 > 12 -th iter, training loss =  0.0339384748229 ; accu_train =  1.0\n",
      " > dev loss =  0.118010400452 ; accu_dev =  0.9806 \n",
      "\n",
      "6000 > 15 -th iter, training loss =  0.0626121668859 ; accu_train =  0.984375\n",
      " > dev loss =  0.121769568156 ; accu_dev =  0.98 \n",
      "\n",
      "7000 > 17 -th iter, training loss =  0.00832436705083 ; accu_train =  1.0\n",
      " > dev loss =  0.121951396481 ; accu_dev =  0.9802 \n",
      "\n",
      "8000 > 20 -th iter, training loss =  0.00382426962237 ; accu_train =  1.0\n",
      " > dev loss =  0.124009897412 ; accu_dev =  0.9803 \n",
      "\n",
      "9000 > 23 -th iter, training loss =  0.0170666990938 ; accu_train =  1.0\n",
      " > dev loss =  0.121426489056 ; accu_dev =  0.9812 \n",
      "\n",
      "10000 > 25 -th iter, training loss =  0.0112128479906 ; accu_train =  1.0\n",
      " > dev loss =  0.126072464188 ; accu_dev =  0.9821 \n",
      "\n",
      "11000 > 28 -th iter, training loss =  0.0113698667278 ; accu_train =  1.0\n",
      " > dev loss =  0.12502573745 ; accu_dev =  0.9816 \n",
      "\n",
      "12000 > 30 -th iter, training loss =  0.00899187248346 ; accu_train =  1.0\n",
      " > dev loss =  0.124753559378 ; accu_dev =  0.9833 \n",
      "\n",
      "13000 > 33 -th iter, training loss =  0.00316317873262 ; accu_train =  1.0\n",
      " > dev loss =  0.126656609882 ; accu_dev =  0.9826 \n",
      "\n",
      "14000 > 35 -th iter, training loss =  0.00683270713602 ; accu_train =  1.0\n",
      " > dev loss =  0.127786620158 ; accu_dev =  0.9831 \n",
      "\n",
      "15000 > 38 -th iter, training loss =  0.00198128184252 ; accu_train =  1.0\n",
      " > dev loss =  0.129785072822 ; accu_dev =  0.9828 \n",
      "\n",
      "16000 > 41 -th iter, training loss =  0.00563757468474 ; accu_train =  1.0\n",
      " > dev loss =  0.133960711912 ; accu_dev =  0.9825 \n",
      "\n",
      "17000 > 43 -th iter, training loss =  0.00227442169464 ; accu_train =  1.0\n",
      " > dev loss =  0.134523122388 ; accu_dev =  0.982 \n",
      "\n",
      "18000 > 46 -th iter, training loss =  0.00255015996019 ; accu_train =  1.0\n",
      " > dev loss =  0.129420332418 ; accu_dev =  0.9824 \n",
      "\n",
      "19000 > 48 -th iter, training loss =  0.00131861261715 ; accu_train =  1.0\n",
      " > dev loss =  0.13125317622 ; accu_dev =  0.9818 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEFCAYAAADt1CyEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\naAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7bb515add8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'update_parameters': 73.22195863723755, 'total': 584.751088142395, 'forwardpropagation_all': 242.0554347038269, 'backpropagate_all': 257.19710540771484, 'tmp': 81.49314045906067}\n",
      "final test set accuracy:  0.9829\n"
     ]
    }
   ],
   "source": [
    "timer = {'forwardpropagation_all':0, 'backpropagate_all':0, 'update_parameters':0, 'tmp':0}\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"test model using random data\"\"\"\n",
    "    m = 10\n",
    "    X = np.random.randn(784,m)\n",
    "    Y = one_hot(np.random.randint(low=0, high=10, size=(m,1)))\n",
    "    #print(X[:,1])\n",
    "    #print(Y[:,1])\n",
    "    costs = model(X,Y,learning_rate=0.1, print_every=1, iteration=1,\n",
    "                 hidden_layers=[10,2])\n",
    "    plt.plot(costs[1], costs[0])\n",
    "    plt.show()\n",
    "    \n",
    "def main(trial=True):\n",
    "    \"\"\"test model using real MNIST dataset\"\"\"\n",
    "    mnist = load_mnist()\n",
    "    mnist = shuffle_divide_dataset(mnist)\n",
    "    mnist = standardize(mnist)\n",
    "    mnist = flat_stack(mnist)\n",
    "    mnist = one_hot(mnist)\n",
    "    \n",
    "    if trial:\n",
    "        print(\"Trail\")\n",
    "        costs = model(mnist['X_train'][:,:16], mnist['Y_train'][:,:16], mnist['X_dev'], mnist['Y_dev'],\n",
    "                  learning_rate=0.1, print_every=100, iteration=1,\n",
    "                  batch_size=8, regularization=0, keep_prop=0.3,\n",
    "                  hidden_layers=[5,2])\n",
    "    else:\n",
    "        costs = model(mnist['X_train'], mnist['Y_train'], mnist['X_dev'], mnist['Y_dev'],\n",
    "                  learning_rate=0.1, print_every=1000, iteration=50,\n",
    "                  batch_size=128, regularization=0, keep_prop=0.8,\n",
    "                  hidden_layers=[784,1024])\n",
    "        \n",
    "    plt.plot(costs[1], costs[0])\n",
    "    plt.plot(costs[3], costs[2])\n",
    "    plt.title(\"Cost Value\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(timer)\n",
    "\n",
    "    forwardpropagation_all(mnist['X_test'], mnist['Y_test'], with_dropout=False)\n",
    "    Y_hat_test = cache['A'+str(hyper_parameters['L'])]\n",
    "    Y_predict_test = predict(Y_hat_test)\n",
    "    accu_test = accuracy(Y_predict_test, mnist['Y_test'])\n",
    "    print('final test set accuracy: ', accu_test)\n",
    "    \n",
    "main(trial=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate Result =  [[-0.00788258  0.03832934]\n",
      " [ 0.00287326  0.00975664]]\n",
      "Calculated gradient =  [[-0.00811102  0.03876382]\n",
      " [ 0.00319633  0.00850782]]\n",
      "0.0550442322486\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This section is used for testing gradient.\n",
    "After testing passed, and bugs are eliminated, they should not run any more during trainning.\n",
    "\"\"\"\n",
    "def test_gradient_general(loss_function, loss_back_function, epsilon):\n",
    "    m = 30\n",
    "    nx = 7\n",
    "    ny = m\n",
    "    X = np.random.rand(nx,m)\n",
    "    Y = np.identity(m)\n",
    "    W = np.random.randn(m, nx)\n",
    "    b = np.random.randn(m,1)\n",
    "    \n",
    "    Approximate = np.zeros(X.shape)\n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(X.shape[1]):\n",
    "            X_p = X.copy()\n",
    "            X_p[i][j] += epsilon\n",
    "            Loss_p = loss_function(X_p, Y, W, b)\n",
    "            X_m = X.copy()\n",
    "            X_m[i][j] -= epsilon\n",
    "            Loss_m = loss_function(X_m, Y, W, b)\n",
    "            Approximate[i][j] = (Loss_p - Loss_m) / 2 / epsilon\n",
    "    Calculated = loss_back_function(X, Y, W, b)\n",
    "    difference = np.linalg.norm(Calculated - Approximate) / (np.linalg.norm(Calculated) + np.linalg.norm(Approximate))\n",
    "    print(\"Approximate Result = \", Approximate[:2,:2])\n",
    "    print(\"Calculated gradient = \", Calculated[:2,:2])\n",
    "    return difference\n",
    "\n",
    "\"\"\" \n",
    "These testing functions might be a little tricky. \n",
    "We pass two functions into the general testing function and let the testing function call them to compute difference.\n",
    "*args can stand for any arguments passed into those functions.\n",
    "\"\"\"\n",
    "def test_loss_gradient(epsilon=1e-10): #passed\n",
    "    def loss_function(*args):\n",
    "        # args: X Y\n",
    "        return loss(args[0], args[1])\n",
    "    def loss_back_function(*args):\n",
    "        return loss_backpropagation(args[0], args[1])\n",
    "    return test_gradient_general(loss_function, loss_back_function, epsilon)\n",
    "\n",
    "def test_relu_loss_gradient(epsilon=1e-10): #passed\n",
    "    def loss_function(*args):\n",
    "        # args: X Y\n",
    "        return loss(relu(args[0]), args[1])\n",
    "    def loss_back_function(*args):\n",
    "        A = relu(args[0])\n",
    "        dL_dA = loss_backpropagation(A, args[1])\n",
    "        dL_dZ = relu_backpropagation(args[0], A, dL_dA)\n",
    "        return dL_dZ\n",
    "    return test_gradient_general(loss_function, loss_back_function, epsilon)\n",
    "\n",
    "def test_softmax_loss_gradient(epsilon=1e-10): # I guess it is passed...\n",
    "    def loss_function(*args):\n",
    "        # args: X Y\n",
    "        return loss(softmax(args[0]), args[1])\n",
    "    def loss_back_function(*args):\n",
    "        A = softmax(args[0])\n",
    "        #print(\"Y_hat = \", A[:2,:2])\n",
    "        dL_dZ = softmax_loss_backpropagation(A, args[1])\n",
    "        return dL_dZ\n",
    "    return test_gradient_general(loss_function, loss_back_function, epsilon)\n",
    "        \n",
    "def test_linear_softmax_loss_gradient(epsilon=1e-10): # \n",
    "    np.random.seed(1)\n",
    "    def loss_function(*args):\n",
    "        # args: X Y W b\n",
    "        return loss(softmax( linear(args[0], args[2], args[3]) ), args[1])\n",
    "    def loss_back_function(*args):\n",
    "        A = softmax( linear(args[0], args[2], args[3]) )\n",
    "        #print(\"Y_hat = \", A[:2,:2])\n",
    "        dL_dZ = softmax_loss_backpropagation(A, args[1])\n",
    "        dL_dA_prev,_,_ = linear_backpropagation(args[0], args[2], args[3], dL_dZ)\n",
    "        return dL_dA_prev\n",
    "    return test_gradient_general(loss_function, loss_back_function, epsilon)\n",
    "        \n",
    "print(test_linear_softmax_loss_gradient(epsilon=1e-10)) #still have relatively large difference on this step."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
