{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this notebook, I am going to implement a deep network from numpy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Python Standard Library struct and array\n",
    "# for dealing with reading dataset from file\n",
    "import struct\n",
    "from array import array\n",
    "from time import time\n",
    "\n",
    "# Numpy for calculating\n",
    "import numpy as np\n",
    "\n",
    "# To draw the training and dev cost value curve\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Test Cases\n",
    "import test\n",
    "\n",
    "# Some Useful Helper Functions\n",
    "import helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. Preparing Data\n",
    "\n",
    "## Load MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_mnist():\n",
    "    \"\"\" \n",
    "    load MNIST dataset into numpy array \n",
    "    MNIST dataset can be downloaded manually.\n",
    "    url: http://yann.lecun.com/exdb/mnist/\n",
    "    \"\"\"\n",
    "    ret = {}\n",
    "    with open('MNIST/train-images.idx3-ubyte', 'rb') as f:\n",
    "        magic, size, rows, cols = struct.unpack(\">IIII\", f.read(16))\n",
    "        assert(magic==2051)\n",
    "        ret['X_train'] = np.array(array(\"B\", f.read())).reshape(size,rows,cols)\n",
    "\n",
    "    with open('MNIST/t10k-images.idx3-ubyte', 'rb') as f:\n",
    "        magic, size, rows, cols = struct.unpack(\">IIII\", f.read(16))\n",
    "        assert(magic==2051)\n",
    "        ret['X_test'] = np.array(array(\"B\", f.read())).reshape(size,rows,cols)\n",
    "\n",
    "    with open('MNIST/train-labels.idx1-ubyte', 'rb') as f:\n",
    "        magic, size = struct.unpack(\">II\", f.read(8))\n",
    "        assert(magic==2049)\n",
    "        ret['Y_train'] = np.array(array(\"B\", f.read())).reshape(size,1)\n",
    "\n",
    "    with open('MNIST/t10k-labels.idx1-ubyte', 'rb') as f:\n",
    "        magic, size = struct.unpack(\">II\", f.read(8))\n",
    "        assert(magic==2049)\n",
    "        ret['Y_test'] = np.array(array(\"B\", f.read())).reshape(size,1)\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle and divide the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle_divide_dataset( dataset, len_of_dev=10000 ):\n",
    "    \"\"\"\n",
    "    Shuffle and divide the dataset\n",
    "    \n",
    "    len_of_dev: 10,000 is a reasonable number for dev set.\n",
    "                Dev dataset with this size is big enough to measure variance problem.\n",
    "    \"\"\"       \n",
    "    assert('X_train' in dataset)\n",
    "    assert(len(dataset)==4)\n",
    "    \n",
    "    \"\"\" random shuffle the training set \"\"\"\n",
    "    np.random.seed(1)\n",
    "    permutation = np.random.permutation(dataset['X_train'].shape[0])\n",
    "    dataset['X_train'] = dataset['X_train'][permutation]\n",
    "    dataset['Y_train'] = dataset['Y_train'][permutation]\n",
    "\n",
    "    \"\"\" divide trainset into trainset and devset \"\"\"\n",
    "    dataset['X_dev'] = dataset['X_train'][:len_of_dev]\n",
    "    dataset['Y_dev'] = dataset['Y_train'][:len_of_dev]\n",
    "    dataset['X_train'] = dataset['X_train'][len_of_dev:]\n",
    "    dataset['Y_train'] = dataset['Y_train'][len_of_dev:]\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually check the dataset by random visualize some of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def manually_validate_mnist_dataset(dataset):\n",
    "    \"\"\"Manually check the dataset by random visualize some of them\"\"\"\n",
    "    random_train = np.random.randint(1, len(dataset['X_train']))-1\n",
    "    random_dev = np.random.randint(1, len(dataset['X_dev']))-1\n",
    "    random_test = np.random.randint(1, len(dataset['X_test']))-1\n",
    "    print(dataset['Y_train'][random_train], dataset['Y_dev'][random_dev], dataset['Y_test'][random_test])\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1,3, sharey=True, figsize=[10,3])\n",
    "    ax1.imshow(dataset['X_train'][random_train], cmap='gray')\n",
    "    ax2.imshow(dataset['X_dev'][random_dev], cmap='gray')\n",
    "    mappable = ax3.imshow(dataset['X_test'][random_test], cmap='gray')\n",
    "    fig.colorbar(mappable)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize the dataset\n",
    "\n",
    "$$\n",
    "z = \\frac{x - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "#### Notice: the MNIST gray images have a lot of areas of black (0), relatively few areas of white(255), so the standardization has a result of roughly (-0.4, 2.8).\n",
    "#### Maybe we can also use divide 256 to scale the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standardize( dataset ):\n",
    "    \"\"\"use standard sccore to normalize input dataset\"\"\"\n",
    "    assert('X_train' in dataset)\n",
    "    mu = np.mean(dataset['X_train'], keepdims=False)\n",
    "    sigma = np.std(dataset['X_train'], keepdims=False)\n",
    "    for key, val in dataset.items():\n",
    "        if key[:2]=='X_':\n",
    "            dataset[key] = ( dataset[key] - mu ) / sigma\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess dataset into vectors\n",
    "#### Notice: Andrew's course use the format of vector above, but tensorflow does it in it's transpose way.\n",
    "## Flat Images(X) into vectors, and stack them into matrix\n",
    "\n",
    "#### Input Format: X (m, width, height); Output Format:\n",
    "$$ X = \n",
    "\\begin{bmatrix}\n",
    "\\vert & & \\vert & & \\vert \\\\\n",
    "x^{(1)} & ... & x^{(i)} & ... & x^{(m)} \\\\\n",
    "\\vert & & \\vert & & \\vert\n",
    "\\end{bmatrix} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flat_stack( dataset ):\n",
    "    \"\"\"input dataset format: (m, width, height)\"\"\"\n",
    "    for key, val in dataset.items():\n",
    "        if key[:2]=='X_':\n",
    "            width = dataset[key].shape[1]\n",
    "            height = dataset[key].shape[2]\n",
    "            dataset[key] = dataset[key].reshape(-1, width*height).T\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encode Labels(Y)\n",
    "#### Input Format: Y (m, label_number); output Format:\n",
    "$$\n",
    "Y = \n",
    "\\begin{bmatrix}\n",
    "\\vert & & \\vert & & \\vert \\\\\n",
    "y_{one\\_hot}^{(1)} & ... & y_{one\\_hot}^{(i)} & ... & y_{one\\_hot}^{(m)} \\\\\n",
    "\\vert & & \\vert & & \\vert \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot( dataset ):\n",
    "    min_label_number = np.min(dataset['Y_train'], keepdims=False)\n",
    "    max_label_number = np.max(dataset['Y_train'], keepdims=False)\n",
    "    C = max_label_number - min_label_number + 1\n",
    "    for key, val in dataset.items():\n",
    "        if key[:2]=='Y_':\n",
    "            # all label number should be trained in Y_train\n",
    "            assert(min_label_number <= np.min(dataset[key], keepdims=False))\n",
    "            assert(max_label_number >= np.max(dataset[key], keepdims=False))\n",
    "            Y = dataset[key]\n",
    "            Y_onehot = np.zeros((C, Y.shape[0]))\n",
    "            Y_onehot[Y.reshape(-1).astype(int), np.arange(Y.shape[0])] = 1\n",
    "            dataset[key] = Y_onehot\n",
    "    return dataset\n",
    "\n",
    "def back_one_hot(Y_onehot):\n",
    "    \"\"\" This is an inverse function of one hot, in case we need to interpret the result. \"\"\"\n",
    "    Y = np.repeat( [np.arange(Y_onehot.shape[0])], repeats=Y_onehot.shape[1], axis=0 )\n",
    "    assert(Y.shape == Y_onehot.T.shape)\n",
    "    Y = Y[Y_onehot.T.astype(bool)]\n",
    "    return Y.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init cache, parameters, hyper_parameters\n",
    "\n",
    "**cache** is used for store results in steps, such as Z1, A1, Z2, A2 and so on.\n",
    "\n",
    "**parameters** are the model itself, consist of W and b, which can be adjusted during training.\n",
    "\n",
    "**hyper_parameters** are the settings for training.\n",
    "\n",
    "_(I am still thinking should the structure of the network be a parameter? Or should there be structure, parameters, training conditions?)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init(X_size, hidden_layers=[10,10,5], C=10, learning_rate=0.01, \n",
    "         regularization=0.1, keep_parameters=True, keep_prop=1):\n",
    "    \"\"\" init cache, parameters and hyper_parameters \"\"\"\n",
    "    global cache, parameters, hyper_parameters\n",
    "\n",
    "    #if the layers changed, there is no way to keep parameters.\n",
    "    if 'hyper_parameters' in globals():\n",
    "        if hyper_parameters['layers']!=hidden_layers+[C]:\n",
    "            print(hyper_parameters['layers'],hidden_layers+[C])\n",
    "            keep_parameters=False\n",
    "            \n",
    "    cache = {}\n",
    "    hyper_parameters = {}\n",
    "\n",
    "    \"\"\" init hyper_parameters \"\"\"\n",
    "    hyper_parameters['X_size'] = X_size\n",
    "    # layers of network, include the last softmax layer\n",
    "    # hidden layers use ReLU activation function, and the last layer use Softmax function\n",
    "    hyper_parameters['layers'] = hidden_layers + [C]\n",
    "    # L layers in total\n",
    "    hyper_parameters['L'] = len(hyper_parameters['layers'])\n",
    "    # Class numbers 0-9 is 10\n",
    "    hyper_parameters['C'] = C\n",
    "    hyper_parameters['learning_rate'] = learning_rate\n",
    "    hyper_parameters['regularization'] = regularization\n",
    "    hyper_parameters['keep_prop'] = keep_prop\n",
    "    \n",
    "    \"\"\" init W b or any other parameters in every layer \"\"\"\n",
    "    layers = hyper_parameters['layers']\n",
    "    x_size = hyper_parameters['X_size']\n",
    "    if not keep_parameters or 'parameters' not in globals():\n",
    "        parameters = {}\n",
    "        print(\"re-init parameters.\")\n",
    "        cells_prev = x_size\n",
    "        for layer_idx, cells in enumerate(layers):\n",
    "            parameters['W'+str(layer_idx+1)] = np.random.randn(cells, cells_prev) * 0.01\n",
    "            parameters['b'+str(layer_idx+1)] = np.zeros((cells, 1))\n",
    "            cells_prev = layers[layer_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Part 2. Neural Network calculations\n",
    "\n",
    "## Core calculation functions and their backpropagations\n",
    "_(I cannot split the backpropagation into seperated independent steps, since I cannot understand Matrix-by-Matrix derivatives.)_\n",
    "\n",
    "_(For example, the linear matrix function: $Y=WX+B$. What is $\\frac{\\partial Y}{\\partial X}$? I assume that will be a four-rank tensor, but I cannot understand that.)_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "ReLu = max(0, x)\n",
    "$$\n",
    "***\n",
    "$$\n",
    "\\frac{\\partial Loss}{\\partial Z} = \n",
    "\\frac{\\partial Loss}{\\partial A} \\cdot \\frac{\\partial A}{\\partial Z} =\n",
    "\\frac{\\partial Loss}{\\partial A} \\cdot (Z >= 0)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    A = Z * ( Z>=0 )\n",
    "    return A\n",
    "def relu_backpropagation(Z, A, dL_dA):\n",
    "    \"\"\" No need for A \"\"\"\n",
    "    dL_dZ = np.multiply(dL_dA, (Z>=0).astype(np.float32))\n",
    "    return dL_dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Softmax = \\frac{\\exp(Z)}{\\sum_i^n{\\exp(Z)}}\n",
    "$$\n",
    "***\n",
    "$$\n",
    "\\frac{\\partial Loss}{\\partial Z} = \n",
    "\\frac{\\partial Loss}{\\partial A} \\cdot \\frac{\\partial A}{\\partial Z} =\n",
    "\\frac{\\partial Loss}{\\partial A} \\cdot (A-A^2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    Z = Z - np.max(Z, axis=0) # This line is avoid exp overflow, since [1,2] and [1001,1002] have the same softmax result.\n",
    "    exp_Z = np.exp(Z)\n",
    "    A = exp_Z / np.sum(exp_Z, axis=0)\n",
    "    return A\n",
    "def softmax_backpropagation(Z, A, dL_dA):\n",
    "    \"\"\" No need for Z \"\"\"\n",
    "    dL_dZ = np.multiply(dL_dA, (A-np.power(A,2)))\n",
    "    return dL_dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Z = W A_{prev} + B\n",
    "$$\n",
    "***\n",
    "$$\n",
    "\\frac{\\partial Loss}{\\partial A_{prev}} = \n",
    "\\frac{\\partial Loss}{\\partial Z} \\cdot \\frac{\\partial Z}{\\partial A_{prev}} =\n",
    "\\frac{\\partial Loss}{\\partial Z} \\cdot A_{prev}^{\\intercal}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial Loss}{\\partial W} = \n",
    "\\frac{\\partial Loss}{\\partial Z} \\cdot \\frac{\\partial Z}{\\partial W} =\n",
    "W^\\intercal \\cdot \\frac{\\partial Loss}{\\partial Z}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial Loss}{\\partial B} = \n",
    "\\frac{\\partial Loss}{\\partial Z} \\cdot \\frac{\\partial Z}{\\partial B} =\n",
    "\\frac{\\partial Loss}{\\partial Z} \\cdot 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear(A_prev, W, B):\n",
    "    Z = np.dot(W, A_prev) + B\n",
    "    return Z\n",
    "def linear_backpropagation(A_prev, W, B, dL_dZ):\n",
    "    \"\"\" No need for B \"\"\"\n",
    "    m = dL_dZ.shape[1] # I still don't know why move 1/m here from backpropagate_cost.\n",
    "    dL_dA_prev = np.dot(W.T, dL_dZ)\n",
    "    dL_dW = 1/m * np.dot(dL_dZ, A_prev.T)\n",
    "    dL_db = 1/m * np.sum(dL_dZ, axis=1).reshape(-1,1)\n",
    "    return dL_dA_prev, dL_dW, dL_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Loss = -\\frac{1}{m} \\sum_i^m \\sum_j^C {(y_j^{(i)}\\log(\\hat{y}_j^{(i)}) + (1-y_j^{(i)})\\log(1-\\hat{y}_j^{(i)}))}\n",
    "$$\n",
    "#### Notice: if I omit the 1e-10 thing, the A should reach 0.0 or 1.0, therefore the result of log or divide will give a runtime error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(A, Y):\n",
    "    \"\"\" where A is Y_hat, loss is L in every dL \"\"\"\n",
    "    cost = np.multiply(Y, np.log(A+1e-10)) + np.multiply(1-Y, np.log(1 - A+1e-10))\n",
    "    loss = -np.mean(np.sum(cost, axis=0), keepdims=False)\n",
    "    return loss\n",
    "def loss_backpropagation(A, Y):\n",
    "    dL_dA = - (np.divide(Y, A+1e-10) - np.divide(1 - Y, 1 - A+1e-10))\n",
    "    return dL_dA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "accuracy = \\frac{\\sum_i^m (y^{[i]}==\\hat{y}^{[i]})}{m}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(Y_hat):\n",
    "    return np.argmax(Y_hat, axis=0).reshape(-1,1)\n",
    "\n",
    "def accuracy(Y_predict, Y):\n",
    "    return np.sum(np.equal(Y_predict, back_one_hot(Y))) / Y_predict.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Loss_{reg} = Loss + \\frac{\\lambda}{2}(\\sum_l^L W^{[l] \\intercal} \\cdot W^{[l]})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Delta \\frac{\\partial L}{\\partial W^{[l]}} = \\frac{\\lambda}{m}W^{[l]}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_with_regularization(loss):\n",
    "    global parameters, hyper_parameters\n",
    "    if hyper_parameters['regularization']==0:\n",
    "        return loss\n",
    "    s = 0\n",
    "    for i in range(hyper_parameters['L']):\n",
    "        L = str(i+1)\n",
    "        s += np.sum(np.mean(np.dot(parameters['W'+L].T, parameters['W'+L]), axis=1), keepdims=False)\n",
    "    \n",
    "    loss += hyper_parameters['regularization'] / 2 * s\n",
    "    return loss\n",
    "\n",
    "def dL_dW_incremental_with_regularization(L):\n",
    "    global cache, parameters, hyper_parameters\n",
    "    if hyper_parameters['regularization']==0:\n",
    "        return 0\n",
    "    m = cache['Z1'].shape[1]\n",
    "    return hyper_parameters['regularization'] * parameters['W'+L] / m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dropout(A, layer):\n",
    "    global cache\n",
    "    if layer>0 and layer<hyper_parameters['L'] and hyper_parameters['keep_prop']<1:\n",
    "        cache['dropout'+str(layer)] = np.random.rand(A.shape[0], A.shape[1]) < hyper_parameters['keep_prop']\n",
    "        A = A * cache['dropout'+str(layer)] / hyper_parameters['keep_prop']\n",
    "    return A\n",
    "def dropout_backpropagation(A, layer, dA):\n",
    "    \"\"\" No need for A \"\"\"\n",
    "    global cache\n",
    "    if layer>0 and layer<hyper_parameters['L'] and hyper_parameters['keep_prop']<1:\n",
    "        dA = dA * cache['dropout'+str(layer)]\n",
    "    return dA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3. Neural network logic\n",
    "\n",
    "```\n",
    "Init all parameters ---> forward propagation ---+---> compute loss\n",
    "                     ^            +             |\n",
    "                     |            |             +---> compute accuracy\n",
    "                     |            v\n",
    "                     |     back propagation\n",
    "                     |            +\n",
    "                     |            |\n",
    "                     |            v\n",
    "                     |     update parameters\n",
    "                     |            +\n",
    "                     |            |\n",
    "                     +------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" neural network logic level \"\"\"\n",
    "\n",
    "def forwardpropagation_all(X, Y, with_dropout=True):\n",
    "    global cache, parameters, hyper_parameters\n",
    "    layers = hyper_parameters['layers']\n",
    "    A_prev = X\n",
    "    for layer_idx, cells in enumerate(layers):\n",
    "        W = parameters['W'+str(layer_idx+1)]\n",
    "        b = parameters['b'+str(layer_idx+1)]\n",
    "        Z = linear(A_prev, W, b)\n",
    "        if layer_idx==len(layers)-1:\n",
    "            # Last layer's activation function should be softmax\n",
    "            A = softmax(Z)\n",
    "        else:\n",
    "            A = relu(Z)\n",
    "        if with_dropout:\n",
    "            A = dropout(A, layer_idx+1)\n",
    "        cache['Z'+str(layer_idx+1)] = Z\n",
    "        cache['A'+str(layer_idx+1)] = A\n",
    "        A_prev = A\n",
    "    return loss_with_regularization(loss(A, Y))\n",
    "\n",
    "def backpropagate_all(X, Y, with_dropout=True):\n",
    "    global cache, parameters, hyper_parameters\n",
    "    # 1. Last layer has coss function and softmax function\n",
    "    L = str(hyper_parameters['L'])\n",
    "    cache['dA'+L] = loss_backpropagation(cache['A'+L], Y)\n",
    "    AL = cache['A'+L]\n",
    "    dAL = cache['dA'+L]\n",
    "    cache['dZ'+L] = softmax_backpropagation(None, AL, dAL)\n",
    "\n",
    "    # 2. Layers in between are similar: linear and ReLU\n",
    "    for i in np.arange(start=hyper_parameters['L']-1, stop=0, step=-1):\n",
    "        L = str(i)\n",
    "        cache['dA'+L], cache['dW'+str(i+1)], cache['db'+str(i+1)] = \\\n",
    "        linear_backpropagation(cache['A'+L], parameters['W'+str(i+1)], None, cache['dZ'+str(i+1)])\n",
    "        if with_dropout:\n",
    "            cache['dA'+L] = dropout_backpropagation(None, i, cache['dA'+L])\n",
    "        cache['dZ'+L] = relu_backpropagation(cache['Z'+L], None, cache['dA'+L])\n",
    "\n",
    "    # 3. First layer has different parameter.\n",
    "    t = time()\n",
    "    _, cache['dW1'], cache['db1'] = linear_backpropagation(X, parameters['W1'], None, cache['dZ1'])\n",
    "    timer['tmp'] += time()-t\n",
    "\n",
    "def update_parameters():\n",
    "    global cache, parameters\n",
    "    for i in range(hyper_parameters['L']):\n",
    "        L = str(i+1)\n",
    "        parameters['W'+L] -= (cache['dW'+L]+ dL_dW_incremental_with_regularization(L)) * hyper_parameters['learning_rate']        \n",
    "        parameters['b'+L] -= cache['db'+L] * hyper_parameters['learning_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_dev, Y_dev, \n",
    "          learning_rate=0.01, print_every=100, iteration=500, \n",
    "          hidden_layers=[100], batch_size=128, regularization=0,\n",
    "          keep_prop=1):\n",
    "    \n",
    "    init(hidden_layers=hidden_layers, C=Y_train.shape[0], X_size=X_train.shape[0], \n",
    "         learning_rate=learning_rate, regularization=regularization, keep_prop=keep_prop)\n",
    "    \n",
    "    # losses will be returned for plotting\n",
    "    losses = [[],[],[],[]]\n",
    "    m = X_train.shape[1]\n",
    "    step = 0\n",
    "    np.random.seed(1)\n",
    "    t0 = time()\n",
    "    for i in range(iteration):\n",
    "        permutation = np.random.permutation(m)\n",
    "        X_permutated = X_train[:, permutation]\n",
    "        Y_permutated = Y_train[:, permutation]\n",
    "        for j in range( m // batch_size ):\n",
    "            X = X_permutated[:, j*batch_size:(j+1)*batch_size]\n",
    "            Y = Y_permutated[:, j*batch_size:(j+1)*batch_size]\n",
    "            if X.shape[1]==batch_size:\n",
    "                t = time()\n",
    "                loss_value = forwardpropagation_all(X, Y)\n",
    "                timer['forwardpropagation_all'] += time()-t\n",
    "                t = time()\n",
    "                backpropagate_all(X, Y)\n",
    "                timer['backpropagate_all'] += time()-t\n",
    "                t = time()\n",
    "                update_parameters()\n",
    "                timer['update_parameters'] += time()-t\n",
    "                if step%(print_every//10)==0:\n",
    "                    losses[0].append(loss_value)\n",
    "                    losses[1].append(step)\n",
    "                if step%print_every==0:\n",
    "                    Y_hat = cache['A'+str(hyper_parameters['L'])]\n",
    "                    Y_predict = predict(Y_hat)\n",
    "                    accu = accuracy(Y_predict, Y)\n",
    "\n",
    "                    loss_dev_value = forwardpropagation_all(X_dev, Y_dev, with_dropout=False)\n",
    "                    Y_hat_dev = cache['A'+str(hyper_parameters['L'])]\n",
    "                    losses[2].append(loss_dev_value)\n",
    "                    losses[3].append(step)\n",
    "                    Y_predict_dev = predict(Y_hat_dev)\n",
    "                    accu_dev = accuracy(Y_predict_dev, Y_dev)\n",
    "\n",
    "                    print(step,'>',i,'-th iter, training loss = ',loss_value,'; accu_train = ',accu)\n",
    "                    print(' > dev loss = ',loss_dev_value,'; accu_dev = ', accu_dev, '\\n')\n",
    "                step += 1\n",
    "                \n",
    "            else:\n",
    "                print(\"Batch_Size ERROR!!!\")\n",
    "        if np.isnan(loss_value):\n",
    "            print(\"ERROR to nan!!!\")\n",
    "            break\n",
    "    timer['total'] = time() - t0\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 > 0 -th iter, training loss =  0.0104924670664 ; accu_train =  1.0\n",
      " > dev loss =  0.151576971107 ; accu_dev =  0.9811 \n",
      "\n",
      "500 > 1 -th iter, training loss =  0.00872621078224 ; accu_train =  1.0\n",
      " > dev loss =  0.157268083401 ; accu_dev =  0.982 \n",
      "\n",
      "1000 > 2 -th iter, training loss =  0.00584899087109 ; accu_train =  1.0\n",
      " > dev loss =  0.158690516038 ; accu_dev =  0.9826 \n",
      "\n",
      "1500 > 3 -th iter, training loss =  0.0622095534087 ; accu_train =  0.984375\n",
      " > dev loss =  0.165732759062 ; accu_dev =  0.9817 \n",
      "\n",
      "2000 > 5 -th iter, training loss =  0.00482599373826 ; accu_train =  1.0\n",
      " > dev loss =  0.163975646177 ; accu_dev =  0.9821 \n",
      "\n",
      "2500 > 6 -th iter, training loss =  0.135403653859 ; accu_train =  0.984375\n",
      " > dev loss =  0.167321263138 ; accu_dev =  0.9808 \n",
      "\n",
      "3000 > 7 -th iter, training loss =  0.0305113104312 ; accu_train =  0.9921875\n",
      " > dev loss =  0.153466825335 ; accu_dev =  0.9824 \n",
      "\n",
      "3500 > 8 -th iter, training loss =  0.0328277377585 ; accu_train =  0.984375\n",
      " > dev loss =  0.163689371137 ; accu_dev =  0.9805 \n",
      "\n",
      "4000 > 10 -th iter, training loss =  0.0539391857771 ; accu_train =  0.9921875\n",
      " > dev loss =  0.160070675131 ; accu_dev =  0.9813 \n",
      "\n",
      "4500 > 11 -th iter, training loss =  0.059687932794 ; accu_train =  0.9921875\n",
      " > dev loss =  0.157025591642 ; accu_dev =  0.9819 \n",
      "\n",
      "5000 > 12 -th iter, training loss =  0.0443312868824 ; accu_train =  0.984375\n",
      " > dev loss =  0.152328409205 ; accu_dev =  0.9819 \n",
      "\n",
      "5500 > 14 -th iter, training loss =  0.00635324794035 ; accu_train =  1.0\n",
      " > dev loss =  0.150580042167 ; accu_dev =  0.9824 \n",
      "\n",
      "6000 > 15 -th iter, training loss =  0.0377357004616 ; accu_train =  0.9921875\n",
      " > dev loss =  0.155056901966 ; accu_dev =  0.9822 \n",
      "\n",
      "6500 > 16 -th iter, training loss =  0.0068013092711 ; accu_train =  1.0\n",
      " > dev loss =  0.151812797402 ; accu_dev =  0.9821 \n",
      "\n",
      "7000 > 17 -th iter, training loss =  0.177293291207 ; accu_train =  0.9765625\n",
      " > dev loss =  0.153637443163 ; accu_dev =  0.9817 \n",
      "\n",
      "7500 > 19 -th iter, training loss =  0.00887355925356 ; accu_train =  1.0\n",
      " > dev loss =  0.151733038871 ; accu_dev =  0.9819 \n",
      "\n",
      "8000 > 20 -th iter, training loss =  0.0489396581589 ; accu_train =  0.9921875\n",
      " > dev loss =  0.150881178484 ; accu_dev =  0.9811 \n",
      "\n",
      "8500 > 21 -th iter, training loss =  0.0299367371283 ; accu_train =  0.9921875\n",
      " > dev loss =  0.152861769764 ; accu_dev =  0.9814 \n",
      "\n",
      "9000 > 23 -th iter, training loss =  0.0451447050509 ; accu_train =  0.984375\n",
      " > dev loss =  0.137405398698 ; accu_dev =  0.9826 \n",
      "\n",
      "9500 > 24 -th iter, training loss =  0.00147657929032 ; accu_train =  1.0\n",
      " > dev loss =  0.147651796427 ; accu_dev =  0.982 \n",
      "\n",
      "10000 > 25 -th iter, training loss =  0.0199324281728 ; accu_train =  0.9921875\n",
      " > dev loss =  0.143454964679 ; accu_dev =  0.9816 \n",
      "\n",
      "10500 > 26 -th iter, training loss =  0.214795714711 ; accu_train =  0.984375\n",
      " > dev loss =  0.139020426558 ; accu_dev =  0.9823 \n",
      "\n",
      "11000 > 28 -th iter, training loss =  0.138237636353 ; accu_train =  0.96875\n",
      " > dev loss =  0.137410616232 ; accu_dev =  0.9821 \n",
      "\n",
      "11500 > 29 -th iter, training loss =  0.00597108555997 ; accu_train =  1.0\n",
      " > dev loss =  0.140654826983 ; accu_dev =  0.9829 \n",
      "\n",
      "12000 > 30 -th iter, training loss =  0.0271889503339 ; accu_train =  0.9921875\n",
      " > dev loss =  0.140379333944 ; accu_dev =  0.9829 \n",
      "\n",
      "12500 > 32 -th iter, training loss =  0.0459685627845 ; accu_train =  0.984375\n",
      " > dev loss =  0.140730463459 ; accu_dev =  0.9821 \n",
      "\n",
      "13000 > 33 -th iter, training loss =  0.00561796063894 ; accu_train =  1.0\n",
      " > dev loss =  0.145783333348 ; accu_dev =  0.9823 \n",
      "\n",
      "13500 > 34 -th iter, training loss =  0.022703258196 ; accu_train =  0.9921875\n",
      " > dev loss =  0.145083405828 ; accu_dev =  0.9835 \n",
      "\n",
      "14000 > 35 -th iter, training loss =  0.00584621232356 ; accu_train =  1.0\n",
      " > dev loss =  0.142295427733 ; accu_dev =  0.983 \n",
      "\n",
      "14500 > 37 -th iter, training loss =  0.00425174270642 ; accu_train =  1.0\n",
      " > dev loss =  0.141033812532 ; accu_dev =  0.9828 \n",
      "\n",
      "15000 > 38 -th iter, training loss =  0.0285128645128 ; accu_train =  0.9921875\n",
      " > dev loss =  0.142863733831 ; accu_dev =  0.9822 \n",
      "\n",
      "15500 > 39 -th iter, training loss =  0.0245954025727 ; accu_train =  1.0\n",
      " > dev loss =  0.145167003632 ; accu_dev =  0.983 \n",
      "\n",
      "16000 > 41 -th iter, training loss =  0.0351260259957 ; accu_train =  1.0\n",
      " > dev loss =  0.144658845899 ; accu_dev =  0.9827 \n",
      "\n",
      "16500 > 42 -th iter, training loss =  0.0769727680817 ; accu_train =  0.984375\n",
      " > dev loss =  0.14628043131 ; accu_dev =  0.9818 \n",
      "\n",
      "17000 > 43 -th iter, training loss =  0.0316199060012 ; accu_train =  0.9921875\n",
      " > dev loss =  0.144768296952 ; accu_dev =  0.9825 \n",
      "\n",
      "17500 > 44 -th iter, training loss =  0.0057796242505 ; accu_train =  1.0\n",
      " > dev loss =  0.141256503747 ; accu_dev =  0.9825 \n",
      "\n",
      "18000 > 46 -th iter, training loss =  0.00357893783791 ; accu_train =  1.0\n",
      " > dev loss =  0.144976431088 ; accu_dev =  0.9825 \n",
      "\n",
      "18500 > 47 -th iter, training loss =  0.00837588963165 ; accu_train =  1.0\n",
      " > dev loss =  0.141471839712 ; accu_dev =  0.9828 \n",
      "\n",
      "19000 > 48 -th iter, training loss =  0.0672383982408 ; accu_train =  0.9765625\n",
      " > dev loss =  0.146144483778 ; accu_dev =  0.9835 \n",
      "\n",
      "19500 > 50 -th iter, training loss =  0.0456664523219 ; accu_train =  0.9921875\n",
      " > dev loss =  0.143720381469 ; accu_dev =  0.9825 \n",
      "\n",
      "20000 > 51 -th iter, training loss =  0.0203209629021 ; accu_train =  1.0\n",
      " > dev loss =  0.144315833062 ; accu_dev =  0.9823 \n",
      "\n",
      "20500 > 52 -th iter, training loss =  0.00422063989874 ; accu_train =  1.0\n",
      " > dev loss =  0.141797793264 ; accu_dev =  0.9828 \n",
      "\n",
      "21000 > 53 -th iter, training loss =  0.0048733740216 ; accu_train =  1.0\n",
      " > dev loss =  0.150035323717 ; accu_dev =  0.9826 \n",
      "\n",
      "21500 > 55 -th iter, training loss =  0.0207521398734 ; accu_train =  0.9921875\n",
      " > dev loss =  0.146547737998 ; accu_dev =  0.9839 \n",
      "\n",
      "22000 > 56 -th iter, training loss =  0.0305178039342 ; accu_train =  0.9921875\n",
      " > dev loss =  0.145203851806 ; accu_dev =  0.9829 \n",
      "\n",
      "22500 > 57 -th iter, training loss =  0.0221173625587 ; accu_train =  0.9921875\n",
      " > dev loss =  0.150006037311 ; accu_dev =  0.9823 \n",
      "\n",
      "23000 > 58 -th iter, training loss =  0.0031255709879 ; accu_train =  1.0\n",
      " > dev loss =  0.153691887747 ; accu_dev =  0.9832 \n",
      "\n",
      "23500 > 60 -th iter, training loss =  0.0464746982632 ; accu_train =  0.9921875\n",
      " > dev loss =  0.14871897567 ; accu_dev =  0.9829 \n",
      "\n",
      "24000 > 61 -th iter, training loss =  0.0142753683922 ; accu_train =  1.0\n",
      " > dev loss =  0.146394112905 ; accu_dev =  0.9832 \n",
      "\n",
      "24500 > 62 -th iter, training loss =  0.0464305376776 ; accu_train =  0.9921875\n",
      " > dev loss =  0.147207431117 ; accu_dev =  0.9829 \n",
      "\n",
      "25000 > 64 -th iter, training loss =  0.00276659765353 ; accu_train =  1.0\n",
      " > dev loss =  0.149331799702 ; accu_dev =  0.9832 \n",
      "\n",
      "25500 > 65 -th iter, training loss =  0.0324457285125 ; accu_train =  0.9921875\n",
      " > dev loss =  0.147561520634 ; accu_dev =  0.9832 \n",
      "\n",
      "26000 > 66 -th iter, training loss =  0.0368033059995 ; accu_train =  0.9921875\n",
      " > dev loss =  0.148930848344 ; accu_dev =  0.9827 \n",
      "\n",
      "26500 > 67 -th iter, training loss =  0.00720103017795 ; accu_train =  1.0\n",
      " > dev loss =  0.144474248735 ; accu_dev =  0.9835 \n",
      "\n",
      "27000 > 69 -th iter, training loss =  0.0226363578962 ; accu_train =  0.9921875\n",
      " > dev loss =  0.15157869327 ; accu_dev =  0.9827 \n",
      "\n",
      "27500 > 70 -th iter, training loss =  0.010038506629 ; accu_train =  1.0\n",
      " > dev loss =  0.147369533508 ; accu_dev =  0.9838 \n",
      "\n",
      "28000 > 71 -th iter, training loss =  0.00184837889131 ; accu_train =  1.0\n",
      " > dev loss =  0.151797092423 ; accu_dev =  0.983 \n",
      "\n",
      "28500 > 73 -th iter, training loss =  0.0179716829676 ; accu_train =  0.9921875\n",
      " > dev loss =  0.150804536623 ; accu_dev =  0.9827 \n",
      "\n",
      "29000 > 74 -th iter, training loss =  0.0109665358095 ; accu_train =  1.0\n",
      " > dev loss =  0.152233311795 ; accu_dev =  0.9827 \n",
      "\n",
      "29500 > 75 -th iter, training loss =  0.0110276813825 ; accu_train =  1.0\n",
      " > dev loss =  0.152660152702 ; accu_dev =  0.9826 \n",
      "\n",
      "30000 > 76 -th iter, training loss =  0.0305097021447 ; accu_train =  0.9921875\n",
      " > dev loss =  0.15066054059 ; accu_dev =  0.9832 \n",
      "\n",
      "30500 > 78 -th iter, training loss =  0.0333869017486 ; accu_train =  0.9921875\n",
      " > dev loss =  0.152019330875 ; accu_dev =  0.9828 \n",
      "\n",
      "31000 > 79 -th iter, training loss =  0.00170107765637 ; accu_train =  1.0\n",
      " > dev loss =  0.158790764989 ; accu_dev =  0.9819 \n",
      "\n",
      "31500 > 80 -th iter, training loss =  0.00217538606486 ; accu_train =  1.0\n",
      " > dev loss =  0.153363405898 ; accu_dev =  0.9826 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32000 > 82 -th iter, training loss =  0.0115947079016 ; accu_train =  1.0\n",
      " > dev loss =  0.153111120895 ; accu_dev =  0.9826 \n",
      "\n",
      "32500 > 83 -th iter, training loss =  0.00291433907669 ; accu_train =  1.0\n",
      " > dev loss =  0.150031611497 ; accu_dev =  0.9826 \n",
      "\n",
      "33000 > 84 -th iter, training loss =  0.0112873569425 ; accu_train =  1.0\n",
      " > dev loss =  0.149648146826 ; accu_dev =  0.9837 \n",
      "\n",
      "33500 > 85 -th iter, training loss =  0.00872397875975 ; accu_train =  1.0\n",
      " > dev loss =  0.153940079323 ; accu_dev =  0.9826 \n",
      "\n",
      "34000 > 87 -th iter, training loss =  0.0205075473657 ; accu_train =  0.9921875\n",
      " > dev loss =  0.153714020077 ; accu_dev =  0.9819 \n",
      "\n",
      "34500 > 88 -th iter, training loss =  0.019647047073 ; accu_train =  1.0\n",
      " > dev loss =  0.153048408977 ; accu_dev =  0.9833 \n",
      "\n",
      "35000 > 89 -th iter, training loss =  0.0150943773364 ; accu_train =  1.0\n",
      " > dev loss =  0.160013753346 ; accu_dev =  0.9832 \n",
      "\n",
      "35500 > 91 -th iter, training loss =  0.0266415419708 ; accu_train =  0.9921875\n",
      " > dev loss =  0.151767723334 ; accu_dev =  0.982 \n",
      "\n",
      "36000 > 92 -th iter, training loss =  0.00341683383467 ; accu_train =  1.0\n",
      " > dev loss =  0.155260291651 ; accu_dev =  0.9829 \n",
      "\n",
      "36500 > 93 -th iter, training loss =  0.0015080606007 ; accu_train =  1.0\n",
      " > dev loss =  0.160883479361 ; accu_dev =  0.9815 \n",
      "\n",
      "37000 > 94 -th iter, training loss =  0.00368335919364 ; accu_train =  1.0\n",
      " > dev loss =  0.15856580661 ; accu_dev =  0.9834 \n",
      "\n",
      "37500 > 96 -th iter, training loss =  0.0762398213435 ; accu_train =  0.9921875\n",
      " > dev loss =  0.163138648619 ; accu_dev =  0.9829 \n",
      "\n",
      "38000 > 97 -th iter, training loss =  0.0127295302887 ; accu_train =  1.0\n",
      " > dev loss =  0.155894160045 ; accu_dev =  0.9834 \n",
      "\n",
      "38500 > 98 -th iter, training loss =  0.000734042577594 ; accu_train =  1.0\n",
      " > dev loss =  0.156246050253 ; accu_dev =  0.9834 \n",
      "\n",
      "39000 > 100 -th iter, training loss =  0.00343873621002 ; accu_train =  1.0\n",
      " > dev loss =  0.151869337931 ; accu_dev =  0.9831 \n",
      "\n",
      "39500 > 101 -th iter, training loss =  0.0426712848016 ; accu_train =  0.9921875\n",
      " > dev loss =  0.160477739857 ; accu_dev =  0.9822 \n",
      "\n",
      "40000 > 102 -th iter, training loss =  0.0112647656869 ; accu_train =  1.0\n",
      " > dev loss =  0.156187395732 ; accu_dev =  0.983 \n",
      "\n",
      "40500 > 103 -th iter, training loss =  0.0588376823904 ; accu_train =  0.984375\n",
      " > dev loss =  0.156512561722 ; accu_dev =  0.983 \n",
      "\n",
      "41000 > 105 -th iter, training loss =  0.0285410827283 ; accu_train =  0.9921875\n",
      " > dev loss =  0.159942340736 ; accu_dev =  0.9828 \n",
      "\n",
      "41500 > 106 -th iter, training loss =  0.0549495442132 ; accu_train =  0.9921875\n",
      " > dev loss =  0.154326424808 ; accu_dev =  0.9834 \n",
      "\n",
      "42000 > 107 -th iter, training loss =  0.0082259142391 ; accu_train =  1.0\n",
      " > dev loss =  0.155536464468 ; accu_dev =  0.9823 \n",
      "\n",
      "42500 > 108 -th iter, training loss =  0.0031455268419 ; accu_train =  1.0\n",
      " > dev loss =  0.153307748256 ; accu_dev =  0.9828 \n",
      "\n",
      "43000 > 110 -th iter, training loss =  0.0114086181741 ; accu_train =  1.0\n",
      " > dev loss =  0.152515197365 ; accu_dev =  0.9833 \n",
      "\n",
      "43500 > 111 -th iter, training loss =  0.0039439010413 ; accu_train =  1.0\n",
      " > dev loss =  0.160602587333 ; accu_dev =  0.9823 \n",
      "\n",
      "44000 > 112 -th iter, training loss =  0.0303243260157 ; accu_train =  0.9921875\n",
      " > dev loss =  0.153874762861 ; accu_dev =  0.9835 \n",
      "\n",
      "44500 > 114 -th iter, training loss =  0.0010296006713 ; accu_train =  1.0\n",
      " > dev loss =  0.157106358097 ; accu_dev =  0.983 \n",
      "\n",
      "45000 > 115 -th iter, training loss =  0.00680077478353 ; accu_train =  1.0\n",
      " > dev loss =  0.150378924018 ; accu_dev =  0.9833 \n",
      "\n",
      "45500 > 116 -th iter, training loss =  0.00126834807864 ; accu_train =  1.0\n",
      " > dev loss =  0.158365492671 ; accu_dev =  0.9827 \n",
      "\n",
      "46000 > 117 -th iter, training loss =  0.0133320365473 ; accu_train =  1.0\n",
      " > dev loss =  0.157918274813 ; accu_dev =  0.9825 \n",
      "\n",
      "46500 > 119 -th iter, training loss =  0.0111463389011 ; accu_train =  1.0\n",
      " > dev loss =  0.153694287045 ; accu_dev =  0.984 \n",
      "\n",
      "47000 > 120 -th iter, training loss =  0.0371052734975 ; accu_train =  0.9921875\n",
      " > dev loss =  0.158917572319 ; accu_dev =  0.9833 \n",
      "\n",
      "47500 > 121 -th iter, training loss =  0.00649578621751 ; accu_train =  1.0\n",
      " > dev loss =  0.154751047463 ; accu_dev =  0.9828 \n",
      "\n",
      "48000 > 123 -th iter, training loss =  0.00285595383269 ; accu_train =  1.0\n",
      " > dev loss =  0.151675798556 ; accu_dev =  0.984 \n",
      "\n",
      "48500 > 124 -th iter, training loss =  0.00319345542039 ; accu_train =  1.0\n",
      " > dev loss =  0.157060049723 ; accu_dev =  0.984 \n",
      "\n",
      "49000 > 125 -th iter, training loss =  0.00639988329195 ; accu_train =  1.0\n",
      " > dev loss =  0.153867926166 ; accu_dev =  0.9833 \n",
      "\n",
      "49500 > 126 -th iter, training loss =  0.022471313038 ; accu_train =  1.0\n",
      " > dev loss =  0.157718131948 ; accu_dev =  0.9827 \n",
      "\n",
      "50000 > 128 -th iter, training loss =  0.00478423683908 ; accu_train =  1.0\n",
      " > dev loss =  0.151288821979 ; accu_dev =  0.984 \n",
      "\n",
      "50500 > 129 -th iter, training loss =  0.0261508880873 ; accu_train =  0.9921875\n",
      " > dev loss =  0.155306507035 ; accu_dev =  0.9846 \n",
      "\n",
      "51000 > 130 -th iter, training loss =  0.00386787332151 ; accu_train =  1.0\n",
      " > dev loss =  0.1522301865 ; accu_dev =  0.9836 \n",
      "\n",
      "51500 > 132 -th iter, training loss =  0.00931682869213 ; accu_train =  1.0\n",
      " > dev loss =  0.154907652804 ; accu_dev =  0.9838 \n",
      "\n",
      "52000 > 133 -th iter, training loss =  0.0116384011984 ; accu_train =  1.0\n",
      " > dev loss =  0.160378995728 ; accu_dev =  0.9836 \n",
      "\n",
      "52500 > 134 -th iter, training loss =  0.00370721822997 ; accu_train =  1.0\n",
      " > dev loss =  0.15917122042 ; accu_dev =  0.983 \n",
      "\n",
      "53000 > 135 -th iter, training loss =  0.0071523826754 ; accu_train =  1.0\n",
      " > dev loss =  0.160827067148 ; accu_dev =  0.9827 \n",
      "\n",
      "53500 > 137 -th iter, training loss =  0.0101817430659 ; accu_train =  1.0\n",
      " > dev loss =  0.156756794372 ; accu_dev =  0.9839 \n",
      "\n",
      "54000 > 138 -th iter, training loss =  0.0132645793815 ; accu_train =  1.0\n",
      " > dev loss =  0.165257379766 ; accu_dev =  0.9836 \n",
      "\n",
      "54500 > 139 -th iter, training loss =  0.00262136618902 ; accu_train =  1.0\n",
      " > dev loss =  0.161992092056 ; accu_dev =  0.9832 \n",
      "\n",
      "55000 > 141 -th iter, training loss =  0.0398984036285 ; accu_train =  0.9921875\n",
      " > dev loss =  0.164681624598 ; accu_dev =  0.9832 \n",
      "\n",
      "55500 > 142 -th iter, training loss =  0.000776491249545 ; accu_train =  1.0\n",
      " > dev loss =  0.163337896491 ; accu_dev =  0.9827 \n",
      "\n",
      "56000 > 143 -th iter, training loss =  0.00236038894077 ; accu_train =  1.0\n",
      " > dev loss =  0.171034585587 ; accu_dev =  0.9827 \n",
      "\n",
      "56500 > 144 -th iter, training loss =  0.00833301823858 ; accu_train =  1.0\n",
      " > dev loss =  0.165665496968 ; accu_dev =  0.9827 \n",
      "\n",
      "57000 > 146 -th iter, training loss =  0.0107937886394 ; accu_train =  1.0\n",
      " > dev loss =  0.164940771318 ; accu_dev =  0.9821 \n",
      "\n",
      "57500 > 147 -th iter, training loss =  0.0731409688953 ; accu_train =  0.9921875\n",
      " > dev loss =  0.170646352594 ; accu_dev =  0.9823 \n",
      "\n",
      "58000 > 148 -th iter, training loss =  0.00776693118587 ; accu_train =  1.0\n",
      " > dev loss =  0.161637017796 ; accu_dev =  0.9832 \n",
      "\n",
      "58500 > 150 -th iter, training loss =  0.00159279698656 ; accu_train =  1.0\n",
      " > dev loss =  0.159571454433 ; accu_dev =  0.9828 \n",
      "\n",
      "59000 > 151 -th iter, training loss =  0.00248717262486 ; accu_train =  1.0\n",
      " > dev loss =  0.159180089768 ; accu_dev =  0.9835 \n",
      "\n",
      "59500 > 152 -th iter, training loss =  0.00142298412928 ; accu_train =  1.0\n",
      " > dev loss =  0.161324093243 ; accu_dev =  0.9839 \n",
      "\n",
      "60000 > 153 -th iter, training loss =  0.00131594209517 ; accu_train =  1.0\n",
      " > dev loss =  0.162170486113 ; accu_dev =  0.9834 \n",
      "\n",
      "60500 > 155 -th iter, training loss =  0.00277278153367 ; accu_train =  1.0\n",
      " > dev loss =  0.162189714374 ; accu_dev =  0.9829 \n",
      "\n",
      "61000 > 156 -th iter, training loss =  0.0470267071628 ; accu_train =  0.9921875\n",
      " > dev loss =  0.161194810102 ; accu_dev =  0.9838 \n",
      "\n",
      "61500 > 157 -th iter, training loss =  0.00251875936611 ; accu_train =  1.0\n",
      " > dev loss =  0.162286701292 ; accu_dev =  0.9834 \n",
      "\n",
      "62000 > 158 -th iter, training loss =  0.0268891948914 ; accu_train =  0.9921875\n",
      " > dev loss =  0.165202263545 ; accu_dev =  0.9825 \n",
      "\n",
      "62500 > 160 -th iter, training loss =  0.000723725863293 ; accu_train =  1.0\n",
      " > dev loss =  0.161539278023 ; accu_dev =  0.9828 \n",
      "\n",
      "63000 > 161 -th iter, training loss =  0.00148676316143 ; accu_train =  1.0\n",
      " > dev loss =  0.162753853555 ; accu_dev =  0.9832 \n",
      "\n",
      "63500 > 162 -th iter, training loss =  0.152034523267 ; accu_train =  0.9921875\n",
      " > dev loss =  0.162411266839 ; accu_dev =  0.9831 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64000 > 164 -th iter, training loss =  0.000820397945162 ; accu_train =  1.0\n",
      " > dev loss =  0.161428180679 ; accu_dev =  0.9832 \n",
      "\n",
      "64500 > 165 -th iter, training loss =  0.030824594356 ; accu_train =  0.9921875\n",
      " > dev loss =  0.158085321135 ; accu_dev =  0.9833 \n",
      "\n",
      "65000 > 166 -th iter, training loss =  0.00382922747512 ; accu_train =  1.0\n",
      " > dev loss =  0.162246431947 ; accu_dev =  0.9832 \n",
      "\n",
      "65500 > 167 -th iter, training loss =  0.0105583932332 ; accu_train =  1.0\n",
      " > dev loss =  0.169116969636 ; accu_dev =  0.9836 \n",
      "\n",
      "66000 > 169 -th iter, training loss =  0.014717050469 ; accu_train =  0.9921875\n",
      " > dev loss =  0.164743005304 ; accu_dev =  0.9832 \n",
      "\n",
      "66500 > 170 -th iter, training loss =  0.001864816142 ; accu_train =  1.0\n",
      " > dev loss =  0.158182480859 ; accu_dev =  0.9834 \n",
      "\n",
      "67000 > 171 -th iter, training loss =  0.0240942648488 ; accu_train =  0.9921875\n",
      " > dev loss =  0.161399723814 ; accu_dev =  0.9834 \n",
      "\n",
      "67500 > 173 -th iter, training loss =  0.0178390128768 ; accu_train =  0.9921875\n",
      " > dev loss =  0.165913834776 ; accu_dev =  0.9825 \n",
      "\n",
      "68000 > 174 -th iter, training loss =  0.00734021204151 ; accu_train =  1.0\n",
      " > dev loss =  0.167953488382 ; accu_dev =  0.9829 \n",
      "\n",
      "68500 > 175 -th iter, training loss =  0.00885764275306 ; accu_train =  1.0\n",
      " > dev loss =  0.166896449595 ; accu_dev =  0.982 \n",
      "\n",
      "69000 > 176 -th iter, training loss =  0.000704943670687 ; accu_train =  1.0\n",
      " > dev loss =  0.162098439866 ; accu_dev =  0.9831 \n",
      "\n",
      "69500 > 178 -th iter, training loss =  0.00401402394153 ; accu_train =  1.0\n",
      " > dev loss =  0.160856982028 ; accu_dev =  0.9834 \n",
      "\n",
      "70000 > 179 -th iter, training loss =  0.000690870906106 ; accu_train =  1.0\n",
      " > dev loss =  0.164844793724 ; accu_dev =  0.9829 \n",
      "\n",
      "70500 > 180 -th iter, training loss =  0.000898965315445 ; accu_train =  1.0\n",
      " > dev loss =  0.164831314371 ; accu_dev =  0.9826 \n",
      "\n",
      "71000 > 182 -th iter, training loss =  0.00176616448257 ; accu_train =  1.0\n",
      " > dev loss =  0.16833443655 ; accu_dev =  0.983 \n",
      "\n",
      "71500 > 183 -th iter, training loss =  0.000354790642829 ; accu_train =  1.0\n",
      " > dev loss =  0.167808816116 ; accu_dev =  0.982 \n",
      "\n",
      "72000 > 184 -th iter, training loss =  0.013130401948 ; accu_train =  1.0\n",
      " > dev loss =  0.164494643538 ; accu_dev =  0.9828 \n",
      "\n",
      "72500 > 185 -th iter, training loss =  0.00166514236176 ; accu_train =  1.0\n",
      " > dev loss =  0.160647986023 ; accu_dev =  0.9831 \n",
      "\n",
      "73000 > 187 -th iter, training loss =  0.00448495098931 ; accu_train =  1.0\n",
      " > dev loss =  0.166645059301 ; accu_dev =  0.9834 \n",
      "\n",
      "73500 > 188 -th iter, training loss =  0.00252155412455 ; accu_train =  1.0\n",
      " > dev loss =  0.163243803719 ; accu_dev =  0.9823 \n",
      "\n",
      "74000 > 189 -th iter, training loss =  0.002430667346 ; accu_train =  1.0\n",
      " > dev loss =  0.170357482165 ; accu_dev =  0.9821 \n",
      "\n",
      "74500 > 191 -th iter, training loss =  0.000812058173999 ; accu_train =  1.0\n",
      " > dev loss =  0.16775461406 ; accu_dev =  0.9825 \n",
      "\n",
      "75000 > 192 -th iter, training loss =  0.00400915561025 ; accu_train =  1.0\n",
      " > dev loss =  0.17222495746 ; accu_dev =  0.9826 \n",
      "\n",
      "75500 > 193 -th iter, training loss =  0.00758993176969 ; accu_train =  1.0\n",
      " > dev loss =  0.168893359862 ; accu_dev =  0.9835 \n",
      "\n",
      "76000 > 194 -th iter, training loss =  0.000456657893688 ; accu_train =  1.0\n",
      " > dev loss =  0.179392018437 ; accu_dev =  0.9823 \n",
      "\n",
      "76500 > 196 -th iter, training loss =  0.0122837004281 ; accu_train =  1.0\n",
      " > dev loss =  0.172928510837 ; accu_dev =  0.9825 \n",
      "\n",
      "77000 > 197 -th iter, training loss =  0.00275547771424 ; accu_train =  1.0\n",
      " > dev loss =  0.168389049994 ; accu_dev =  0.9837 \n",
      "\n",
      "77500 > 198 -th iter, training loss =  0.0046168457495 ; accu_train =  1.0\n",
      " > dev loss =  0.170681205282 ; accu_dev =  0.9824 \n",
      "\n",
      "78000 > 200 -th iter, training loss =  0.0350734148376 ; accu_train =  0.9921875\n",
      " > dev loss =  0.16116460348 ; accu_dev =  0.9837 \n",
      "\n",
      "78500 > 201 -th iter, training loss =  0.00539706429521 ; accu_train =  1.0\n",
      " > dev loss =  0.160703019302 ; accu_dev =  0.9832 \n",
      "\n",
      "79000 > 202 -th iter, training loss =  0.00732968237198 ; accu_train =  1.0\n",
      " > dev loss =  0.173621105504 ; accu_dev =  0.9829 \n",
      "\n",
      "79500 > 203 -th iter, training loss =  0.00201794192719 ; accu_train =  1.0\n",
      " > dev loss =  0.163659840533 ; accu_dev =  0.9833 \n",
      "\n",
      "80000 > 205 -th iter, training loss =  0.0075084953777 ; accu_train =  1.0\n",
      " > dev loss =  0.171088451624 ; accu_dev =  0.9835 \n",
      "\n",
      "80500 > 206 -th iter, training loss =  0.00516110551249 ; accu_train =  1.0\n",
      " > dev loss =  0.169372881177 ; accu_dev =  0.983 \n",
      "\n",
      "81000 > 207 -th iter, training loss =  0.000708661317755 ; accu_train =  1.0\n",
      " > dev loss =  0.165760273526 ; accu_dev =  0.9836 \n",
      "\n",
      "81500 > 208 -th iter, training loss =  0.00499228062323 ; accu_train =  1.0\n",
      " > dev loss =  0.168076057323 ; accu_dev =  0.9839 \n",
      "\n",
      "82000 > 210 -th iter, training loss =  0.00639143848526 ; accu_train =  1.0\n",
      " > dev loss =  0.168293033571 ; accu_dev =  0.983 \n",
      "\n",
      "82500 > 211 -th iter, training loss =  0.00195213656835 ; accu_train =  1.0\n",
      " > dev loss =  0.165966976385 ; accu_dev =  0.983 \n",
      "\n",
      "83000 > 212 -th iter, training loss =  0.0152005355617 ; accu_train =  0.9921875\n",
      " > dev loss =  0.167984313541 ; accu_dev =  0.9833 \n",
      "\n",
      "83500 > 214 -th iter, training loss =  0.00504843321101 ; accu_train =  1.0\n",
      " > dev loss =  0.166507594498 ; accu_dev =  0.9836 \n",
      "\n",
      "84000 > 215 -th iter, training loss =  0.0154265755299 ; accu_train =  0.9921875\n",
      " > dev loss =  0.168348483743 ; accu_dev =  0.9829 \n",
      "\n",
      "84500 > 216 -th iter, training loss =  0.00130216233956 ; accu_train =  1.0\n",
      " > dev loss =  0.168680497865 ; accu_dev =  0.983 \n",
      "\n",
      "85000 > 217 -th iter, training loss =  0.00404369334089 ; accu_train =  1.0\n",
      " > dev loss =  0.175332833627 ; accu_dev =  0.982 \n",
      "\n",
      "85500 > 219 -th iter, training loss =  0.000342153527251 ; accu_train =  1.0\n",
      " > dev loss =  0.173737593976 ; accu_dev =  0.9823 \n",
      "\n",
      "86000 > 220 -th iter, training loss =  0.00814072035083 ; accu_train =  1.0\n",
      " > dev loss =  0.172093303406 ; accu_dev =  0.9828 \n",
      "\n",
      "86500 > 221 -th iter, training loss =  0.00266861191267 ; accu_train =  1.0\n",
      " > dev loss =  0.173539071617 ; accu_dev =  0.9822 \n",
      "\n",
      "87000 > 223 -th iter, training loss =  0.0272122001559 ; accu_train =  0.9921875\n",
      " > dev loss =  0.168672942398 ; accu_dev =  0.9827 \n",
      "\n",
      "87500 > 224 -th iter, training loss =  7.30414503665e-05 ; accu_train =  1.0\n",
      " > dev loss =  0.170973163334 ; accu_dev =  0.983 \n",
      "\n",
      "88000 > 225 -th iter, training loss =  0.00180151431113 ; accu_train =  1.0\n",
      " > dev loss =  0.170694183208 ; accu_dev =  0.9831 \n",
      "\n",
      "88500 > 226 -th iter, training loss =  0.000377360983814 ; accu_train =  1.0\n",
      " > dev loss =  0.1708248623 ; accu_dev =  0.9828 \n",
      "\n",
      "89000 > 228 -th iter, training loss =  0.00437659263814 ; accu_train =  1.0\n",
      " > dev loss =  0.166049676684 ; accu_dev =  0.9837 \n",
      "\n",
      "89500 > 229 -th iter, training loss =  0.00450073895262 ; accu_train =  1.0\n",
      " > dev loss =  0.169121645282 ; accu_dev =  0.9833 \n",
      "\n",
      "90000 > 230 -th iter, training loss =  0.00299979135084 ; accu_train =  1.0\n",
      " > dev loss =  0.16366836264 ; accu_dev =  0.9839 \n",
      "\n",
      "90500 > 232 -th iter, training loss =  0.0004018711636 ; accu_train =  1.0\n",
      " > dev loss =  0.1692491125 ; accu_dev =  0.9831 \n",
      "\n",
      "91000 > 233 -th iter, training loss =  0.000386573108203 ; accu_train =  1.0\n",
      " > dev loss =  0.167784649417 ; accu_dev =  0.9833 \n",
      "\n",
      "91500 > 234 -th iter, training loss =  0.0122399727212 ; accu_train =  0.9921875\n",
      " > dev loss =  0.175484485488 ; accu_dev =  0.9827 \n",
      "\n",
      "92000 > 235 -th iter, training loss =  0.00327039436587 ; accu_train =  1.0\n",
      " > dev loss =  0.165588955884 ; accu_dev =  0.9835 \n",
      "\n",
      "92500 > 237 -th iter, training loss =  0.00131316123005 ; accu_train =  1.0\n",
      " > dev loss =  0.168513085203 ; accu_dev =  0.9832 \n",
      "\n",
      "93000 > 238 -th iter, training loss =  0.0181762968561 ; accu_train =  0.9921875\n",
      " > dev loss =  0.173499507559 ; accu_dev =  0.9832 \n",
      "\n",
      "93500 > 239 -th iter, training loss =  0.000781494039714 ; accu_train =  1.0\n",
      " > dev loss =  0.17118624964 ; accu_dev =  0.9826 \n",
      "\n",
      "94000 > 241 -th iter, training loss =  0.00189344657143 ; accu_train =  1.0\n",
      " > dev loss =  0.170627887039 ; accu_dev =  0.9834 \n",
      "\n",
      "94500 > 242 -th iter, training loss =  0.0197476510692 ; accu_train =  0.9921875\n",
      " > dev loss =  0.164466884307 ; accu_dev =  0.9839 \n",
      "\n",
      "95000 > 243 -th iter, training loss =  0.000223204470792 ; accu_train =  1.0\n",
      " > dev loss =  0.174924987057 ; accu_dev =  0.9829 \n",
      "\n",
      "95500 > 244 -th iter, training loss =  0.000148013620855 ; accu_train =  1.0\n",
      " > dev loss =  0.175481202589 ; accu_dev =  0.9829 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96000 > 246 -th iter, training loss =  0.000197713953534 ; accu_train =  1.0\n",
      " > dev loss =  0.172762393273 ; accu_dev =  0.9831 \n",
      "\n",
      "96500 > 247 -th iter, training loss =  0.00269795551645 ; accu_train =  1.0\n",
      " > dev loss =  0.175859045019 ; accu_dev =  0.983 \n",
      "\n",
      "97000 > 248 -th iter, training loss =  0.002308267749 ; accu_train =  1.0\n",
      " > dev loss =  0.175079205421 ; accu_dev =  0.9829 \n",
      "\n",
      "97500 > 250 -th iter, training loss =  0.000585688319931 ; accu_train =  1.0\n",
      " > dev loss =  0.179606179634 ; accu_dev =  0.9826 \n",
      "\n",
      "98000 > 251 -th iter, training loss =  0.00236951544772 ; accu_train =  1.0\n",
      " > dev loss =  0.1753869538 ; accu_dev =  0.983 \n",
      "\n",
      "98500 > 252 -th iter, training loss =  0.00194048983819 ; accu_train =  1.0\n",
      " > dev loss =  0.174975545603 ; accu_dev =  0.9835 \n",
      "\n",
      "99000 > 253 -th iter, training loss =  0.00873445894668 ; accu_train =  1.0\n",
      " > dev loss =  0.176293360599 ; accu_dev =  0.984 \n",
      "\n",
      "99500 > 255 -th iter, training loss =  0.000685018180571 ; accu_train =  1.0\n",
      " > dev loss =  0.173900885418 ; accu_dev =  0.9825 \n",
      "\n",
      "100000 > 256 -th iter, training loss =  0.00271835797148 ; accu_train =  1.0\n",
      " > dev loss =  0.170150580085 ; accu_dev =  0.9833 \n",
      "\n",
      "100500 > 257 -th iter, training loss =  0.00961020860241 ; accu_train =  1.0\n",
      " > dev loss =  0.167677290246 ; accu_dev =  0.9841 \n",
      "\n",
      "101000 > 258 -th iter, training loss =  0.00203142532308 ; accu_train =  1.0\n",
      " > dev loss =  0.177370694389 ; accu_dev =  0.9838 \n",
      "\n",
      "101500 > 260 -th iter, training loss =  0.0164770361679 ; accu_train =  0.9921875\n",
      " > dev loss =  0.169050598055 ; accu_dev =  0.9836 \n",
      "\n",
      "102000 > 261 -th iter, training loss =  0.000302166186961 ; accu_train =  1.0\n",
      " > dev loss =  0.174909801887 ; accu_dev =  0.9838 \n",
      "\n",
      "102500 > 262 -th iter, training loss =  0.0097576641713 ; accu_train =  1.0\n",
      " > dev loss =  0.174730507865 ; accu_dev =  0.9841 \n",
      "\n",
      "103000 > 264 -th iter, training loss =  0.00455826591926 ; accu_train =  1.0\n",
      " > dev loss =  0.179406986939 ; accu_dev =  0.9837 \n",
      "\n",
      "103500 > 265 -th iter, training loss =  0.00163992746387 ; accu_train =  1.0\n",
      " > dev loss =  0.172651009311 ; accu_dev =  0.9836 \n",
      "\n",
      "104000 > 266 -th iter, training loss =  0.00221923908955 ; accu_train =  1.0\n",
      " > dev loss =  0.170183457945 ; accu_dev =  0.984 \n",
      "\n",
      "104500 > 267 -th iter, training loss =  0.00219413079309 ; accu_train =  1.0\n",
      " > dev loss =  0.17572716771 ; accu_dev =  0.9837 \n",
      "\n",
      "105000 > 269 -th iter, training loss =  0.000515991196948 ; accu_train =  1.0\n",
      " > dev loss =  0.165930904608 ; accu_dev =  0.9844 \n",
      "\n",
      "105500 > 270 -th iter, training loss =  0.00308089932881 ; accu_train =  1.0\n",
      " > dev loss =  0.165665312042 ; accu_dev =  0.9834 \n",
      "\n",
      "106000 > 271 -th iter, training loss =  0.0141456894767 ; accu_train =  0.9921875\n",
      " > dev loss =  0.177132253046 ; accu_dev =  0.9834 \n",
      "\n",
      "106500 > 273 -th iter, training loss =  0.00507851086029 ; accu_train =  1.0\n",
      " > dev loss =  0.178810347712 ; accu_dev =  0.9827 \n",
      "\n",
      "107000 > 274 -th iter, training loss =  0.00644935397482 ; accu_train =  1.0\n",
      " > dev loss =  0.175350599091 ; accu_dev =  0.9829 \n",
      "\n",
      "107500 > 275 -th iter, training loss =  0.0110843769738 ; accu_train =  1.0\n",
      " > dev loss =  0.171530980829 ; accu_dev =  0.984 \n",
      "\n",
      "108000 > 276 -th iter, training loss =  0.000289947555608 ; accu_train =  1.0\n",
      " > dev loss =  0.169539800127 ; accu_dev =  0.9839 \n",
      "\n",
      "108500 > 278 -th iter, training loss =  0.00188536017553 ; accu_train =  1.0\n",
      " > dev loss =  0.175780006696 ; accu_dev =  0.9831 \n",
      "\n",
      "109000 > 279 -th iter, training loss =  0.00548299047975 ; accu_train =  1.0\n",
      " > dev loss =  0.180886683797 ; accu_dev =  0.983 \n",
      "\n",
      "109500 > 280 -th iter, training loss =  0.00944548251639 ; accu_train =  1.0\n",
      " > dev loss =  0.179336436557 ; accu_dev =  0.984 \n",
      "\n",
      "110000 > 282 -th iter, training loss =  0.00205373306985 ; accu_train =  1.0\n",
      " > dev loss =  0.175330674261 ; accu_dev =  0.9837 \n",
      "\n",
      "110500 > 283 -th iter, training loss =  0.00344989964984 ; accu_train =  1.0\n",
      " > dev loss =  0.180380264151 ; accu_dev =  0.9835 \n",
      "\n",
      "111000 > 284 -th iter, training loss =  0.00154417810497 ; accu_train =  1.0\n",
      " > dev loss =  0.178974428153 ; accu_dev =  0.9833 \n",
      "\n",
      "111500 > 285 -th iter, training loss =  0.00696888629255 ; accu_train =  1.0\n",
      " > dev loss =  0.168344649953 ; accu_dev =  0.9838 \n",
      "\n",
      "112000 > 287 -th iter, training loss =  0.001739505655 ; accu_train =  1.0\n",
      " > dev loss =  0.172917148641 ; accu_dev =  0.9834 \n",
      "\n",
      "112500 > 288 -th iter, training loss =  0.0067612129735 ; accu_train =  1.0\n",
      " > dev loss =  0.175568924401 ; accu_dev =  0.984 \n",
      "\n",
      "113000 > 289 -th iter, training loss =  0.00322740980819 ; accu_train =  1.0\n",
      " > dev loss =  0.173862698723 ; accu_dev =  0.9832 \n",
      "\n",
      "113500 > 291 -th iter, training loss =  0.0190396475196 ; accu_train =  1.0\n",
      " > dev loss =  0.17533125407 ; accu_dev =  0.9836 \n",
      "\n",
      "114000 > 292 -th iter, training loss =  0.00031146816867 ; accu_train =  1.0\n",
      " > dev loss =  0.175172462042 ; accu_dev =  0.9834 \n",
      "\n",
      "114500 > 293 -th iter, training loss =  0.00272175628741 ; accu_train =  1.0\n",
      " > dev loss =  0.178449209497 ; accu_dev =  0.9833 \n",
      "\n",
      "115000 > 294 -th iter, training loss =  0.00856748814839 ; accu_train =  1.0\n",
      " > dev loss =  0.181040117843 ; accu_dev =  0.9841 \n",
      "\n",
      "115500 > 296 -th iter, training loss =  0.00125468792372 ; accu_train =  1.0\n",
      " > dev loss =  0.187269630727 ; accu_dev =  0.9839 \n",
      "\n",
      "116000 > 297 -th iter, training loss =  0.00225016126713 ; accu_train =  1.0\n",
      " > dev loss =  0.174986049052 ; accu_dev =  0.9836 \n",
      "\n",
      "116500 > 298 -th iter, training loss =  0.02247476994 ; accu_train =  0.9921875\n",
      " > dev loss =  0.17258596071 ; accu_dev =  0.9833 \n",
      "\n",
      "117000 > 300 -th iter, training loss =  6.02751572234e-05 ; accu_train =  1.0\n",
      " > dev loss =  0.17566135314 ; accu_dev =  0.9835 \n",
      "\n",
      "117500 > 301 -th iter, training loss =  0.0105332565851 ; accu_train =  1.0\n",
      " > dev loss =  0.176565496538 ; accu_dev =  0.9839 \n",
      "\n",
      "118000 > 302 -th iter, training loss =  0.000333541653873 ; accu_train =  1.0\n",
      " > dev loss =  0.177911788812 ; accu_dev =  0.9829 \n",
      "\n",
      "118500 > 303 -th iter, training loss =  0.000251541940907 ; accu_train =  1.0\n",
      " > dev loss =  0.178717563674 ; accu_dev =  0.9836 \n",
      "\n",
      "119000 > 305 -th iter, training loss =  0.0023666032118 ; accu_train =  1.0\n",
      " > dev loss =  0.179388200813 ; accu_dev =  0.9831 \n",
      "\n",
      "119500 > 306 -th iter, training loss =  0.00843769246324 ; accu_train =  1.0\n",
      " > dev loss =  0.176638525076 ; accu_dev =  0.9834 \n",
      "\n",
      "120000 > 307 -th iter, training loss =  0.00558723418673 ; accu_train =  1.0\n",
      " > dev loss =  0.180097879213 ; accu_dev =  0.983 \n",
      "\n",
      "120500 > 308 -th iter, training loss =  0.000341072031436 ; accu_train =  1.0\n",
      " > dev loss =  0.177594396114 ; accu_dev =  0.9833 \n",
      "\n",
      "121000 > 310 -th iter, training loss =  0.00288188197629 ; accu_train =  1.0\n",
      " > dev loss =  0.178364206692 ; accu_dev =  0.9838 \n",
      "\n",
      "121500 > 311 -th iter, training loss =  0.00210313379404 ; accu_train =  1.0\n",
      " > dev loss =  0.180744355742 ; accu_dev =  0.9834 \n",
      "\n",
      "122000 > 312 -th iter, training loss =  0.000904166103588 ; accu_train =  1.0\n",
      " > dev loss =  0.171668899779 ; accu_dev =  0.9842 \n",
      "\n",
      "122500 > 314 -th iter, training loss =  0.00736138923391 ; accu_train =  1.0\n",
      " > dev loss =  0.173936434603 ; accu_dev =  0.9842 \n",
      "\n",
      "123000 > 315 -th iter, training loss =  0.00296108546065 ; accu_train =  1.0\n",
      " > dev loss =  0.174484059805 ; accu_dev =  0.9834 \n",
      "\n",
      "123500 > 316 -th iter, training loss =  0.00485938968235 ; accu_train =  1.0\n",
      " > dev loss =  0.177404637949 ; accu_dev =  0.9835 \n",
      "\n",
      "124000 > 317 -th iter, training loss =  0.00596986825811 ; accu_train =  1.0\n",
      " > dev loss =  0.175176940006 ; accu_dev =  0.9835 \n",
      "\n",
      "124500 > 319 -th iter, training loss =  0.00346072286379 ; accu_train =  1.0\n",
      " > dev loss =  0.177974385945 ; accu_dev =  0.9835 \n",
      "\n",
      "125000 > 320 -th iter, training loss =  0.000765172423498 ; accu_train =  1.0\n",
      " > dev loss =  0.172030360753 ; accu_dev =  0.9837 \n",
      "\n",
      "125500 > 321 -th iter, training loss =  0.00731072110875 ; accu_train =  1.0\n",
      " > dev loss =  0.176855592554 ; accu_dev =  0.9832 \n",
      "\n",
      "126000 > 323 -th iter, training loss =  0.000189519931306 ; accu_train =  1.0\n",
      " > dev loss =  0.181332540597 ; accu_dev =  0.9835 \n",
      "\n",
      "126500 > 324 -th iter, training loss =  0.000176777014188 ; accu_train =  1.0\n",
      " > dev loss =  0.179077885077 ; accu_dev =  0.9834 \n",
      "\n",
      "127000 > 325 -th iter, training loss =  0.00469080249778 ; accu_train =  1.0\n",
      " > dev loss =  0.178081270798 ; accu_dev =  0.9841 \n",
      "\n",
      "127500 > 326 -th iter, training loss =  0.00233105480833 ; accu_train =  1.0\n",
      " > dev loss =  0.177330167683 ; accu_dev =  0.9837 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128000 > 328 -th iter, training loss =  0.0107809802755 ; accu_train =  1.0\n",
      " > dev loss =  0.176535981599 ; accu_dev =  0.9837 \n",
      "\n",
      "128500 > 329 -th iter, training loss =  0.0826088342089 ; accu_train =  0.984375\n",
      " > dev loss =  0.175768177746 ; accu_dev =  0.9835 \n",
      "\n",
      "129000 > 330 -th iter, training loss =  0.00749632777903 ; accu_train =  1.0\n",
      " > dev loss =  0.177369980926 ; accu_dev =  0.9837 \n",
      "\n",
      "129500 > 332 -th iter, training loss =  0.00177142285783 ; accu_train =  1.0\n",
      " > dev loss =  0.175120084969 ; accu_dev =  0.9832 \n",
      "\n",
      "130000 > 333 -th iter, training loss =  0.00160958228405 ; accu_train =  1.0\n",
      " > dev loss =  0.177157107313 ; accu_dev =  0.9829 \n",
      "\n",
      "130500 > 334 -th iter, training loss =  0.0473895821177 ; accu_train =  0.9921875\n",
      " > dev loss =  0.176275166383 ; accu_dev =  0.9835 \n",
      "\n",
      "131000 > 335 -th iter, training loss =  0.0198975077641 ; accu_train =  0.9921875\n",
      " > dev loss =  0.174464119034 ; accu_dev =  0.9846 \n",
      "\n",
      "131500 > 337 -th iter, training loss =  0.00221809567612 ; accu_train =  1.0\n",
      " > dev loss =  0.173323355816 ; accu_dev =  0.9833 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "timer = {'forwardpropagation_all':0, 'backpropagate_all':0, 'update_parameters':0, 'tmp':0}\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"test model using random data\"\"\"\n",
    "    m = 10\n",
    "    X = np.random.randn(784,m)\n",
    "    Y = one_hot(np.random.randint(low=0, high=10, size=(m,1)))\n",
    "    #print(X[:,1])\n",
    "    #print(Y[:,1])\n",
    "    costs = model(X,Y,learning_rate=0.1, print_every=1, iteration=1,\n",
    "                 hidden_layers=[10,2])\n",
    "    plt.plot(costs[1], costs[0])\n",
    "    plt.show()\n",
    "    \n",
    "def main(trial=True):\n",
    "    \"\"\"test model using real MNIST dataset\"\"\"\n",
    "    mnist = load_mnist()\n",
    "    mnist = shuffle_divide_dataset(mnist)\n",
    "    mnist = standardize(mnist)\n",
    "    mnist = flat_stack(mnist)\n",
    "    mnist = one_hot(mnist)\n",
    "    if trial:\n",
    "        print(\"Trail\")\n",
    "        costs = model(mnist['X_train'][:,:16], mnist['Y_train'][:,:16], mnist['X_dev'], mnist['Y_dev'],\n",
    "                  learning_rate=0.1, print_every=100, iteration=1,\n",
    "                  batch_size=8, regularization=0, keep_prop=0.3,\n",
    "                  hidden_layers=[784,12])\n",
    "    else:\n",
    "        costs = model(mnist['X_train'], mnist['Y_train'], mnist['X_dev'], mnist['Y_dev'],\n",
    "                  learning_rate=0.1, print_every=500, iteration=20000,\n",
    "                  batch_size=128, regularization=0, keep_prop=0.5,\n",
    "                  hidden_layers=[784,1024,1024])\n",
    "        \n",
    "    plt.plot(costs[1], costs[0])\n",
    "    plt.plot(costs[3], costs[2])\n",
    "    plt.title(\"Cost Value\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(timer)\n",
    "\n",
    "    forwardpropagation_all(mnist['X_test'], mnist['Y_test'], with_dropout=False)\n",
    "    Y_hat_test = cache['A'+str(hyper_parameters['L'])]\n",
    "    Y_predict_test = predict(Y_hat_test)\n",
    "    accu_test = accuracy(Y_predict_test, mnist['Y_test'])\n",
    "    print('final test set accuracy: ', accu_test)\n",
    "    \n",
    "main(trial=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.names_shape_in(cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
