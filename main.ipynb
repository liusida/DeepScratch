{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todo list\n",
    "\n",
    "1. one_hot √\n",
    "\n",
    "* mini-batch\n",
    "\n",
    "* normalization\n",
    "\n",
    "* train/dev/test set √\n",
    "\n",
    "* linear function √\n",
    "\n",
    "* sigmoid function\n",
    "\n",
    "* tanh function\n",
    "\n",
    "* relu function √\n",
    "\n",
    "* softmax function √\n",
    "\n",
    "* loss function √\n",
    "\n",
    "* cost function ?\n",
    "\n",
    "* regularization\n",
    "\n",
    "* drop out\n",
    "\n",
    "* batch normalization\n",
    "\n",
    "* momentum\n",
    "\n",
    "* exponentially moving average\n",
    "\n",
    "* Adam\n",
    "\n",
    "* all backpropagation of above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\"\"\" helper functions \"\"\"\n",
    "def names_in(dictionary):\n",
    "    \"\"\" list all names in a dictionary \"\"\"\n",
    "    print([name for name,_ in sorted(dictionary.items())])\n",
    "def names_shape_in(dictionary):\n",
    "    pprint([(name, val.shape) for name,val in sorted(dictionary.items())])\n",
    "\n",
    "def debug_show_all_variables():\n",
    "    print(\"cache: \", end='')\n",
    "    names_shape_in(cache)\n",
    "    print(\"parameters: \", end='')\n",
    "    names_shape_in(parameters)\n",
    "    print(\"hyper_parameters: \", end='')\n",
    "    names_in(hyper_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_mnist():\n",
    "    import struct\n",
    "    from array import array\n",
    "    \"\"\" \n",
    "    load MNIST dataset into numpy array \n",
    "    MNIST dataset can be downloaded manually.\n",
    "    url: http://yann.lecun.com/exdb/mnist/\n",
    "    \"\"\"\n",
    "    ret = {}\n",
    "    with open('MNIST/train-images.idx3-ubyte', 'rb') as f:\n",
    "        magic, size, rows, cols = struct.unpack(\">IIII\", f.read(16))\n",
    "        assert(magic==2051)\n",
    "        ret['X_train'] = np.array(array(\"B\", f.read())).reshape(size,rows,cols)\n",
    "\n",
    "    with open('MNIST/t10k-images.idx3-ubyte', 'rb') as f:\n",
    "        magic, size, rows, cols = struct.unpack(\">IIII\", f.read(16))\n",
    "        assert(magic==2051)\n",
    "        ret['X_test'] = np.array(array(\"B\", f.read())).reshape(size,rows,cols)\n",
    "\n",
    "    with open('MNIST/train-labels.idx1-ubyte', 'rb') as f:\n",
    "        magic, size = struct.unpack(\">II\", f.read(8))\n",
    "        assert(magic==2049)\n",
    "        ret['Y_train'] = np.array(array(\"B\", f.read())).reshape(size,1)\n",
    "\n",
    "    with open('MNIST/t10k-labels.idx1-ubyte', 'rb') as f:\n",
    "        magic, size = struct.unpack(\">II\", f.read(8))\n",
    "        assert(magic==2049)\n",
    "        ret['Y_test'] = np.array(array(\"B\", f.read())).reshape(size,1)\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (50000, 28, 28) X_dev: (10000, 28, 28) X_test: (10000, 28, 28)\n",
      "Y_train: (50000, 1) Y_dev: (10000, 1) Y_test: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "mnist_original = load_mnist()\n",
    "\n",
    "\"\"\" random shuffle the training set \"\"\"\n",
    "np.random.seed(1)\n",
    "permutation = np.random.permutation(mnist_original['X_train'].shape[0])\n",
    "mnist_original['X_train'] = mnist_original['X_train'][permutation]\n",
    "mnist_original['Y_train'] = mnist_original['Y_train'][permutation]\n",
    "\n",
    "\"\"\" divide trainset into trainset and devset \"\"\"\n",
    "len_of_dev = 10000\n",
    "mnist_original['X_dev'] = mnist_original['X_train'][:len_of_dev]\n",
    "mnist_original['Y_dev'] = mnist_original['Y_train'][:len_of_dev]\n",
    "mnist_original['X_train'] = mnist_original['X_train'][len_of_dev:]\n",
    "mnist_original['Y_train'] = mnist_original['Y_train'][len_of_dev:]\n",
    "\n",
    "print('X_train:', mnist_original['X_train'].shape,\n",
    "      'X_dev:', mnist_original['X_dev'].shape,\n",
    "      'X_test:', mnist_original['X_test'].shape)\n",
    "print('Y_train:', mnist_original['Y_train'].shape,\n",
    "      'Y_dev:', mnist_original['Y_dev'].shape,\n",
    "      'Y_test:', mnist_original['Y_test'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def manually_validate_dataset(dataset):\n",
    "    random_train = np.random.randint(1, len(dataset['X_train']))-1\n",
    "    random_dev = np.random.randint(1, len(dataset['X_dev']))-1\n",
    "    random_test = np.random.randint(1, len(dataset['X_test']))-1\n",
    "    print(dataset['Y_train'][random_train], dataset['Y_dev'][random_dev], dataset['Y_test'][random_test])\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1,3, sharey=True, figsize=[10,3])\n",
    "    ax1.imshow(dataset['X_train'][random_train], cmap='gray')\n",
    "    ax2.imshow(dataset['X_dev'][random_dev], cmap='gray')\n",
    "    ax3.imshow(dataset['X_test'][random_test], cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "#manually_validate_dataset(mnist_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ X = \n",
    "\\begin{bmatrix}\n",
    "\\vert & & \\vert & & \\vert \\\\\n",
    "x^{(1)} & ... & x^{(i)} & ... & x^{(m)} \\\\\n",
    "\\vert & & \\vert & & \\vert\n",
    "\\end{bmatrix} \n",
    "\\quad \\quad\n",
    "Y = \n",
    "\\begin{bmatrix}\n",
    "\\vert & & \\vert & & \\vert \\\\\n",
    "y_{one\\_hot}^{(1)} & ... & y_{one\\_hot}^{(i)} & ... & y_{one\\_hot}^{(m)} \\\\\n",
    "\\vert & & \\vert & & \\vert \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 50000) (10, 50000)\n"
     ]
    }
   ],
   "source": [
    "mnist = {}\n",
    "\n",
    "\"\"\" X is 28*28 image \"\"\"\n",
    "def flatten(X):\n",
    "    \"\"\" prepare X to (nx, m) shape \"\"\"\n",
    "    X = X.reshape(-1, 28*28).T\n",
    "    return X\n",
    "\n",
    "def normalize(X):\n",
    "    u = 0\n",
    "    sigma = 255\n",
    "    return (X-u) / sigma\n",
    "\n",
    "mnist['X_train'] = normalize(flatten(mnist_original['X_train']))\n",
    "mnist['X_dev'] = normalize(flatten(mnist_original['X_dev']))\n",
    "mnist['X_test'] = normalize(flatten(mnist_original['X_test']))\n",
    "\n",
    "\"\"\" Y is label 0-9 \"\"\"\n",
    "def one_hot(Y, C):\n",
    "    \"\"\" prepare Y to (1, m) shape \"\"\"\n",
    "    assert(Y.shape[1]==1)\n",
    "    Y_ret = np.zeros((Y.shape[0], C))\n",
    "    Y_ret[np.arange(Y.shape[0]), Y.reshape(-1).astype(int)] = 1\n",
    "    Y_ret = Y_ret.T\n",
    "    return Y_ret\n",
    "\n",
    "def back_one_hot(Y):\n",
    "    \"\"\" convert one hot Y back to real number \"\"\"\n",
    "    Y_ret = np.repeat( [np.arange(Y.shape[0])], repeats=Y.shape[1], axis=0 )\n",
    "    assert(Y_ret.shape == Y.T.shape)\n",
    "    Y_ret = Y_ret[Y.T.astype(bool)]\n",
    "    return Y_ret.reshape(-1,1)\n",
    "\n",
    "mnist['Y_train'] = one_hot(mnist_original['Y_train'], 10)\n",
    "mnist['Y_dev'] = one_hot(mnist_original['Y_dev'], 10)\n",
    "mnist['Y_test'] = one_hot(mnist_original['Y_test'], 10)\n",
    "\n",
    "print(mnist['X_train'].shape, mnist['Y_train'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" init cache, parameters and hyper_parameters \"\"\"\n",
    "def init(X_size, hidden_layers=[10,10,5], C=10, learning_rate=0.01):\n",
    "    global cache, parameters, hyper_parameters\n",
    "    cache = {}\n",
    "    parameters = {}\n",
    "    hyper_parameters = {}\n",
    "    initialize_hyper_parameters(X_size, hidden_layers, C, learning_rate)\n",
    "    initialize_parameters()\n",
    "\n",
    "def initialize_hyper_parameters(X_size, hidden_layers, C, learning_rate):\n",
    "    global hyper_parameters\n",
    "    hyper_parameters['X_size'] = X_size\n",
    "    # layers of network, include the last softmax layer\n",
    "    # hidden layers use ReLU activation function, and the last layer use Softmax function\n",
    "    hyper_parameters['layers'] = hidden_layers + [C]\n",
    "    # L layers in total\n",
    "    hyper_parameters['L'] = len(hyper_parameters['layers'])\n",
    "    # Class numbers 0-9 is 10\n",
    "    hyper_parameters['C'] = C\n",
    "    hyper_parameters['learning_rate'] = learning_rate\n",
    "\n",
    "def initialize_parameters():\n",
    "    \"\"\" init W b or any other parameters in every layer \"\"\"\n",
    "    global parameters, hyper_parameters\n",
    "    layers = hyper_parameters['layers']\n",
    "    x_size = hyper_parameters['X_size']\n",
    "    cells_prev = x_size\n",
    "    for layer_idx, cells in enumerate(layers):\n",
    "        parameters['W'+str(layer_idx+1)] = np.random.randn(cells, cells_prev) * 0.01\n",
    "        parameters['b'+str(layer_idx+1)] = np.zeros((cells, 1))\n",
    "        cells_prev = layers[layer_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "ReLu = max(0, x)\n",
    "\\quad \\quad\n",
    "Softmax = \\frac{\\exp(Z)}{\\sum_i^n{\\exp(Z)}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "Z = W X + b\n",
    "\\quad \\quad\n",
    "A = active(Z)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(X):\n",
    "    return X * (X > 0)\n",
    "\n",
    "def softmax(X):\n",
    "    s = np.sum(np.exp(X), axis=0)\n",
    "    return np.exp(X) / s\n",
    "\n",
    "def forward_propagation_each_layer(W, A_prev, b, activation_function=ReLU):\n",
    "    Z = np.dot(W, A_prev) + b\n",
    "    A = activation_function(Z)\n",
    "    return Z,A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is logistic regression loss:\n",
    "$$\n",
    "L(\\hat{y}, y) = -\\frac{1}{m} \\sum_i^m{(y_i\\log(\\hat{y}_i) + (1-y_i)\\log(1-\\hat{y}_i))}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is multi-class classification loss:\n",
    "$$\n",
    "L(\\hat{y}, y) = -\\frac{1}{m} \\sum_i^m \\sum_j^C (y_j^{(i)}\\log(\\hat{y}_j^{(i)}))\n",
    "$$\n",
    "where m is batch size, C is class number (10 classes), (i) is the i-th example, j is the vertical bit of result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(Y_hat, Y):\n",
    "    A = np.multiply(Y, np.log(Y_hat))\n",
    "    B = np.multiply(1-Y, np.log(1-Y_hat))\n",
    "    Loss = -A-B\n",
    "    #why not use Loss = -np.sum(A+B) / m?\n",
    "    return Loss\n",
    "\n",
    "def cost(loss):\n",
    "    return np.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(Y_hat):\n",
    "    return np.argmax(Y_hat,axis=0).reshape(-1,1)\n",
    "\n",
    "def accuracy(Y_predict, Y):\n",
    "    return np.sum(np.equal(Y_predict, back_one_hot(Y))) / Y_predict.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This formula is wrong, This is the logistic regression(binary classifier) Loss\n",
    "\n",
    "\n",
    "We already know L formula above:\n",
    "$$\n",
    "L = -\\frac{1}{m} \\sum_i^{m}[y^{(i)} log(\\hat{y}^{(i)}) + (1-y^{(i)}) log(1-\\hat{y}^{(i)})]\n",
    "$$\n",
    "\n",
    "$$\n",
    "A^{[L]} = \\hat{Y} = \n",
    "\\begin{bmatrix}\n",
    "\\hat{y}^{(1)} & ... & \\hat{y}^{(m)} \n",
    "\\end{bmatrix}\n",
    "\\quad, \\quad\n",
    "Y =\n",
    "\\begin{bmatrix}\n",
    "y^{(1)} & ... & y^{(m)}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "So:\n",
    "$$\n",
    "\"dAL\" = \\frac{\\partial L}{\\partial A^{[L]}}\n",
    "= - \\frac{1}{m} ( \\frac{Y}{A^{[L]}} - \\frac{1-Y}{1-A^{[L]}} )\n",
    "= \\frac{A^{[L]}-Y}{m A^{[L]}(1-A^{[L]})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def backpropagate_cost(Y, AL):\n",
    "    #m = Y.shape[1] #why not 1/m?\n",
    "    #dAL = -np.divide(Y, AL) #why not use this formula?\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    return dAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\"dZL\"\n",
    "= \\frac{\\partial L}{\\partial Z^{[L]}}\n",
    "= \\frac{\\partial L}{\\partial A^{[L]}} \\frac{\\partial A^{[L]}}{\\partial Z^{[L]}}\n",
    "= \"dAL\" * \\frac{\\partial A^{[L]}}{\\partial Z^{[L]}}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial A^{[L]}}{\\partial Z^{[L]}}\n",
    "= A^{[L]} - (A^{[L]})^{2}\n",
    "$$\n",
    "$$\n",
    "\"dZL\" = \"dAL\" * (\"AL\" - \"AL\"^{2})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagate_softmax(AL, dAL, Y=None, ZL=None):\n",
    "    dZ = np.multiply(dAL, (AL-np.power(AL,2)))\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\because\n",
    "$$\n",
    "\n",
    "$$\n",
    "Z^{[l]} = W^{[l]} * A^{[l-1]} + b^{[l]}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\"dZ\" = \\frac{\\partial L}{\\partial Z^{[l]}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\therefore\n",
    "$$\n",
    "\n",
    "$$\n",
    "\"dA\\_prev\" \n",
    "= \\frac{\\partial L}{\\partial A^{[l-1]}}\n",
    "= \\frac{\\partial L}{\\partial Z^{[l]}} \\frac{\\partial Z^{[l]}}{\\partial A^{[l-1]}}\n",
    "= \"dZ\" * W^{[l]}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\"dW\"\n",
    "= \\frac{\\partial L}{\\partial W^{[l]}}\n",
    "= \\frac{\\partial L}{\\partial Z^{[l]}} \\frac{\\partial Z^{[l]}}{\\partial W^{[l]}}\n",
    "= \"dZ\" * A^{[l-1]}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\"db\"\n",
    "= \\frac{\\partial L}{\\partial b^{[l]}}\n",
    "= \\frac{\\partial L}{\\partial Z^{[l]}} \\frac{\\partial Z^{[l]}}{\\partial b^{[l]}}\n",
    "= \"dZ\" * 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backpropagate_linear(dZ, W, A_prev):\n",
    "    m = dZ.shape[1] # I still don't know why move 1/m here from backpropagate_cost.\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    dW = 1/m * np.dot(dZ, A_prev.T)\n",
    "    db = 1/m * np.sum(dZ, axis=1).reshape(-1,1)\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\because\n",
    "$$\n",
    "\n",
    "$$\n",
    "A^{[l]} = ReLU(Z^{[l]}) = max(0, Z^{[l]})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\therefore\n",
    "$$\n",
    "\n",
    "$$\n",
    "\"dZ\"\n",
    "= \\frac{\\partial L}{\\partial Z^{[l]}}\n",
    "= \\frac{\\partial L}{\\partial A^{[l]}} \\frac{\\partial A^{[l]}}{\\partial Z^{[l]}}\n",
    "= \"dA\" * ReLU^{-1}(Z^{[l]})\n",
    "= \"dA\" * \n",
    "\\begin{bmatrix}\n",
    "1 (z \\geq 0)\\\\\n",
    "0 (z < 0)\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def backpropagate_ReLU(dA, Z):\n",
    "    return np.multiply(dA, (Z>=0).astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" neural network logic level \"\"\"\n",
    "\n",
    "def forwardpropagation_all(X):\n",
    "    global cache, parameters, hyper_parameters\n",
    "    layers = hyper_parameters['layers']\n",
    "    A_prev = X\n",
    "    for layer_idx, cells in enumerate(layers):\n",
    "        W = parameters['W'+str(layer_idx+1)]\n",
    "        b = parameters['b'+str(layer_idx+1)]\n",
    "        if layer_idx==len(layers)-1:\n",
    "            # Last layer's activation function should be softmax\n",
    "            Z, A = forward_propagation_each_layer(W, A_prev, b, softmax)\n",
    "        else:\n",
    "            Z, A = forward_propagation_each_layer(W, A_prev, b)\n",
    "        cache['Z'+str(layer_idx+1)] = Z\n",
    "        cache['A'+str(layer_idx+1)] = A\n",
    "        A_prev = A\n",
    "\n",
    "def backpropagate_all(X, Y):\n",
    "    global mnist, cache, parameters, hyper_parameters\n",
    "    # 1. Last layer has coss function and softmax function\n",
    "    L = str(hyper_parameters['L'])\n",
    "    cache['dA'+L] = backpropagate_cost(Y,cache['A'+L])\n",
    "    AL = cache['A'+L]\n",
    "    dAL = cache['dA'+L]\n",
    "    cache['dZ'+L] = backpropagate_softmax(AL, dAL)\n",
    "\n",
    "    # 2. Layers in between are similar: linear and ReLU\n",
    "    for i in np.arange(start=hyper_parameters['L']-1, stop=0, step=-1):\n",
    "        L = str(i)\n",
    "        cache['dA'+L], cache['dW'+str(i+1)], cache['db'+str(i+1)] = backpropagate_linear(cache['dZ'+str(i+1)], \n",
    "                                                                           parameters['W'+str(i+1)], \n",
    "                                                                           cache['A'+L])\n",
    "        cache['dZ'+L] = backpropagate_ReLU(cache['dA'+L], cache['Z'+L])\n",
    "\n",
    "    # 3. First layer has different parameter.\n",
    "    _, cache['dW1'], cache['db1'] = backpropagate_linear(cache['dZ1'], parameters['W1'], X)\n",
    "\n",
    "def update_parameters():\n",
    "    global cache, parameters\n",
    "    for i in range(hyper_parameters['L']):\n",
    "        L = str(i+1)\n",
    "        parameters['W'+L] -= cache['dW'+L] * hyper_parameters['learning_rate']\n",
    "        parameters['b'+L] -= cache['db'+L] * hyper_parameters['learning_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 > training error =  0.325000638609 ; accu = 0.15625\n",
      "30 > training error =  0.316938782974 ; accu = 0.1796875\n",
      "60 > training error =  0.291045093358 ; accu = 0.203125\n",
      "90 > training error =  0.224817473317 ; accu = 0.4609375\n",
      "120 > training error =  0.143926714829 ; accu = 0.8515625\n",
      "150 > training error =  0.0824082807758 ; accu = 0.953125\n",
      "180 > training error =  0.046768492164 ; accu = 0.9765625\n",
      "210 > training error =  0.0272478887268 ; accu = 0.9921875\n",
      "240 > training error =  0.0165749803639 ; accu = 1.0\n",
      "270 > training error =  0.0108118323484 ; accu = 1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD7CAYAAACG50QgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl8VOXd9/HPL5nsIQkhCXuEkIBsIhAWccMNxZ1W23pj\n+7jdaFVEn2q11ar1LlZbsbUtrm1dbkV91CK1bqXKoqBoouyLQNi3hDUQkpDlev6YgUaakAmEnMnM\n9/16zWsmZ64Tftdrwvecuc451zHnHCIiEhmivC5ARERajkJfRCSCKPRFRCKIQl9EJIIo9EVEIohC\nX0Qkgij0RUQiiEJfRCSCKPRFRCKIz+sCDpeRkeG6devmdRkiIq1KYWHhdudcZmPtQi70u3XrRkFB\ngddliIi0Kma2Lph2Gt4REYkgCn0RkQii0BcRiSAKfRGRCKLQFxGJIAp9EZEIotAXEYkgIXee/tEq\nWLuT2Su30ybOxwntEsnJTCI7PYlYn7ZrIiIHhU3of7V+F3/4aOW3lkVHGV3bJtA9I4mczOTAcxI5\nGcm0T4nDzDyqVkTEGxZqN0bPz893R3tFrnOO0opq1m4vo2j7PtaUlLF6exlrSspYs72M8qqaQ20T\nY6MPbQxy6mwMumcmkRwXNttCEYkQZlbonMtvrF1YpZuZkZoQw4CuaQzomvat92prHVtLK1izvYyi\nkn0UbS+jqKSMBRt28+7CzdTW2fZltYn7jw1Cn04pdEiJ17cDEWnVwir0jyQqyuiUlkCntAROzc34\n1nsVVTWs37mfopJ/f0Mo2l7Gh0u2srPswKF2GclxnNQllf6dUzmpSyqDstvSNim2pbsiInLUIib0\njyQ+Jpqe7dvQs32b/3hv9/4DrC7Zx+JNpSzcuIdFm3Yzc0XxoW8GJ3Zow7Du6QzPaccpPdqRlqiN\ngIiErrAa028pZZXVLNlcypdrd/J50Q4K1u6ivKqG6Chj8AltObd3Fuf0bk+PzGSvSxWRCBHsmL5C\nvxlU1dSycOMeZq4oZvrSbSzfuheAvKxkxgzqzGUnd6ZzWoLHVYpIOFPoe2jjrv18tKyYdxZspmDd\nLsxgWPd0rhzclYsHdCTOF+11iSISZhT6IWL9jv28PX8TU7/exJrtZWQkx3LV0GyuHn4C7VPivS5P\nRMKEQj/EOOeYu3oHz89Zy0fLtxFtxsUndeTWs/PIzdLYv4gcm4g8Tz+UmRmn5mZwam4G63fs56XP\n1jLli/VMW7CZS07qxG3n5JKb9Z9nD4mINCft6XtoZ9kBnvukiBfnrqW8qoZLB3TirvN70aVtotel\niUgro+GdVuRg+P/10zUA3HB6d348MlfTQYhI0IINfU1BGQLSk2K5+4ITmXHnSEb368DkGas5+7GZ\nfLB4q9eliUiYaTT0zSzazCab2Swzm2tm/Rtol2lm081sRqBtp8Dyi81sjpl9YWZ3NHcHwkmntAR+\n/4OBTL15BO2S47jp5UJu+t9CtpVWeF2aiISJYPb0rwB8zrkzgfuASfU1cs6VOOfOc86dBawG+ptZ\nIvAYMBoYAfzYzLKbp/TwNTC7LX+/9VT/3v+KYs57fBYfLN7idVkiEgaCCf0RwLtmNgSYAPRqqKGZ\nXW9mi4C+wNxA26VALfA0UAYMrGe9cWZWYGYFJSUlTe9FGIqJjuLHI3vwwe1n0D0jiZte/op7py6i\nos700CIiTRXsmP4YYBT+vf7qhho55/7inOsPvAhMDCxuj39v/2Hg4wbWe9Y5l++cy8/MzAy29ojQ\nPSOJN24awY1n5PDKvPVcPnkO63fs97osEWmlggn9QsA55yYCg4FlAGY23syeaGCd3UA5sBLoCtwF\nbARGAl8eY80RJ9YXxc8u7M0L1w5hy54KLpv8KfOKdnhdloi0QsGE/qtArJnNxr/3fvBgbDaQd7CR\nmY0ws5lm9iH+bwUTnXP7gLuBGYHHU865zc3ZgUgyslcWb99yKm2TYrn6L/N4/cv1XpckIq2MztNv\nhfaUV3HrlK/4ZOV27jq/F7eclet1SSLiMZ2nH8ZSE2J4/pohjBnYmd9+uIJH3l9OqG28RSQ06ZLP\nVsoXHcWkKweQGBvN07NWU1ZZzS8v7UtUlO7hKyINU+i3YlFRxq8u70dynI9nZhcRHWU8cEkf3bxd\nRBqk0G/lzIx7Rp9ITa3jz5+uISXex/8d1eClFCIS4RT6YcDMuPei3uyrrOYPH68iOd7HuDN6eF2W\niIQghX6YMDMmjunPvspqHn5vORnJcXxnUBevyxKREKPQDyPRUcbj3zuZnWUHuPuthXROS2BYTjuv\nyxKREKJTNsNMrC+Kp8YOJjs9kXH/W0hRyT6vSxKREKLQD0OpiTE8f81QoqOM6174kl1lB7wuSURC\nhEI/TGW3S+S5Hw1m8+4Kbnvta2pqdfGWiCj0w9rgE9L55WV9+WTldh6fvsLrckQkBCj0w9xVQ7O5\namhXJs9YrdsviohCPxI8eGlfBnRN4843FrBaB3ZFIppCPwLE+aJ5auwgYqKN8VO+1t23RCKYQj9C\ndEpL4LErB7B0SymPvL/c63JExCMK/QhyTu/2XHdqd16Yu5bpS7d5XY6IeEChH2HuHt2Lfp1TuOvN\nBWzeXe51OSLSwhT6ESbOF80frxpEVXUtt782n+qaWq9LEpEWpNCPQN0zkvjVmH58sXYnT85c7XU5\nItKCFPoRaszALlw6oBN/+Gglizft8bocEWkhCv0I9tBlfWmXHMsdr8/XaZwiEUKhH8HSEmP5zRUD\nWFm8j8enf+N1OSLSAhT6Ee7MnpmMHZbNc58UMa9oh9fliMhxptAXfn5hb7LTE/nJGwvYV1ntdTki\nchw1GvpmFm1mk81slpnNNbP+DbTLMrOpgXazzCwrsPwFM5tvZjPN7Jnm7oAcu6Q4H5OuHMDm3eX8\n6h9LvS5HRI6jYPb0rwB8zrkzgfuASQ202wlMCLSbA4yt897tzrmRzrkbj6laOW7yu6Uz7owevPbl\nBj5erqt1RcJVMKE/AnjXzIYAE4Be9TVyzlU759YHfuwEFAVebwceDezpX1XfumY2zswKzKygpKSk\naT2QZnPHeXn0at+Gn/1tEXvKq7wuR0SOg2DH9McAo/Dv9R9x0NfMbgGqnHPTAJxzdzrnhgGXAxPN\nLOnwdZxzzzrn8p1z+ZmZmU3qgDSfOF80v73yJEr2VvLr95Z5XY6IHAfBhH4h4JxzE4HBwDIAMxtv\nZk/UbWhmDwA5wLh6fk8NUAnohq0h7KQuaYeGeT5ZqW9dIuEmmNB/FYg1s9nAROCOwPJsIO9gIzP7\nLnAP/g3DDDN7KbD8CTP7GPgbcItzTuMGIe72c/PIyUzinrcW6WwekTBjzoXWDbPz8/NdQUGB12VE\nvIK1O7nymc/44fATeOiyfl6XIyKNMLNC51x+Y+10nr7UK79bOteM6MZLn63TRVsiYUShLw266/xe\nZKcncvdbCyk/oLl5RMKBQl8alBjr45Hv9mftjv08Pn2F1+WISDNQ6MsRjeiRwdhh2fzl0zV8tX6X\n1+WIyDFS6Euj7hl9Ih1S4vnpmws1BbNIK6fQl0a1iY/h4e/0Z1XxPv708SqvyxGRY6DQl6CM7JXF\ndwZ15ulZq1mxda/X5YjIUVLoS9Duu6gPbeJ9/HzqImprQ+v6DhEJjkJfgpaeFMu9F/WhcN0upnyx\nvvEVRCTkKPSlSb47qDMjerTj0feXs620wutyRKSJFPrSJGbGxDH9qayp5ZfvLPG6HBFpIoW+NFn3\njCRuOzuX9xZt5aNluuGKSGui0JejMu6MHvRsn8z905ZQppk4RVoNhb4clVhfFA+P6c+m3eU8Pv0b\nr8sRkSAp9OWo5XdLZ+ywbJ6fs4ZFG/d4XY6IBEGhL8fkpxecSLvkOH4+dRE1OndfJOQp9OWYpCbE\ncN9FvVm0aY/O3RdpBRT6cswuHdCJU3La8dsPlrN9X6XX5YjIESj05ZiZGf9zeV/Kq2p45P3lXpcj\nIkeg0JdmkZvVhutPy+HNwo0UrN3pdTki0gCFvjSb8Wfn0jE1nvveXkx1Ta3X5YhIPRT60myS4nzc\nf3Eflm/dy0ufrfO6HBGph0JfmtUF/TpwRs9Mfjf9G4o1IZtIyFHoS7MyM355aV8qq2t5+L1lXpcj\nIodpNPTNLNrMJpvZLDOba2b9G2iXZWZTA+1mmVlWYPnFZjbHzL4wszuauwMSerpnJHHjmTm8PX8z\nn63e4XU5IlJHMHv6VwA+59yZwH3ApAba7QQmBNrNAcaaWSLwGDAaGAH82Myyj71sCXU3j8ylS9sE\n7p+2mCod1BUJGcGE/gjgXTMbAkwAetXXyDlX7Zw7eElmJ6Ao0HYpUAs8DZQBAw9f18zGmVmBmRWU\nlJQ0vRcSchJio3ngkr6sLN7Hi3PXel2OiAQEO6Y/BhiFf6//iPPomtktQJVzblpgUXv8e/sPAx/X\nt45z7lnnXL5zLj8zMzPIkiTUnds7i7N6ZfL7f63UQV2REBFM6BcCzjk3ERgMLAMws/Fm9kTdhmb2\nAJADjAssWgl0Be4CNgIjgS+bpXIJeWbGA5f05YAO6oqEjGBC/1Ug1sxmAxOBgwdjs4G8g43M7LvA\nPfg3DDPM7CXn3D7gbmBG4PGUc25zM9YvIa5bnYO684p0UFfEa+ZcaE2Hm5+f7woKCrwuQ5pR+YEa\nzn18Fm3iffxj/Gn4onWmsEhzM7NC51x+Y+30v0+Ou4TYaH6hK3VFQoJCX1rE+X3b//tK3b06qCvi\nFYW+tAgz48FL+lBRremXRbyk0JcWk5OZzH+fnsPfvtqk6ZdFPKLQlxZ169m5dEqN5xfTlmj6ZREP\nKPSlRSXG+rjv4j4s21LKK/N0T12RlqbQlxY3ul8HTsvN4LF/rtA9dUVamEJfWpyZ8eClfamoquFR\nHdQVaVEKffFEblYy153WnTcKN1K4bpfX5YhEDIW+eOa2s/PokBLPA39fTE1taF0ZLhKuFPrimaQ4\nH/de1JvFm0qZ8oUO6oq0BIW+eOrikzoyokc7HvtwBTvLDnhdjkjYU+iLpw7eU7essprffKCDuiLH\nm0JfPJfXvg3XntqN1ws2MH/Dbq/LEQlrCn0JCbedk0dmchz3T9NBXZHjSaEvIaFNfAw/v7A3Czfu\n4fUvN3hdjkjYUuhLyLjs5E4M7Z7Obz5czi4d1BU5LhT6EjLMjIcu68veimp++88VXpcjEpYU+hJS\nTuyQwo9OOYFXv1jPwo06qCvS3BT6EnLuOK8n7ZLiuH/aEmp1UFekWSn0JeSkxMfws9EnMn/Dbt4s\n3Oh1OSJhRaEvIek7gzqTf0JbHvlgOXv2V3ldjkjYUOhLSPIf1O3H7v0HmDRdB3VFmotCX0JWn04p\n/HD4Cbz8+Tod1BVpJo2GvplFm9lkM5tlZnPNrH8D7VLN7D4zKzazkXWWzzSzzwPPv2zG2iUC/OT8\nXmQkx3HPW4uo0j11RY5ZMHv6VwA+59yZwH3ApAba9QCWA+/X894PnHMjnXMPHF2ZEqlS4mN46LK+\nLN1Syl8+XeN1OSKtXjChPwJ418yGABOAXvU1cs595Zx7Ezj8HLutwCtmNt3MzqtvXTMbZ2YFZlZQ\nUlLShPIlElzQryOj+rTnd9O/Ye32Mq/LEWnVgh3THwOMwr/XX92Uf8A59wPn3KnAdcDTDbR51jmX\n75zLz8zMbMqvlwjx0GX9iI2O4t63F+Gczt0XOVrBhH4h4JxzE4HBwDIAMxtvZk804d+qBUqbXqII\ndEiN5+7RJzJn1Q6duy9yDIIJ/VeBWDObDUwE7ggszwbyDjYys6FmVgBcDDxjZo8Glr9uZh8BzwHX\nNGPtEmH+a2g2Q7q1ZeJ7y9i+r9LrckRaJQu1r8r5+fmuoKDA6zIkRK0q3suFT3zKBf068IerBnpd\njkjIMLNC51x+Y+10nr60KrlZbbj5rB78fcFm/rV0m9fliLQ6Cn1pdW4emcuJHdrws6mLNO++SBMp\n9KXVifVFMel7A9hVdoAH31nidTkirYpCX1qlvp1SGX92HtPmb+aDxVu8Lkek1VDoS6t181k96Nc5\nhXunLmaHzuYRCYpCX1qtmOgoJl15MnsrqvnFtMW6aEskCAp9adV6dWjD7efl8d6irfxjoYZ5RBqj\n0JdWb9zpOQzomsYvpi2meG+F1+WIhDSFvrR6vugoJl05gPIDNdz5xkLdV1fkCBT6EhZys5L5xcV9\nmP1NCc/PXet1OSIhS6EvYWPssGzO69OeR99fzpLNe7wuRyQkKfQlbJgZj373JNISY5jw2nzKD9R4\nXZJIyFHoS1hJT4rl8e+dzKrifUx8b6nX5YiEHIW+hJ3T8jK48YwcXv58PR8s3up1OSIhRaEvYekn\no3oxoEsqd725gPU79ntdjkjIUOhLWIr1RfGn/xpElBk/fqWQiiqN74uAQl/CWNf0RB7/3gCWbC7l\noX9ofF8EFPoS5s7p3Z6bzuzBlHnrefvrTV6XI+I5hb6EvTtH9WRot3R+9rdFrNy21+tyRDyl0Jew\n54uO4o//NZCkOB/j/reQPfurvC5JxDMKfYkI7VPiefrqQWzctZ/xr31NjebnkQil0JeIkd8tnYcu\n68fsb0r4zQfLvS5HxBM+rwsQaUlXDc1m6eZSnpldRO+OKVw+sLPXJYm0KO3pS8S5/5I+DO2ezt1v\nLWThxt1elyPSohoNfTOLNrPJZjbLzOaaWf8G2qWa2X1mVmxmI+ssv8HMPjWzAjP7fjPWLnJUYqKj\neGrsIDKS47j+xQI27S73uiSRFhPMnv4VgM85dyZwHzCpgXY9gOXA+wcXmFkX4GbgbOAc4DEzSzym\nikWaQbvkOJ6/dggVVTVc+/wXlFbojB6JDMGE/gjgXTMbAkwAetXXyDn3lXPuTaDuaRFDgJlAW+BZ\noAzIO3xdMxsX+CZQUFJS0rQeiBylnu3b8MzVgykqKePHLxdyoLrW65JEjrtgx/THAKPw7/VXN/Hf\n6A3cC4wHltTXwDn3rHMu3zmXn5mZ2cRfL3L0RuRm8Mh3T2LOqh3cO3URzulUTglvwYR+IeCccxOB\nwcAyADMbb2ZPNLLufKAbcAdQgX8DsOKoqxU5Dq4Y3IUJ5+TxRuFGfv+vlV6XI3JcBXPK5qvAKDOb\nDVQBNwWWZ1NnqMbMhgJP4g/5U8zsbefc3Wb2HPBZYN2fOucqmrF+kWZx+7l5bNpdzhMfraRtYgzX\nnNrd65JEjgsLta+z+fn5rqCgwOsyJAJV19Ry8ytf8c+l2/jd9wcwZmAXr0sSCZqZFTrn8htrp/P0\nRQJ80VH84aqBnJLTjjvfWMi/lm7zuiSRZqfQF6kjPiaa5/5PPv06pXDzlK/4bPUOr0sSaVYKfZHD\nJMf5eP7aoWSnJ3L9i18yr0jBL+FDoS9Sj/SkWKbcMIyOqfFc+8KXfLFmp9cliTQLhb5IA7JS4nn1\nv4fTMTWea57/QsEvYUGhL3IECn4JNwp9kUbUDf4f/XUeM1cUe12SyFFT6IsEISslntdvPIWcjGRu\neLGAdxZs9rokkaOi0BcJUkZyHK/dOJxB2W257bWveWXeOq9LEmkyhb5IE6TEx/DidUM5q1cW905d\nzJ8+XqlJ2qRVUeiLNFFCbDTP/HAwl5/cicf++Q33vLWIqhpNyyytg+6RK3IUYqKjePx7J9M1PZE/\nfryKjbv38+TYwaQmxHhdmsgRaU9f5ChFRRk/GdWL315xEvOKdnLFU3PZsHO/12WJHJFCX+QYXZnf\nlZeuG8q20gounzxH8/VISFPoizSDEbkZTL3lVNISY7j6L/P48ydFOsArIUmhL9JMemQm8/Ytp3Ju\n7yx+9e4ybn99PuUHarwuS+RbFPoizahNfAxPjR3MXef34u8LNjPmyTmsKt7ndVkihyj0RZpZVJRx\ny1m5PH/NEIr3VnLJHz/l/xVs0HCPhASFvshxMrJXFu9POJ2Tu6bx0zcXMuG1+eytqPK6LIlwCn2R\n46h9Sjwv3zCMO0f15N1FW7jwD5/wuW7KIh5S6IscZ9FRxq1n5/H6uOEYxg+e/ZwH/76E/QeqvS5N\nIpBCX6SF5HdL54PbT+eaEd14Ye5aLvi99vql5Sn0RVpQYqyPBy/ty2vjhgPwg2c/54FpizXWLy1G\noS/igeE57Q7t9b/0+TrOmTSLafM36QwfOe4aDX0zizazyWY2y8zmmln/BtolmdkUM5tpZh+bWZfA\n8plm9nng+ZfN3QGR1urgXv/Um0+lfUo8E16bz9g/z2NV8V6vS5MwFsye/hWAzzl3JnAfMKmBdrcC\ni5xzI4GXgfvrvPcD59xI59wDx1KsSDg6uWsab99yKv9zeT8Wb9rDBb//hF+/v0xDPnJcBBP6I4B3\nzWwIMAHo1Ui7C4BLgCGB5VuBV8xsupmdd6wFi4Sj6Cjjh8NPYMadI/nOoM48M6uIM387kxfnruVA\ntebql+ZjjY0hmtkTQAqwCvgNsNw516OedtOADcBS4BVgpnNuYJ33uwaW1bfuOGAcQHZ29uB163Qb\nOolsCzfu5tfvLeezoh2c0C6Rn55/Ihf274CZeV2ahCgzK3TO5TfWLpg9/ULAOecmAoOBZYF/YHxg\ng1C3XbFz7klgNDDnsN9TC5TW9w845551zuU75/IzMzODKEkkvJ3UJY0p/z2M568dQrwvmlumfMXl\nT85l9jclOtgrxySYO2e9Cowys9lAFXBTYHk2kFen3RP4h3FmAnuB6wHM7HUgA6gErmmWqkUigJlx\nVq8szsjL5K2vNvK76d/wo79+wcDsNG47J4+RPTO15y9N1ujwTkvLz893BQUFXpchEnIqq2t4s3Aj\nT85Yzabd5Qzokspt5+Rx9olZCn8JenhHoS/SyhyormXq1xv504xVbNhZTs/2ydxwWg6XntyJ+Jho\nr8sTjyj0RcJcVU0t7yzYzHOfrGHZllIykmP54fBuXD08m3bJcV6XJy1MoS8SIZxzfLZ6B899UsSM\nFSXE+aK4ZEAnxg7L5uSuaRr6iRDBhn4wB3JFJISZGSNyMxiRm8Gq4r38dc5a3v56E28WbqR3xxTG\nDsvm8oGdSY7Tf3fRnr5IWNpbUcW0+ZuZMm89S7eUkhgbzUX9OzJmUGeGd29HVJT2/sONhndEBOcc\nCzbuYcq8dby3aCv7KqvplBrPZQM7M2ZgZ3q2b+N1idJMFPoi8i3lB2qYvmwbU7/ayOyV26mpdfTt\nlMKlAzoxul9Hstslel2iHAOFvog0qGRvJe8s2Mzb8zexcOMeAHp3TGF0vw5c0K8DeVnJOgDcyij0\nRSQoG3bu58MlW/lg8VYK1+/COcjJSOK8vu0Z2TOL/G5tiYnWrTdCnUJfRJqsuLSCD5du44PFW5hX\ntJPqWkdynI/TcjMY2SuTM3tl0jE1wesypR4KfRE5Jnsrqpizagezvilm5ooStuypAKBn+2ROyWnH\n8Jx2DO2ergvBQoRCX0SajXOOb7btY+aKYj5dtZ2Ctbsor6oB/BuB4XU2AhnaCHhCoS8ix01VTS2L\nNu3h86IdfF60k4K1O9l/wL8RyE5PZGB2Gid3TWNgdlv6dEwh1qdjAsebQl9EWszBjcCXa3Yyf8Nu\nvl6/m62l/uGg2Ogo+nZO4eSuafTpmEKfTinkZbXRhqCZaRoGEWkxMdFRDMpuy6DstoeWbdlTzvz1\nuw9tBF77YsOhISFflJGblXxoI9CnYwq9O6bQNinWqy5EDIW+iBwXHVMT6Ng/gdH9OwJQU+tYu6OM\npZtLWballKVbSvl01Xb+9vWmQ+tkJMeSk5lMj8xkcrOS6ZGZRI/MZDqnJWjqiGai0BeRFhEdZfQI\nBPolAzodWr59XyXLtvg3BKuLy1hdso/3F29h9/6qQ23iY6LIyUimW0YiXdMT6do2kex0/6NTWoKG\nippAoS8inspIjuP0vExOz/v3/bGdc+wsO8DqkjJWFe9jdYn/sXzrXv61tJgDNbWH2kaZ/1tF1/QE\nstMT6ZyWSMfUeDrUebSJ8+kK4wCFvoiEHDOjXXIc7ZLjGNo9/Vvv1dY6tu2tYP2O/azfuZ8NOwPP\nu8r5eHkJ2/dV/sfvS4qNpkNqPB1TEwLP8WSlxJORFEtGmzjaBZ4jYeOg0BeRViUqyvzHC1ITGJbT\n7j/er6yuobi0kq2lFWzZU8HWPeWBZ//Pn67cTvHeCmrrOXEx1hd1aEOQkfzvjUG7pFjSEmNJS4gh\nLdH/SE2IJS0xptVNUaHQF5GwEueL9o/7pzc8a2h1TS07yw6wfd8Btu+rZPu+SnYEXpcEXm8rrWDp\n5lJ2lFVSVdPwqe3JcT5S62wM0hJiSU2MIS0hhjbxMSTH+0iJ95Ec5/P/HOejTbz/kRznw9fCGw2F\nvohEHF90FFkp/iGexjjnKC2vZk95FbvLD7B7fxW7y6vYvT/wer9/+Z7A8uV7Sv1t91dRXd/XicMk\nxESTHNgI3H5uTy6tc5D7eFDoi4gcgZmRmhhDamIM2QR/zwHnHBVVteytrGJvRTX7KqrZV1nN3gr/\nz3sDP9dd1jYx5jj2xE+hLyJyHJgZCbHRJMRGkxVCNyhrXUcgRETkmDQa+mYWbWaTzWyWmc01s/4N\ntEsysylmNtPMPjazLoHlN5jZp2ZWYGbfb+4OiIhI8ILZ078C8DnnzgTuAyY10O5WYJFzbiTwMnB/\nIPhvBs4GzgEeMzPdiFNExCPBhP4I4F0zGwJMAHo10u4C4BJgSOAxE2gLPAuUAXmHr2hm4wLfBApK\nSkqa3AkREQlOsGP6Y4BR+Pf6q4/QbhyQA1xTZ1lv4F5gPLCkvpWcc8865/Kdc/mZmZn1NRERkWYQ\nTOgXAs45NxEYDCwDMLPxZvbEYe2KnXNPAqOBOcB8oBtwB1CBfwOwotmqFxGRJgnmlM1XgVFmNhuo\nAm4KLM/m20M1TwCvmNlMYC9wvXOu2MyeAz4LrPtT51xFcxUvIiJNoztniYiEgVZ7u0QzKwHWHeXq\nGcD2ZizHS+pLaFJfQpP6Aic45xo9KBpyoX8szKwgmC1da6C+hCb1JTSpL8HTFbkiIhFEoS8iEkHC\nLfSf9bqAZqS+hCb1JTSpL0EKqzF9ERE5snDb0xcRkSNQ6IuIRJCwCP1gp38OdWZWEZiaeqaZjTGz\noWY228yvn8eCAAADHElEQVQ+M7PHvK7vSMwsPjA1x1ozuyaw7GIzm2NmX5jZHYFlWWb2TqCP75hZ\nqqeF16OBvrxgZvMDdT8TWFbvdOKhxMwSzexlM5thZvPMrFd9f1eh3pcG+vGgmS0P1Dwt0K5VZIGZ\nfRjoy2dm1r9FPxPnXKt/AN8Hngm8Phv4p9c1HWU/1h728wIgO/B6NjDC6xqPUPsJwA3ARPwT7iUC\ny4EU/NN9fIN/6o6ngKsC6zwE/Nzr2hvrS2DZC8DIw9rdDfws8Po64Fmva2+gPz0Cz+PwT5fyH39X\nraEv9fTjwYOfT502rSoLgPuBG1vyMwmLPX2Cn/451FUHtvZ/M7PBQA2wxcx+A8Tin6o6JDnn1jnn\n/ox/jiXwfwZLgVrgafzTag/k35/Vj4BhhGCf6ukL+K+QfDSw13VVYFl904mHHOfc6sDLTsBO6v+7\nCvm+HNaPImA3cKuZfWJmtwXeaxVZYGYXmVkB8CPgH7TgZxIuoQ/BT/8cspxzuc65M/DfhOZXQDzw\nJ/yT3r3iZW1HqT3wGPAw8HGd5fcClcCdXhR1NJxzdzrnhgGXAxPNLCnwVn3TiYccM7sc/0b3SRr+\nuwr5vtTpx2Tn3O+d/8rV84CrzexgwId8Fjjn3g3Ufhv+naIW+0zCJfTrnf65FTNgDRAD/Br/FNUX\nAnO9LKqJVgJdgbuAjcBI4Ev8n9US59zrwEX4p+BuTWrwb7AOUP904iHHzK4HrgSudM6VUP/fVcj3\n5bB+1A3zmsDzXlpfFuwG9tOCn0lYnKdvZjHA8/jHjKuAm5xzK72tqmnMLAt4C3+gFAO34N+jeRT/\nvQimO+ce8q7CIwscZHob/1fvSuAT4H3gJ4Gfn3fO/dnMugIv4d+wbQFucM6VeVN1/Rroyy6gP/6A\n+bVz7uPAQehXgGTqTCfuTdX1M7N8YB7+wKjFv7F6hMP+rkK9Lw30Ywb+PXoHPO2c+3+tIQvMrDv+\nY0QVQClwO/5hqBb5TMIi9EVEJDjhMrwjIiJBUOiLiEQQhb6ISARR6IuIRBCFvohIBFHoi4hEEIW+\niEgEUeiLiESQ/w8fsdljSKR0YAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc1b765fc18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def model(X, Y, learning_rate=0.01, print_every=100, iteration=500, hidden_layers=[100], batch_size=128):\n",
    "    init(hidden_layers=hidden_layers, C=Y.shape[0], X_size=X.shape[0], learning_rate=learning_rate)\n",
    "    # costs will be returned for plotting\n",
    "    costs = []\n",
    "    X_full = X\n",
    "    Y_full = Y\n",
    "    np.random.seed(1)\n",
    "    for i in range(iteration):\n",
    "        permutation = np.random.permutation(X.shape[1])\n",
    "        X = X_full[:, permutation]\n",
    "        X = X[:, :batch_size]\n",
    "        Y = Y_full[:, permutation]\n",
    "        Y = Y[:, :batch_size]\n",
    "        \n",
    "        forwardpropagation_all(X)\n",
    "        Y_hat = cache['A'+str(hyper_parameters['L'])]\n",
    "        cost_value = cost(loss(Y_hat, Y))\n",
    "        costs.append(cost_value)\n",
    "        if i%print_every==0:\n",
    "            Y_predict = predict(Y_hat)\n",
    "            accu = accuracy(Y_predict, Y)\n",
    "            print(i,'> training error = ',cost_value,'; accu =',accu)\n",
    "        backpropagate_all(X, Y)\n",
    "        update_parameters()\n",
    "        if np.isnan(cost_value):\n",
    "            print(\"ERROR to nan!!!\")\n",
    "            break\n",
    "    return costs\n",
    "\n",
    "def test_model():\n",
    "    m = 10\n",
    "    X = np.random.randn(784,m)\n",
    "    Y = one_hot(np.random.randint(low=0, high=10, size=(m,1)), C=10)\n",
    "    #print(X[:,1])\n",
    "    #print(Y[:,1])\n",
    "    costs = model(X,Y,learning_rate=0.1, print_every=10, iteration=100,\n",
    "                 hidden_layers=[10,10])\n",
    "    plt.plot(costs)\n",
    "    plt.show()\n",
    "def test_model_1():\n",
    "    costs = model(mnist['X_train'], mnist['Y_train'], \n",
    "                  learning_rate=0.1, print_every=30, iteration=300,\n",
    "                  batch_size=128,\n",
    "                  hidden_layers=[784,1000])\n",
    "    plt.plot(costs)\n",
    "    plt.show()\n",
    "\n",
    "test_model_1()\n",
    "#model(mnist['X_train'], mnist['Y_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache: [('A1', (784, 128)),\n",
      " ('A2', (1000, 128)),\n",
      " ('A3', (10, 128)),\n",
      " ('Z1', (784, 128)),\n",
      " ('Z2', (1000, 128)),\n",
      " ('Z3', (10, 128)),\n",
      " ('dA1', (784, 128)),\n",
      " ('dA2', (1000, 128)),\n",
      " ('dA3', (10, 128)),\n",
      " ('dW1', (784, 784)),\n",
      " ('dW2', (1000, 784)),\n",
      " ('dW3', (10, 1000)),\n",
      " ('dZ1', (784, 128)),\n",
      " ('dZ2', (1000, 128)),\n",
      " ('dZ3', (10, 128)),\n",
      " ('db1', (784, 1)),\n",
      " ('db2', (1000, 1)),\n",
      " ('db3', (10, 1))]\n",
      "parameters: [('W1', (784, 784)),\n",
      " ('W2', (1000, 784)),\n",
      " ('W3', (10, 1000)),\n",
      " ('b1', (784, 1)),\n",
      " ('b2', (1000, 1)),\n",
      " ('b3', (10, 1))]\n",
      "hyper_parameters: ['C', 'L', 'X_size', 'layers', 'learning_rate']\n"
     ]
    }
   ],
   "source": [
    "debug_show_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
