{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this notebook, I am going to implement a deep network from numpy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Python Standard Library struct and array\n",
    "# for dealing with reading dataset from file\n",
    "import struct\n",
    "from array import array\n",
    "\n",
    "# Numpy for calculating\n",
    "import numpy as np\n",
    "\n",
    "# To draw the training and dev cost value curve\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Test Cases\n",
    "import test\n",
    "\n",
    "# Some Useful Helper Functions\n",
    "import helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_mnist():\n",
    "    \"\"\" \n",
    "    load MNIST dataset into numpy array \n",
    "    MNIST dataset can be downloaded manually.\n",
    "    url: http://yann.lecun.com/exdb/mnist/\n",
    "    \"\"\"\n",
    "    ret = {}\n",
    "    with open('MNIST/train-images.idx3-ubyte', 'rb') as f:\n",
    "        magic, size, rows, cols = struct.unpack(\">IIII\", f.read(16))\n",
    "        assert(magic==2051)\n",
    "        ret['X_train'] = np.array(array(\"B\", f.read())).reshape(size,rows,cols)\n",
    "\n",
    "    with open('MNIST/t10k-images.idx3-ubyte', 'rb') as f:\n",
    "        magic, size, rows, cols = struct.unpack(\">IIII\", f.read(16))\n",
    "        assert(magic==2051)\n",
    "        ret['X_test'] = np.array(array(\"B\", f.read())).reshape(size,rows,cols)\n",
    "\n",
    "    with open('MNIST/train-labels.idx1-ubyte', 'rb') as f:\n",
    "        magic, size = struct.unpack(\">II\", f.read(8))\n",
    "        assert(magic==2049)\n",
    "        ret['Y_train'] = np.array(array(\"B\", f.read())).reshape(size,1)\n",
    "\n",
    "    with open('MNIST/t10k-labels.idx1-ubyte', 'rb') as f:\n",
    "        magic, size = struct.unpack(\">II\", f.read(8))\n",
    "        assert(magic==2049)\n",
    "        ret['Y_test'] = np.array(array(\"B\", f.read())).reshape(size,1)\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle and divide the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle_divide_dataset( dataset, len_of_dev=10000 ):\n",
    "    \"\"\"\n",
    "    Shuffle and divide the dataset\n",
    "    \n",
    "    len_of_dev: 10,000 is a reasonable number for dev set.\n",
    "                Dev dataset with this size is big enough to measure variance problem.\n",
    "    \"\"\"       \n",
    "    assert('X_train' in dataset)\n",
    "    assert(len(dataset)==4)\n",
    "    \n",
    "    \"\"\" random shuffle the training set \"\"\"\n",
    "    np.random.seed(1)\n",
    "    permutation = np.random.permutation(dataset['X_train'].shape[0])\n",
    "    dataset['X_train'] = dataset['X_train'][permutation]\n",
    "    dataset['Y_train'] = dataset['Y_train'][permutation]\n",
    "\n",
    "    \"\"\" divide trainset into trainset and devset \"\"\"\n",
    "    dataset['X_dev'] = dataset['X_train'][:len_of_dev]\n",
    "    dataset['Y_dev'] = dataset['Y_train'][:len_of_dev]\n",
    "    dataset['X_train'] = dataset['X_train'][len_of_dev:]\n",
    "    dataset['Y_train'] = dataset['Y_train'][len_of_dev:]\n",
    "\n",
    "    print('X_train:', dataset['X_train'].shape,\n",
    "          'X_dev:', dataset['X_dev'].shape,\n",
    "          'X_test:', dataset['X_test'].shape)\n",
    "    print('Y_train:', dataset['Y_train'].shape,\n",
    "          'Y_dev:', dataset['Y_dev'].shape,\n",
    "          'Y_test:', dataset['Y_test'].shape)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually check the dataset by random visualize some of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def manually_validate_mnist_dataset(dataset):\n",
    "    \"\"\"Manually check the dataset by random visualize some of them\"\"\"\n",
    "    random_train = np.random.randint(1, len(dataset['X_train']))-1\n",
    "    random_dev = np.random.randint(1, len(dataset['X_dev']))-1\n",
    "    random_test = np.random.randint(1, len(dataset['X_test']))-1\n",
    "    print(dataset['Y_train'][random_train], dataset['Y_dev'][random_dev], dataset['Y_test'][random_test])\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1,3, sharey=True, figsize=[10,3])\n",
    "    ax1.imshow(dataset['X_train'][random_train], cmap='gray')\n",
    "    ax2.imshow(dataset['X_dev'][random_dev], cmap='gray')\n",
    "    mappable = ax3.imshow(dataset['X_test'][random_test], cmap='gray')\n",
    "    fig.colorbar(mappable)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize the dataset\n",
    "\n",
    "$$\n",
    "z = \\frac{x - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "#### Notice: the MNIST gray images have a lot of areas of black (0), relatively few areas of white(255), so the standardization has a result of roughly (-0.4, 2.8).\n",
    "#### Maybe we can also use divide 256 to scale the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standardize( dataset ):\n",
    "    \"\"\"use standard sccore to normalize input dataset\"\"\"\n",
    "    assert('X_train' in dataset)\n",
    "    mu = np.mean(dataset['X_train'], keepdims=False)\n",
    "    sigma = np.std(dataset['X_train'], keepdims=False)\n",
    "    for key, val in dataset.items():\n",
    "        if key[:2]=='X_':\n",
    "            dataset[key] = ( dataset[key] - mu ) / sigma\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess dataset into vectors\n",
    "#### Notice: Andrew's course use the format of vector above, but tensorflow does it in it's transpose way.\n",
    "## Flat Images(X) into vectors, and stack them into matrix\n",
    "\n",
    "#### Input Format: X (m, width, height); Output Format:\n",
    "$$ X = \n",
    "\\begin{bmatrix}\n",
    "\\vert & & \\vert & & \\vert \\\\\n",
    "x^{(1)} & ... & x^{(i)} & ... & x^{(m)} \\\\\n",
    "\\vert & & \\vert & & \\vert\n",
    "\\end{bmatrix} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flat_stack( dataset ):\n",
    "    \"\"\"input dataset format: (m, width, height)\"\"\"\n",
    "    for key, val in dataset.items():\n",
    "        if key[:2]=='X_':\n",
    "            width = dataset[key].shape[1]\n",
    "            height = dataset[key].shape[2]\n",
    "            dataset[key] = dataset[key].reshape(-1, width*height).T\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encode Labels(Y)\n",
    "#### Input Format: Y (m, label_number); output Format:\n",
    "$$\n",
    "Y = \n",
    "\\begin{bmatrix}\n",
    "\\vert & & \\vert & & \\vert \\\\\n",
    "y_{one\\_hot}^{(1)} & ... & y_{one\\_hot}^{(i)} & ... & y_{one\\_hot}^{(m)} \\\\\n",
    "\\vert & & \\vert & & \\vert \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot( dataset ):\n",
    "    min_label_number = np.min(dataset['Y_train'], keepdims=False)\n",
    "    max_label_number = np.max(dataset['Y_train'], keepdims=False)\n",
    "    C = max_label_number - min_label_number + 1\n",
    "    for key, val in dataset.items():\n",
    "        if key[:2]=='Y_':\n",
    "            # all label number should be trained in Y_train\n",
    "            assert(min_label_number <= np.min(dataset[key], keepdims=False))\n",
    "            assert(max_label_number >= np.max(dataset[key], keepdims=False))\n",
    "            Y = dataset[key]\n",
    "            Y_onehot = np.zeros((C, Y.shape[0]))\n",
    "            Y_onehot[Y.reshape(-1).astype(int), np.arange(Y.shape[0])] = 1\n",
    "            dataset[key] = Y_onehot\n",
    "    return dataset\n",
    "\n",
    "def back_one_hot(Y_onehot):\n",
    "    \"\"\" This is an inverse function of one hot, in case we need to interpret the result. \"\"\"\n",
    "    Y = np.repeat( [np.arange(Y_onehot.shape[0])], repeats=Y_onehot.shape[1], axis=0 )\n",
    "    assert(Y.shape == Y_onehot.T.shape)\n",
    "    Y = Y[Y_onehot.T.astype(bool)]\n",
    "    return Y.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init cache, parameters, hyper_parameters\n",
    "\n",
    "**cache** is used for store results in steps, such as Z1, A1, Z2, A2 and so on.\n",
    "\n",
    "**parameters** are the model itself, consist of W and b, which can be adjusted during training.\n",
    "\n",
    "**hyper_parameters** are the settings for training.\n",
    "\n",
    "_(I am still thinking should the structure of the network be a parameter? Or should there be structure, parameters, training conditions?)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init(X_size, hidden_layers=[10,10,5], C=10, learning_rate=0.01, regularization=0.1):\n",
    "    \"\"\" init cache, parameters and hyper_parameters \"\"\"\n",
    "    global cache, parameters, hyper_parameters\n",
    "    cache = {}\n",
    "    parameters = {}\n",
    "    hyper_parameters = {}\n",
    "\n",
    "    \"\"\" init hyper_parameters \"\"\"\n",
    "    hyper_parameters['X_size'] = X_size\n",
    "    # layers of network, include the last softmax layer\n",
    "    # hidden layers use ReLU activation function, and the last layer use Softmax function\n",
    "    hyper_parameters['layers'] = hidden_layers + [C]\n",
    "    # L layers in total\n",
    "    hyper_parameters['L'] = len(hyper_parameters['layers'])\n",
    "    # Class numbers 0-9 is 10\n",
    "    hyper_parameters['C'] = C\n",
    "    hyper_parameters['learning_rate'] = learning_rate\n",
    "    hyper_parameters['regularization'] = regularization\n",
    "    # tiny_float to avoid divide by zero\n",
    "    hyper_parameters['tiny_float'] = 1e-7\n",
    "    \n",
    "    \"\"\" init W b or any other parameters in every layer \"\"\"\n",
    "    layers = hyper_parameters['layers']\n",
    "    x_size = hyper_parameters['X_size']\n",
    "    cells_prev = x_size\n",
    "    for layer_idx, cells in enumerate(layers):\n",
    "        parameters['W'+str(layer_idx+1)] = np.random.randn(cells, cells_prev) * 0.01\n",
    "        parameters['b'+str(layer_idx+1)] = np.zeros((cells, 1))\n",
    "        cells_prev = layers[layer_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "# after this line, the code is still waiting for refactory\n",
    "# I paste those code just for testing, make sure I don't have any \n",
    "# mistake above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ReLU(X):\n",
    "    return X * (X > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    s = np.sum(np.exp(X), axis=0)\n",
    "    return np.exp(X) / s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation_each_layer(W, A_prev, b, activation_function=ReLU):\n",
    "    Z = np.dot(W, A_prev) + b\n",
    "    A = activation_function(Z)\n",
    "    return Z,A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(Y_hat, Y):\n",
    "    A = np.multiply(Y, np.log(Y_hat+hyper_parameters['tiny_float']))\n",
    "    B = np.multiply(1-Y, np.log(1-Y_hat+hyper_parameters['tiny_float']))\n",
    "    Loss = -A-B\n",
    "    #why not use Loss = -np.sum(A+B) / m?\n",
    "    return Loss\n",
    "\n",
    "def cost(loss):\n",
    "    return np.mean(loss)\n",
    "\n",
    "def cost_with_regularization(cost):\n",
    "    global parameters, hyper_parameters\n",
    "    if hyper_parameters['regularization']==0:\n",
    "        return cost\n",
    "    s = 0\n",
    "    for i in range(hyper_parameters['L']):\n",
    "        L = str(i+1)\n",
    "        s += np.sum(np.mean(np.dot(parameters['W'+L].T, parameters['W'+L]), axis=1), keepdims=False)\n",
    "    \n",
    "    cost += hyper_parameters['regularization'] / 2 * s\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(Y_hat):\n",
    "    return np.argmax(Y_hat,axis=0).reshape(-1,1)\n",
    "\n",
    "def accuracy(Y_predict, Y):\n",
    "    return np.sum(np.equal(Y_predict, back_one_hot(Y))) / Y_predict.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backpropagate_cost(Y, AL):\n",
    "    #m = Y.shape[1] #why not 1/m?\n",
    "    #dAL = -np.divide(Y, AL) #why not use this formula?\n",
    "    dAL = - (np.divide(Y, AL+hyper_parameters['tiny_float']) - np.divide(1 - Y, 1 - AL+hyper_parameters['tiny_float']))\n",
    "    return dAL\n",
    "\n",
    "def backpropagate_softmax(AL, dAL, Y=None, ZL=None):\n",
    "    dZ = np.multiply(dAL, (AL-np.power(AL,2)))\n",
    "    return dZ\n",
    "\n",
    "def backpropagate_linear(dZ, W, A_prev):\n",
    "    m = dZ.shape[1] # I still don't know why move 1/m here from backpropagate_cost.\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    dW = 1/m * np.dot(dZ, A_prev.T)\n",
    "    db = 1/m * np.sum(dZ, axis=1).reshape(-1,1)\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def backpropagate_ReLU(dA, Z):\n",
    "    return np.multiply(dA, (Z>=0).astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" neural network logic level \"\"\"\n",
    "\n",
    "def forwardpropagation_all(X):\n",
    "    global cache, parameters, hyper_parameters\n",
    "    layers = hyper_parameters['layers']\n",
    "    A_prev = X\n",
    "    for layer_idx, cells in enumerate(layers):\n",
    "        W = parameters['W'+str(layer_idx+1)]\n",
    "        b = parameters['b'+str(layer_idx+1)]\n",
    "        if layer_idx==len(layers)-1:\n",
    "            # Last layer's activation function should be softmax\n",
    "            Z, A = forward_propagation_each_layer(W, A_prev, b, softmax)\n",
    "        else:\n",
    "            Z, A = forward_propagation_each_layer(W, A_prev, b)\n",
    "        cache['Z'+str(layer_idx+1)] = Z\n",
    "        cache['A'+str(layer_idx+1)] = A\n",
    "        A_prev = A\n",
    "\n",
    "def backpropagate_all(X, Y):\n",
    "    global cache, parameters, hyper_parameters\n",
    "    # 1. Last layer has coss function and softmax function\n",
    "    L = str(hyper_parameters['L'])\n",
    "    cache['dA'+L] = backpropagate_cost(Y,cache['A'+L])\n",
    "    AL = cache['A'+L]\n",
    "    dAL = cache['dA'+L]\n",
    "    cache['dZ'+L] = backpropagate_softmax(AL, dAL)\n",
    "\n",
    "    # 2. Layers in between are similar: linear and ReLU\n",
    "    for i in np.arange(start=hyper_parameters['L']-1, stop=0, step=-1):\n",
    "        L = str(i)\n",
    "        cache['dA'+L], cache['dW'+str(i+1)], cache['db'+str(i+1)] = backpropagate_linear(cache['dZ'+str(i+1)], \n",
    "                                                                           parameters['W'+str(i+1)], \n",
    "                                                                           cache['A'+L])\n",
    "        cache['dZ'+L] = backpropagate_ReLU(cache['dA'+L], cache['Z'+L])\n",
    "\n",
    "    # 3. First layer has different parameter.\n",
    "    _, cache['dW1'], cache['db1'] = backpropagate_linear(cache['dZ1'], parameters['W1'], X)\n",
    "\n",
    "def update_parameters():\n",
    "    global cache, parameters\n",
    "    m = cache['dZ1'].shape[1]\n",
    "    for i in range(hyper_parameters['L']):\n",
    "        L = str(i+1)\n",
    "        parameters['W'+L] -= cache['dW'+L] * hyper_parameters['learning_rate'] \\\n",
    "        + hyper_parameters['regularization'] * parameters['W'+L] * hyper_parameters['learning_rate'] / m\n",
    "        parameters['b'+L] -= cache['db'+L] * hyper_parameters['learning_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X_full, Y_full, X_dev, Y_dev, \n",
    "          learning_rate=0.01, print_every=100, iteration=500, \n",
    "          hidden_layers=[100], batch_size=128, regularization=0):\n",
    "    init(hidden_layers=hidden_layers, C=Y_full.shape[0], X_size=X_full.shape[0], \n",
    "         learning_rate=learning_rate, regularization=regularization)\n",
    "    # costs will be returned for plotting\n",
    "    costs = []\n",
    "    costs_index = []\n",
    "    costs_dev = []\n",
    "    costs_dev_index = []\n",
    "    m = X_full.shape[1]\n",
    "    step = 0\n",
    "    np.random.seed(1)\n",
    "    for i in range(iteration):\n",
    "        permutation = np.random.permutation(m)\n",
    "        X_permutated = X_full[:, permutation]\n",
    "        Y_permutated = Y_full[:, permutation]\n",
    "        for j in range( m // batch_size ):\n",
    "            X = X_permutated[:, j*batch_size:(j+1)*batch_size]\n",
    "            Y = Y_permutated[:, j*batch_size:(j+1)*batch_size]\n",
    "            if X.shape[1]==batch_size:\n",
    "                forwardpropagation_all(X)\n",
    "                Y_hat = cache['A'+str(hyper_parameters['L'])]\n",
    "                cost_value = cost_with_regularization(cost(loss(Y_hat, Y)))\n",
    "                costs.append(cost_value)\n",
    "                costs_index.append(step)\n",
    "                backpropagate_all(X, Y)\n",
    "                update_parameters()\n",
    "                \n",
    "                step += 1\n",
    "                if step%print_every==0:\n",
    "                    Y_predict = predict(Y_hat)\n",
    "                    accu = accuracy(Y_predict, Y)\n",
    "\n",
    "                    forwardpropagation_all(X_dev)\n",
    "                    Y_hat_dev = cache['A'+str(hyper_parameters['L'])]\n",
    "                    cost_dev_value = cost_with_regularization(cost(loss(Y_hat_dev, Y_dev)))\n",
    "                    costs_dev.append(cost_dev_value)\n",
    "                    costs_dev_index.append(step)\n",
    "                    Y_predict_dev = predict(Y_hat_dev)\n",
    "                    accu_dev = accuracy(Y_predict_dev, Y_dev)\n",
    "\n",
    "                    print(step,'>',i,'-th iter, training error = ',cost_value,'; accu_train = ',accu)\n",
    "                    print(' > dev error = ',cost_dev_value,'; accu_dev = ', accu_dev)\n",
    "                if np.isnan(cost_value):\n",
    "                    print(\"ERROR to nan!!!\")\n",
    "                    break\n",
    "            else:\n",
    "                print(\"Batch_Size ERROR!!!\")\n",
    "    return costs, costs_index, costs_dev, costs_dev_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_model():\n",
    "    \"\"\"test model using random data\"\"\"\n",
    "    m = 10\n",
    "    X = np.random.randn(784,m)\n",
    "    Y = one_hot(np.random.randint(low=0, high=10, size=(m,1)), C=10)\n",
    "    #print(X[:,1])\n",
    "    #print(Y[:,1])\n",
    "    costs = model(X,Y,learning_rate=0.1, print_every=10, iteration=100,\n",
    "                 hidden_layers=[10,10])\n",
    "    plt.plot(costs)\n",
    "    plt.show()\n",
    "    \n",
    "def test_model_1():\n",
    "    \"\"\"test model using real MNIST dataset\"\"\"\n",
    "    mnist = load_mnist()\n",
    "    mnist = shuffle_divide_dataset(mnist)\n",
    "    mnist = standardize(mnist)\n",
    "    mnist = flat_stack(mnist)\n",
    "    mnist = one_hot(mnist)\n",
    "    \n",
    "    costs, costs_index, costs_dev, costs_dev_index = \\\n",
    "    model(mnist['X_train'], mnist['Y_train'], mnist['X_dev'], mnist['Y_dev'],\n",
    "                  learning_rate=0.3, print_every=500, iteration=2,\n",
    "                  batch_size=128, regularization=0,\n",
    "                  hidden_layers=[784,100])\n",
    "    \n",
    "    plt.plot(costs_index, costs)\n",
    "    plt.plot(costs_dev_index, costs_dev)\n",
    "    plt.title(\"Cost Value\")\n",
    "    plt.show()\n",
    "    \n",
    "test_model_1()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
